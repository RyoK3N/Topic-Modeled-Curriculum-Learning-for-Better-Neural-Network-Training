{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "741f71f5",
   "metadata": {},
   "source": [
    "\n",
    "## Topic Modeling for TMCL: LDA Implementation\n",
    "\n",
    "This notebook implements Latent Dirichlet Allocation (LDA) topic modeling for the Topic-Modeled Curriculum Learning (TMCL) framework. We'll process both vision and NLP datasets to extract topic distributions that will serve as unsupervised difficulty metrics for curriculum learning.\n",
    "\n",
    "### Mathematical Foundation of LDA\n",
    "\n",
    "Latent Dirichlet Allocation is a generative probabilistic model that represents documents as mixtures of latent topics, where each topic is characterized by a distribution over words (or features). The generative process for a document $d$ is:\n",
    "\n",
    "1. Choose topic proportions $\\theta_d \\sim \\text{Dirichlet}(\\alpha)$\n",
    "2. For each word position $n$ in document $d$:\n",
    "    - Choose a topic $z_{dn} \\sim \\text{Multinomial}(\\theta_d)$\n",
    "    - Choose a word $w_{dn} \\sim \\text{Multinomial}(\\phi_{z_{dn}})$\n",
    " \n",
    " Where:\n",
    " - $\\alpha$ is the Dirichlet prior parameter for document-topic distributions\n",
    " - $\\phi_k$ is the word distribution for topic $k$\n",
    " - $\\theta_d$ is the topic distribution for document $d$\n",
    " \n",
    " The inference goal is to estimate the posterior distribution $P(\\theta, z | w, \\alpha, \\beta)$, which is typically done using variational inference or Gibbs sampling.\n",
    " \n",
    " For TMCL, we're interested in the document-topic distributions $\\theta_d$ which represent how much each topic contributes to a sample. The entropy of this distribution will serve as our difficulty metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9eebf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: data/experiment/101\n",
      "Output directory: results/tmcl/topic_models\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Plotting \n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Deep learning imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hugging Face imports\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Base directory where datasets are stored\n",
    "BASE_DIR = \"data/experiment/101\"\n",
    "OUTPUT_DIR = \"results/tmcl/topic_models\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5467d278",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Data Loading and Preprocessing\n",
    " \n",
    "We need to handle two different types of data:\n",
    " - **Text data (AG News, IMDb)**: Requires tokenization and vectorization\n",
    " - **Image data (CIFAR-10/100, MNIST, Fashion-MNIST)**: Requires feature extraction using pre-trained models\n",
    " \n",
    "#### Let's create utility functions to load and preprocess each dataset type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a1dfa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading ag_news text dataset...\n",
      "Training samples: 120000, Test samples: 7600\n",
      "\n",
      "Loading imdb text dataset...\n",
      "Training samples: 25000, Test samples: 25000\n",
      "\n",
      "Loading cifar10 image dataset...\n",
      "Training samples: 50000, Test samples: 10000\n",
      "\n",
      "Loading cifar100 image dataset...\n",
      "Training samples: 50000, Test samples: 10000\n",
      "\n",
      "Loading mnist image dataset...\n",
      "Training samples: 60000, Test samples: 10000\n",
      "\n",
      "Loading fashion_mnist image dataset...\n",
      "Training samples: 60000, Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DatasetLoader:\n",
    "    \"\"\"Utility class to load and preprocess datasets for TMCL.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir):\n",
    "        self.base_dir = base_dir\n",
    "        self.datasets = {}\n",
    "        \n",
    "    def load_text_dataset(self, dataset_name):\n",
    "        \"\"\"Load and preprocess text datasets (AG News, IMDb).\"\"\"\n",
    "        print(f\"\\nLoading {dataset_name} text dataset...\")\n",
    "        \n",
    "        # Load dataset from cache\n",
    "        dataset = load_dataset(dataset_name, cache_dir=os.path.join(self.base_dir, dataset_name))\n",
    "        \n",
    "        # Convert to pandas DataFrame for easier processing\n",
    "        train_df = pd.DataFrame(dataset['train'])\n",
    "        test_df = pd.DataFrame(dataset['test'])\n",
    "        \n",
    "        print(f\"Training samples: {len(train_df)}, Test samples: {len(test_df)}\")\n",
    "        \n",
    "        return {\n",
    "            'train': train_df,\n",
    "            'test': test_df,\n",
    "            'name': dataset_name\n",
    "        }\n",
    "    \n",
    "    def load_image_dataset(self, dataset_name):\n",
    "        \"\"\"Load and preprocess image datasets (CIFAR, MNIST, Fashion-MNIST).\"\"\"\n",
    "        print(f\"\\nLoading {dataset_name} image dataset...\")\n",
    "        \n",
    "        # Load dataset from cache\n",
    "        dataset = load_dataset(dataset_name, cache_dir=os.path.join(self.base_dir, dataset_name))\n",
    "        \n",
    "        # Convert to format suitable for feature extraction\n",
    "        train_data = dataset['train']\n",
    "        test_data = dataset['test']\n",
    "        \n",
    "        print(f\"Training samples: {len(train_data)}, Test samples: {len(test_data)}\")\n",
    "        \n",
    "        return {\n",
    "            'train': train_data,\n",
    "            'test': test_data,\n",
    "            'name': dataset_name\n",
    "        }\n",
    "    \n",
    "    def get_all_datasets(self):\n",
    "        \"\"\"Load all datasets for TMCL experiments.\"\"\"\n",
    "        text_datasets = ['ag_news', 'imdb']\n",
    "        image_datasets = ['cifar10', 'cifar100', 'mnist', 'fashion_mnist']\n",
    "        \n",
    "        for name in text_datasets:\n",
    "            self.datasets[name] = self.load_text_dataset(name)\n",
    "        \n",
    "        for name in image_datasets:\n",
    "            self.datasets[name] = self.load_image_dataset(name)\n",
    "        \n",
    "        return self.datasets\n",
    "\n",
    "# Load all datasets\n",
    "loader = DatasetLoader(BASE_DIR)\n",
    "datasets = loader.get_all_datasets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126cd90d",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Feature Extraction Pipeline\n",
    "#### 2.1 Text Feature Extraction\n",
    "\n",
    "For text datasets, we'll use two approaches:\n",
    "1. **Bag-of-Words (BoW)**: Simple count-based representation\n",
    "2. **TF-IDF**: Term Frequency-Inverse Document Frequency weighted representation\n",
    "\n",
    "The mathematical transformation for TF-IDF is:\n",
    "\n",
    "$$\n",
    " \\text{tfidf}(t, d) = \\text{tf}(t, d) \\times \\log\\left(\\frac{N}{\\text{df}(t)}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\text{tf}(t, d)$ is the term frequency of term $t$ in document $d$\n",
    "- $\\text{df}(t)$ is the document frequency of term $t$ (number of documents containing $t$)\n",
    "- $N$ is the total number of documents\n",
    "\n",
    "This transformation gives higher weight to terms that are frequent in a document but rare across the corpus, helping to identify distinctive topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dada86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text_data(text_data, max_features=10000, use_tfidf=True):\n",
    "    \"\"\"\n",
    "    Preprocess text data for topic modeling.\n",
    "    \n",
    "    Args:\n",
    "        text_data: List of text samples\n",
    "        max_features: Maximum number of features to extract\n",
    "        use_tfidf: Whether to use TF-IDF or simple count vectorization\n",
    "    \n",
    "    Returns:\n",
    "        feature_matrix: Document-term matrix\n",
    "        vectorizer: Fitted vectorizer object\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing text data...\")\n",
    "    \n",
    "    # Clean and normalize text\n",
    "    def clean_text(text):\n",
    "        if isinstance(text, str):\n",
    "            return text.lower().strip()\n",
    "        return \"\"\n",
    "    \n",
    "    cleaned_texts = [clean_text(text) for text in text_data]\n",
    "    \n",
    "    # Choose vectorizer based on configuration\n",
    "    if use_tfidf:\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            stop_words='english',\n",
    "            min_df=5,  # Ignore terms that appear in fewer than 5 documents\n",
    "            max_df=0.95,  # Ignore terms that appear in more than 95% of documents\n",
    "            ngram_range=(1, 2)  # Use both unigrams and bigrams\n",
    "        )\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(\n",
    "            max_features=max_features,\n",
    "            stop_words='english',\n",
    "            min_df=5,\n",
    "            max_df=0.95,\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "    \n",
    "    # Transform text to feature matrix\n",
    "    feature_matrix = vectorizer.fit_transform(cleaned_texts)\n",
    "    \n",
    "    print(f\"Feature matrix shape: {feature_matrix.shape}\")\n",
    "    print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "    \n",
    "    return feature_matrix, vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e9c85",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 Image Feature Extraction\n",
    "\n",
    "For image datasets, we need to extract meaningful features using pre-trained deep learning models. The mathematical foundation involves:\n",
    "\n",
    "1. **Feature extraction**: Using a pre-trained CNN (ResNet-18) to extract high-level features\n",
    "2. **Dimensionality reduction**: Applying PCA or direct use of penultimate layer features\n",
    " \n",
    "The transformation can be represented as:\n",
    " \n",
    "$$\n",
    " \\mathbf{f}_i = \\text{ResNet-18}_{\\text{penultimate}}(\\mathbf{x}_i)\n",
    "$$\n",
    " \n",
    "Where:\n",
    " - $\\mathbf{x}_i$ is the input image\n",
    " - $\\mathbf{f}_i \\in \\mathbb{R}^{512}$ is the 512-dimensional feature vector\n",
    " - $\\text{ResNet-18}_{\\text{penultimate}}$ represents the ResNet-18 model up to the penultimate layer\n",
    " \n",
    "These features capture semantic information about the image content, which can then be clustered into topics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9216ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageFeatureExtractor(nn.Module):\n",
    "    \"\"\"Feature extractor using pre-trained ResNet-18.\"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        # Use modern weights parameter instead of deprecated pretrained\n",
    "        weights = models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        self.resnet = models.resnet18(weights=weights)\n",
    "        \n",
    "        # Remove the final classification layer\n",
    "        self.features = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "        \n",
    "        # Freeze the model parameters\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        print(\"Initialized ResNet-18 feature extractor (frozen)\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract features from input images.\"\"\"\n",
    "        # Handle grayscale images (1 channel) by converting to 3 channels\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        features = self.features(x)\n",
    "        features = torch.flatten(features, 1)\n",
    "        return features\n",
    "\n",
    "def extract_image_features(dataset, batch_size=128, max_samples=None):\n",
    "    \"\"\"\n",
    "    Extract features from image dataset using pre-trained ResNet-18.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Hugging Face dataset object\n",
    "        batch_size: Batch size for feature extraction\n",
    "        max_samples: Maximum number of samples to process\n",
    "    \n",
    "    Returns:\n",
    "        feature_matrix: numpy array of extracted features\n",
    "        labels: corresponding labels\n",
    "    \"\"\"\n",
    "    print(\"Extracting image features...\")\n",
    "    \n",
    "    # Setup device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize feature extractor\n",
    "    feature_extractor = ImageFeatureExtractor(pretrained=True).to(device)\n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    # Setup data loader\n",
    "    class HFImageDataset(Dataset):\n",
    "        def __init__(self, hf_dataset, transform=None):\n",
    "            self.dataset = hf_dataset\n",
    "            self.transform = transform\n",
    "            self.image_key = 'image' if 'image' in hf_dataset.features else 'img'\n",
    "            self.label_key = 'label' if 'label' in hf_dataset.features else 'fine_label'\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            item = self.dataset[idx]\n",
    "            image = item[self.image_key]\n",
    "            label = item[self.label_key]\n",
    "            \n",
    "            # Convert PIL image to tensor\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            else:\n",
    "                image = transforms.ToTensor()(image)\n",
    "            \n",
    "            return image, label\n",
    "    \n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    full_dataset = HFImageDataset(dataset, transform=transform)\n",
    "    \n",
    "    if max_samples and max_samples < len(full_dataset):\n",
    "        indices = torch.randperm(len(full_dataset))[:max_samples]\n",
    "        subset_dataset = torch.utils.data.Subset(full_dataset, indices)\n",
    "        dataloader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=False)\n",
    "        print(f\"Using {max_samples} samples out of {len(full_dataset)}\")\n",
    "    else:\n",
    "        dataloader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Extract features\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            \n",
    "            features = feature_extractor(images)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Concatenate all features\n",
    "    feature_matrix = np.vstack(all_features)\n",
    "    labels = np.array(all_labels)\n",
    "    \n",
    "    print(f\"Feature matrix shape: {feature_matrix.shape}\")\n",
    "    print(f\"Feature dimension: {feature_matrix.shape[1]}\")\n",
    "    \n",
    "    return feature_matrix, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b2da86",
   "metadata": {},
   "source": [
    "\n",
    "### 3. LDA Topic Modeling Implementation\n",
    " \n",
    "Now we implement the core LDA topic modeling. The mathematical formulation involves:\n",
    " \n",
    "**Dirichlet Prior**: \n",
    "$$\n",
    " \\theta_d \\sim \\text{Dirichlet}(\\alpha)\n",
    "$$\n",
    " \n",
    "**Topic-Word Distribution**:\n",
    "$$\n",
    " \\phi_k \\sim \\text{Dirichlet}(\\beta)\n",
    "$$\n",
    " \n",
    "**Log-likelihood maximization**:\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi) = \\sum_{d=1}^D \\sum_{n=1}^{N_d} \\log \\sum_{k=1}^K \\theta_{dk} \\phi_{kw_{dn}}\n",
    "$$\n",
    " \n",
    "Where:\n",
    " - $D$ is the number of documents\n",
    " - $N_d$ is the number of words in document $d$\n",
    " - $K$ is the number of topics\n",
    " - $\\theta_{dk}$ is the probability of topic $k$ in document $d$\n",
    " - $\\phi_{kw}$ is the probability of word $w$ in topic $k$\n",
    " \n",
    "We use scikit-learn's implementation which uses variational inference to approximate the posterior distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb429b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_model(feature_matrix, n_topics=50, random_state=42):\n",
    "    \"\"\"\n",
    "    Train LDA model on feature matrix.\n",
    "    \n",
    "    Args:\n",
    "        feature_matrix: Document-term matrix or feature matrix\n",
    "        n_topics: Number of topics to extract\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        lda_model: Trained LDA model\n",
    "        topic_distributions: Topic distributions for each document\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining LDA model with {n_topics} topics...\")\n",
    "    \n",
    "    # Initialize LDA model\n",
    "    lda_model = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        max_iter=100,\n",
    "        learning_method='online',\n",
    "        batch_size=128,\n",
    "        evaluate_every=10,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,  # Use all available cores\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    topic_distributions = lda_model.fit_transform(feature_matrix)\n",
    "    \n",
    "    # Normalize topic distributions to sum to 1\n",
    "    topic_distributions = topic_distributions / topic_distributions.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    print(f\"LDA model trained successfully\")\n",
    "    print(f\"Topic distributions shape: {topic_distributions.shape}\")\n",
    "    \n",
    "    return lda_model, topic_distributions\n",
    "\n",
    "def analyze_lda_topics(lda_model, vectorizer=None, top_n=10):\n",
    "    \"\"\"\n",
    "    Analyze and display the top words for each topic.\n",
    "    \n",
    "    Args:\n",
    "        lda_model: Trained LDA model\n",
    "        vectorizer: Vectorizer used for text data (optional)\n",
    "        top_n: Number of top words to display per topic\n",
    "    \n",
    "    Returns:\n",
    "        topic_words: Dictionary mapping topic indices to top words\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing LDA topics...\")\n",
    "    \n",
    "    topic_words = {}\n",
    "    \n",
    "    if vectorizer is not None:\n",
    "        # Get feature names (words)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        for topic_idx, topic in enumerate(lda_model.components_):\n",
    "            top_indices = topic.argsort()[::-1][:top_n]\n",
    "            top_words = [feature_names[i] for i in top_indices]\n",
    "            topic_words[topic_idx] = top_words\n",
    "            \n",
    "            print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n",
    "    else:\n",
    "        print(\"No vectorizer provided - skipping word analysis for image features\")\n",
    "    \n",
    "    return topic_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b32b82",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Difficulty Score Calculation\n",
    " \n",
    "The core innovation of TMCL is using topic distributions to compute sample difficulty. The primary metric is **Shannon entropy**:\n",
    " \n",
    "$$\n",
    " D_{\\text{entropy}}(x_i) = H(P) = -\\sum_{t=1}^{T} P(t \\mid x_i) \\log P(t \\mid x_i)\n",
    "$$\n",
    " \n",
    "Where:\n",
    " - $P(t \\mid x_i)$ is the probability of topic $t$ for sample $x_i$\n",
    " - $T$ is the total number of topics\n",
    " \n",
    "**Interpretation**:\n",
    " - **Low entropy** ($H(P) \\approx 0$): Sample belongs predominantly to one topic → **Easy sample**\n",
    " - **High entropy** ($H(P) \\approx \\log T$): Sample has uniform topic distribution → **Hard sample**\n",
    " \n",
    " We also implement alternative difficulty metrics :\n",
    " \n",
    "**Max Probability (Purity)**:\n",
    "$$\n",
    " D_{\\text{max}}(x_i) = 1 - \\max_t P(t \\mid x_i)\n",
    "$$\n",
    "\n",
    "**Composite Score**:\n",
    "$$\n",
    " D_{\\text{comp}}(x_i) = \\lambda H(P) + (1 - \\lambda) D_{\\text{max}}(x_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a189c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_difficulty_scores(topic_distributions, metric='entropy', lambda_comp=0.5):\n",
    "    \"\"\"\n",
    "    Compute difficulty scores from topic distributions.\n",
    "    \n",
    "    Args:\n",
    "        topic_distributions: Array of topic distributions [n_samples, n_topics]\n",
    "        metric: Difficulty metric to use ('entropy', 'max_prob', 'composite')\n",
    "        lambda_comp: Weight for composite metric (only used if metric='composite')\n",
    "    \n",
    "    Returns:\n",
    "        difficulty_scores: Array of difficulty scores [n_samples]\n",
    "    \"\"\"\n",
    "    print(f\"\\nComputing difficulty scores using {metric} metric...\")\n",
    "    \n",
    "    if metric == 'entropy':\n",
    "        # Shannon entropy\n",
    "        epsilon = 1e-10  # Small constant to avoid log(0)\n",
    "        entropy = -np.sum(topic_distributions * np.log(topic_distributions + epsilon), axis=1)\n",
    "        difficulty_scores = entropy\n",
    "        \n",
    "    elif metric == 'max_prob':\n",
    "        # 1 - max probability (higher = more ambiguous)\n",
    "        max_prob = np.max(topic_distributions, axis=1)\n",
    "        difficulty_scores = 1 - max_prob\n",
    "        \n",
    "    elif metric == 'composite':\n",
    "        # Composite of entropy and max probability\n",
    "        epsilon = 1e-10\n",
    "        entropy = -np.sum(topic_distributions * np.log(topic_distributions + epsilon), axis=1)\n",
    "        max_prob = np.max(topic_distributions, axis=1)\n",
    "        max_prob_diff = 1 - max_prob\n",
    "        \n",
    "        difficulty_scores = lambda_comp * entropy + (1 - lambda_comp) * max_prob_diff\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric: {metric}\")\n",
    "    \n",
    "    # Normalize scores to [0, 1] range\n",
    "    difficulty_scores = (difficulty_scores - difficulty_scores.min()) / (difficulty_scores.max() - difficulty_scores.min() + 1e-10)\n",
    "    \n",
    "    print(f\"Difficulty scores computed. Range: [{difficulty_scores.min():.4f}, {difficulty_scores.max():.4f}]\")\n",
    "    print(f\"Mean difficulty: {difficulty_scores.mean():.4f}, Std: {difficulty_scores.std():.4f}\")\n",
    "    \n",
    "    return difficulty_scores\n",
    "\n",
    "def visualize_difficulty_distribution(difficulty_scores, dataset_name, metric_name):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of difficulty scores.\n",
    "    \n",
    "    Args:\n",
    "        difficulty_scores: Array of difficulty scores\n",
    "        dataset_name: Name of the dataset\n",
    "        metric_name: Name of the difficulty metric\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot histogram\n",
    "    sns.histplot(difficulty_scores, bins=50, kde=True)\n",
    "    \n",
    "    plt.title(f'Difficulty Score Distribution - {dataset_name} ({metric_name})')\n",
    "    plt.xlabel('Difficulty Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save plot\n",
    "    output_path = os.path.join(OUTPUT_DIR, f'{dataset_name}_difficulty_{metric_name}.png')\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Difficulty distribution plot saved to: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f81988",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Complete TMCL Pipeline Implementation\n",
    "\n",
    "Now we integrate all components into a complete pipeline for each dataset type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcfefce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_text_dataset(dataset_info, n_topics=50, max_features=10000):\n",
    "    \"\"\"\n",
    "    Complete pipeline for processing text datasets for TMCL.\n",
    "    \n",
    "    Args:\n",
    "        dataset_info: Dictionary containing dataset information\n",
    "        n_topics: Number of topics for LDA\n",
    "        max_features: Maximum number of text features\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary containing all processing results\n",
    "    \"\"\"\n",
    "    dataset_name = dataset_info['name']\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing text dataset: {dataset_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Combine train and test for topic modeling (unsupervised)\n",
    "    all_texts = pd.concat([dataset_info['train'], dataset_info['test']])\n",
    "    \n",
    "    # Determine text column based on dataset\n",
    "    text_column = 'text' if 'text' in all_texts.columns else 'sentence'\n",
    "    label_column = 'label' if 'label' in all_texts.columns else 'target'\n",
    "    \n",
    "    print(f\"Text column: {text_column}, Label column: {label_column}\")\n",
    "    \n",
    "    # 1. Preprocess text data\n",
    "    feature_matrix, vectorizer = preprocess_text_data(\n",
    "        all_texts[text_column].tolist(),\n",
    "        max_features=max_features,\n",
    "        use_tfidf=True\n",
    "    )\n",
    "    \n",
    "    # 2. Train LDA model\n",
    "    lda_model, topic_distributions = train_lda_model(\n",
    "        feature_matrix, \n",
    "        n_topics=n_topics,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 3. Analyze topics\n",
    "    topic_words = analyze_lda_topics(lda_model, vectorizer, top_n=10)\n",
    "    \n",
    "    # 4. Compute difficulty scores\n",
    "    difficulty_scores_entropy = compute_difficulty_scores(topic_distributions, metric='entropy')\n",
    "    difficulty_scores_max = compute_difficulty_scores(topic_distributions, metric='max_prob')\n",
    "    difficulty_scores_comp = compute_difficulty_scores(topic_distributions, metric='composite', lambda_comp=0.7)\n",
    "    \n",
    "    # 5. Visualize distributions\n",
    "    visualize_difficulty_distribution(difficulty_scores_entropy, dataset_name, 'entropy')\n",
    "    visualize_difficulty_distribution(difficulty_scores_max, dataset_name, 'max_prob')\n",
    "    \n",
    "    # 6. Save results\n",
    "    results = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'feature_matrix': feature_matrix,\n",
    "        'vectorizer': vectorizer,\n",
    "        'lda_model': lda_model,\n",
    "        'topic_distributions': topic_distributions,\n",
    "        'topic_words': topic_words,\n",
    "        'difficulty_scores': {\n",
    "            'entropy': difficulty_scores_entropy,\n",
    "            'max_prob': difficulty_scores_max,\n",
    "            'composite': difficulty_scores_comp\n",
    "        },\n",
    "        'labels': all_texts[label_column].values if label_column in all_texts.columns else None,\n",
    "        'sample_ids': range(len(all_texts))\n",
    "    }\n",
    "    \n",
    "    # Save to disk\n",
    "    output_file = os.path.join(OUTPUT_DIR, f'{dataset_name}_tmcl_results.pkl')\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    print(f\"Results saved to: {output_file}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_image_dataset(dataset_info, n_topics=15, max_samples=None):\n",
    "    \"\"\"\n",
    "    Complete pipeline for processing image datasets for TMCL.\n",
    "    \n",
    "    Args:\n",
    "        dataset_info: Dictionary containing dataset information\n",
    "        n_topics: Number of topics for LDA\n",
    "        max_samples: Maximum number of samples to process\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary containing all processing results\n",
    "    \"\"\"\n",
    "    dataset_name = dataset_info['name']\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing image dataset: {dataset_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # 1. Extract features from training data\n",
    "    train_features, train_labels = extract_image_features(\n",
    "        dataset_info['train'], \n",
    "        batch_size=128,\n",
    "        max_samples=max_samples\n",
    "    )\n",
    "    \n",
    "    # 2. Extract features from test data\n",
    "    test_features, test_labels = extract_image_features(\n",
    "        dataset_info['test'], \n",
    "        batch_size=128,\n",
    "        max_samples=max_samples\n",
    "    )\n",
    "    \n",
    "    # 3. Combine features for topic modeling\n",
    "    all_features = np.vstack([train_features, test_features])\n",
    "    all_labels = np.concatenate([train_labels, test_labels])\n",
    "    \n",
    "    print(f\"Combined feature matrix shape: {all_features.shape}\")\n",
    "    \n",
    "    # 4. Normalize features using MinMaxScaler to ensure non-negative values\n",
    "    # LDA requires non-negative input data\n",
    "    scaler = MinMaxScaler()\n",
    "    all_features_normalized = scaler.fit_transform(all_features)\n",
    "    print(f\"Feature range after scaling: [{all_features_normalized.min():.4f}, {all_features_normalized.max():.4f}]\")\n",
    "    \n",
    "    # 5. Train LDA model\n",
    "    lda_model, topic_distributions = train_lda_model(\n",
    "        all_features_normalized, \n",
    "        n_topics=n_topics,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 6. Compute difficulty scores\n",
    "    difficulty_scores_entropy = compute_difficulty_scores(topic_distributions, metric='entropy')\n",
    "    difficulty_scores_max = compute_difficulty_scores(topic_distributions, metric='max_prob')\n",
    "    difficulty_scores_comp = compute_difficulty_scores(topic_distributions, metric='composite', lambda_comp=0.7)\n",
    "    \n",
    "    # 7. Visualize distributions\n",
    "    visualize_difficulty_distribution(difficulty_scores_entropy, dataset_name, 'entropy')\n",
    "    visualize_difficulty_distribution(difficulty_scores_max, dataset_name, 'max_prob')\n",
    "    \n",
    "    # 8. Save results\n",
    "    results = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'feature_matrix': all_features_normalized,\n",
    "        'scaler': scaler,\n",
    "        'lda_model': lda_model,\n",
    "        'topic_distributions': topic_distributions,\n",
    "        'difficulty_scores': {\n",
    "            'entropy': difficulty_scores_entropy,\n",
    "            'max_prob': difficulty_scores_max,\n",
    "            'composite': difficulty_scores_comp\n",
    "        },\n",
    "        'labels': all_labels,\n",
    "        'sample_ids': range(len(all_features))\n",
    "    }\n",
    "    \n",
    "    # Save to disk\n",
    "    output_file = os.path.join(OUTPUT_DIR, f'{dataset_name}_tmcl_results.pkl')\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    print(f\"Results saved to: {output_file}\")\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3479844c",
   "metadata": {},
   "source": [
    "\n",
    "### 6. Execute TMCL Pipeline for All Datasets\n",
    " \n",
    "Now we run the complete pipeline for all datasets. This will generate the topic distributions and difficulty scores needed for the curriculum learning experiments.\n",
    "\n",
    "## Process text datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86eb95ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFImageDataset(Dataset):\n",
    "    \"\"\"Dataset wrapper for Hugging Face image datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Determine correct keys for different datasets\n",
    "        if 'img' in hf_dataset.features:\n",
    "            self.image_key = 'img'\n",
    "        elif 'image' in hf_dataset.features:\n",
    "            self.image_key = 'image'\n",
    "        else:\n",
    "            raise ValueError(\"Could not find image key in dataset features\")\n",
    "        \n",
    "        if 'label' in hf_dataset.features:\n",
    "            self.label_key = 'label'\n",
    "        elif 'fine_label' in hf_dataset.features:\n",
    "            self.label_key = 'fine_label'\n",
    "        elif 'coarse_label' in hf_dataset.features:\n",
    "            self.label_key = 'coarse_label'\n",
    "        else:\n",
    "            raise ValueError(\"Could not find label key in dataset features\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[int(idx)]  # Ensure idx is integer\n",
    "        image = item[self.image_key]\n",
    "        label = item[self.label_key]\n",
    "        \n",
    "        # Convert grayscale images to RGB by repeating the channel\n",
    "        if image.mode == 'L':  # Grayscale image\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        # Convert PIL image to tensor\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def extract_image_features(dataset, batch_size=128, max_samples=None):\n",
    "    \"\"\"\n",
    "    Extract features from image dataset using pre-trained ResNet-18.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Hugging Face dataset object\n",
    "        batch_size: Batch size for feature extraction\n",
    "        max_samples: Maximum number of samples to process\n",
    "    \n",
    "    Returns:\n",
    "        feature_matrix: numpy array of extracted features\n",
    "        labels: corresponding labels\n",
    "    \"\"\"\n",
    "    print(\"Extracting image features...\")\n",
    "    \n",
    "    # Setup device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize feature extractor\n",
    "    feature_extractor = ImageFeatureExtractor(pretrained=True).to(device)\n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    # Define transformations - handle grayscale conversion here as backup\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda x: x.convert('RGB') if x.mode == 'L' else x),  # Ensure RGB\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    full_dataset = HFImageDataset(dataset, transform=transform)\n",
    "    \n",
    "    if max_samples and max_samples < len(full_dataset):\n",
    "        # Use random subset\n",
    "        indices = torch.randperm(len(full_dataset))[:max_samples]\n",
    "        subset_dataset = torch.utils.data.Subset(full_dataset, indices.numpy())\n",
    "        dataloader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=False)\n",
    "        print(f\"Using {max_samples} samples out of {len(full_dataset)}\")\n",
    "    else:\n",
    "        dataloader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Extract features\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            \n",
    "            features = feature_extractor(images)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Concatenate all features\n",
    "    if all_features:\n",
    "        feature_matrix = np.vstack(all_features)\n",
    "        labels = np.array(all_labels)\n",
    "    else:\n",
    "        raise ValueError(\"No features extracted - dataset might be empty\")\n",
    "    \n",
    "    print(f\"Feature matrix shape: {feature_matrix.shape}\")\n",
    "    print(f\"Feature dimension: {feature_matrix.shape[1]}\")\n",
    "    \n",
    "    return feature_matrix, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e23fcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ag_news with optimizations...\n",
      "✓ Limited to 20000 samples\n",
      "\n",
      "==================================================\n",
      "Processing text dataset: ag_news\n",
      "==================================================\n",
      "Text column: text, Label column: label\n",
      "Preprocessing text data...\n",
      "Feature matrix shape: (20000, 2000)\n",
      "Vocabulary size: 2000\n",
      "\n",
      "Training LDA model with 20 topics...\n",
      "iteration: 1 of max_iter: 100\n",
      "iteration: 2 of max_iter: 100\n",
      "iteration: 3 of max_iter: 100\n",
      "iteration: 4 of max_iter: 100\n",
      "iteration: 5 of max_iter: 100\n",
      "iteration: 6 of max_iter: 100\n",
      "iteration: 7 of max_iter: 100\n",
      "iteration: 8 of max_iter: 100\n",
      "iteration: 9 of max_iter: 100\n",
      "iteration: 10 of max_iter: 100, perplexity: 8630.4607\n",
      "iteration: 11 of max_iter: 100\n",
      "iteration: 12 of max_iter: 100\n",
      "iteration: 13 of max_iter: 100\n",
      "iteration: 14 of max_iter: 100\n",
      "iteration: 15 of max_iter: 100\n",
      "iteration: 16 of max_iter: 100\n",
      "iteration: 17 of max_iter: 100\n",
      "iteration: 18 of max_iter: 100\n",
      "iteration: 19 of max_iter: 100\n",
      "iteration: 20 of max_iter: 100, perplexity: 8620.9762\n",
      "iteration: 21 of max_iter: 100\n",
      "iteration: 22 of max_iter: 100\n",
      "iteration: 23 of max_iter: 100\n",
      "iteration: 24 of max_iter: 100\n",
      "iteration: 25 of max_iter: 100\n",
      "iteration: 26 of max_iter: 100\n",
      "iteration: 27 of max_iter: 100\n",
      "iteration: 28 of max_iter: 100\n",
      "iteration: 29 of max_iter: 100\n",
      "iteration: 30 of max_iter: 100, perplexity: 8616.3677\n",
      "iteration: 31 of max_iter: 100\n",
      "iteration: 32 of max_iter: 100\n",
      "iteration: 33 of max_iter: 100\n",
      "iteration: 34 of max_iter: 100\n",
      "iteration: 35 of max_iter: 100\n",
      "iteration: 36 of max_iter: 100\n",
      "iteration: 37 of max_iter: 100\n",
      "iteration: 38 of max_iter: 100\n",
      "iteration: 39 of max_iter: 100\n",
      "iteration: 40 of max_iter: 100, perplexity: 8613.8663\n",
      "iteration: 41 of max_iter: 100\n",
      "iteration: 42 of max_iter: 100\n",
      "iteration: 43 of max_iter: 100\n",
      "iteration: 44 of max_iter: 100\n",
      "iteration: 45 of max_iter: 100\n",
      "iteration: 46 of max_iter: 100\n",
      "iteration: 47 of max_iter: 100\n",
      "iteration: 48 of max_iter: 100\n",
      "iteration: 49 of max_iter: 100\n",
      "iteration: 50 of max_iter: 100, perplexity: 8613.7844\n",
      "LDA model trained successfully\n",
      "Topic distributions shape: (20000, 20)\n",
      "\n",
      "Analyzing LDA topics...\n",
      "Topic 0: 39, football, winning, world 39, college, division, cbs, world, biggest, don\n",
      "Topic 1: oil, oil prices, prices, trial, men, crude, energy, manchester, barrel, stage\n",
      "Topic 2: sony, warned, asian, jets, rights, watch, numbers, planning, nintendo, elections\n",
      "Topic 3: china, phone, cell, mobile, drug, chinese, comes, missing, beijing, calls\n",
      "Topic 4: cup, 39, gold, final, olympic, win, coach, title, team, points\n",
      "Topic 5: microsoft, software, ibm, windows, web, security, linux, pc, service, internet\n",
      "Topic 6: quot, athens, half, french, 39, india, france, champion, tour, paris\n",
      "Topic 7: ap, 39, google, new, red, game, season, sox, league, york\n",
      "Topic 8: video, grand, arsenal, game, site, cisco, family, man, atlanta, formula\n",
      "Topic 9: iraq, said, killed, afp, police, iraqi, reuters, people, baghdad, troops\n",
      "Topic 10: chip, trade, intel, japan, green, takes, future, nokia, august, office\n",
      "Topic 11: company, 39, com, music, new, said, reuters, million, corp, oracle\n",
      "Topic 12: iran, australia, canadian, st, nuclear, canada, test, press, cricket, prize\n",
      "Topic 13: lt, gt, lt gt, gt lt, england, champions, real, match, madrid, experts\n",
      "Topic 14: darfur, air, sudan, workers, airlines, rules, union, african, labor, force\n",
      "Topic 15: talks, european, minister, eu, prime, prime minister, court, union, australian, east\n",
      "Topic 16: open, sun, russian, round, williams, source, russia, open source, shot, need\n",
      "Topic 17: space, nasa, old, year old, hurricane, year, los, angeles, los angeles, ivan\n",
      "Topic 18: ap, ap ap, 39, states, united, palestinian, like, united states, state, gaza\n",
      "Topic 19: reuters, percent, quarter, sales, profit, year, stocks, dollar, said, growth\n",
      "\n",
      "Computing difficulty scores using entropy metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.6277, Std: 0.1527\n",
      "\n",
      "Computing difficulty scores using max_prob metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.5877, Std: 0.1897\n",
      "\n",
      "Computing difficulty scores using composite metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.6250, Std: 0.1576\n",
      "Difficulty distribution plot saved to: results/tmcl/topic_models/ag_news_difficulty_entropy.png\n",
      "Difficulty distribution plot saved to: results/tmcl/topic_models/ag_news_difficulty_max_prob.png\n",
      "Results saved to: results/tmcl/topic_models/ag_news_tmcl_results.pkl\n",
      "\n",
      "Processing imdb with optimizations...\n",
      "✓ Limited to 20000 samples\n",
      "\n",
      "==================================================\n",
      "Processing text dataset: imdb\n",
      "==================================================\n",
      "Text column: text, Label column: label\n",
      "Preprocessing text data...\n",
      "Feature matrix shape: (20000, 2000)\n",
      "Vocabulary size: 2000\n",
      "\n",
      "Training LDA model with 20 topics...\n",
      "iteration: 1 of max_iter: 100\n",
      "iteration: 2 of max_iter: 100\n",
      "iteration: 3 of max_iter: 100\n",
      "iteration: 4 of max_iter: 100\n",
      "iteration: 5 of max_iter: 100\n",
      "iteration: 6 of max_iter: 100\n",
      "iteration: 7 of max_iter: 100\n",
      "iteration: 8 of max_iter: 100\n",
      "iteration: 9 of max_iter: 100\n",
      "iteration: 10 of max_iter: 100, perplexity: 2988.2335\n",
      "iteration: 11 of max_iter: 100\n",
      "iteration: 12 of max_iter: 100\n",
      "iteration: 13 of max_iter: 100\n",
      "iteration: 14 of max_iter: 100\n",
      "iteration: 15 of max_iter: 100\n",
      "iteration: 16 of max_iter: 100\n",
      "iteration: 17 of max_iter: 100\n",
      "iteration: 18 of max_iter: 100\n",
      "iteration: 19 of max_iter: 100\n",
      "iteration: 20 of max_iter: 100, perplexity: 2987.8630\n",
      "iteration: 21 of max_iter: 100\n",
      "iteration: 22 of max_iter: 100\n",
      "iteration: 23 of max_iter: 100\n",
      "iteration: 24 of max_iter: 100\n",
      "iteration: 25 of max_iter: 100\n",
      "iteration: 26 of max_iter: 100\n",
      "iteration: 27 of max_iter: 100\n",
      "iteration: 28 of max_iter: 100\n",
      "iteration: 29 of max_iter: 100\n",
      "iteration: 30 of max_iter: 100, perplexity: 2987.8013\n",
      "LDA model trained successfully\n",
      "Topic distributions shape: (20000, 20)\n",
      "\n",
      "Analyzing LDA topics...\n",
      "Topic 0: silent, poor, film, rating, minutes, porn, watching, good, world, watching film\n",
      "Topic 1: dan, br, anna, jim, ve, br br, failure, movie, gang, son\n",
      "Topic 2: film, cost, highly recommend, good thing, chris, ring, available, expecting, channel, christmas\n",
      "Topic 3: seagal, steven, impression, force, final, lost, fights, blame, attack, double\n",
      "Topic 4: fi, sci, sci fi, channel, angel, science fiction, alien, creatures, planet, space\n",
      "Topic 5: film, cost, fly, memories, emotion, puts, vs, outstanding, afraid, funny\n",
      "Topic 6: rental, br, movie, br br, watch, ve, robin, 10, ve seen, dr\n",
      "Topic 7: movie, bad, worst, waste, seen, worst movie, waste time, acting, terrible, movies\n",
      "Topic 8: br, alan, br br, war, band, film, tears, movie, piece, dvd\n",
      "Topic 9: cost, film, rip, good thing, wondering, tape, watched, rented, scream, henry\n",
      "Topic 10: scott, christmas, george, classic, stewart, finest, collection, version, viewers, br\n",
      "Topic 11: br, film, movie, br br, eric, better, movies, acting, like, just\n",
      "Topic 12: questions, female, women, sean, sisters, film makers, wide, event, wanted, fascinating\n",
      "Topic 13: russian, european, teenage, dirty, film, famous, parts, daughter, father, island\n",
      "Topic 14: film, cost, jean, vampire, good film, seeing, forever, outstanding, rented, touching\n",
      "Topic 15: enjoyable, bad, halloween, wouldn, add, movie, warning, offer, harry, logic\n",
      "Topic 16: fu, treated, cameo, chinese, years old, fights, humour, hands, agree, hair\n",
      "Topic 17: br, film, fox, date, amazing, steve, difficult, comedies, br br, rare\n",
      "Topic 18: br, br br, movie, film, like, just, good, story, really, time\n",
      "Topic 19: martial, arts, martial arts, van, fight, fu, chinese, fights, enter, bruce\n",
      "\n",
      "Computing difficulty scores using entropy metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.2461, Std: 0.1141\n",
      "\n",
      "Computing difficulty scores using max_prob metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.1385, Std: 0.1057\n",
      "\n",
      "Computing difficulty scores using composite metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.2311, Std: 0.1119\n",
      "Difficulty distribution plot saved to: results/tmcl/topic_models/imdb_difficulty_entropy.png\n",
      "Difficulty distribution plot saved to: results/tmcl/topic_models/imdb_difficulty_max_prob.png\n",
      "Results saved to: results/tmcl/topic_models/imdb_tmcl_results.pkl\n",
      "\n",
      "Processing cifar10 with optimizations...\n",
      "\n",
      "==================================================\n",
      "Processing image dataset: cifar10\n",
      "==================================================\n",
      "Extracting image features...\n",
      "Using device: cpu\n",
      "Initialized ResNet-18 feature extractor (frozen)\n",
      "Using 5000 samples out of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 40/40 [01:12<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (5000, 512)\n",
      "Feature dimension: 512\n",
      "Extracting image features...\n",
      "Using device: cpu\n",
      "Initialized ResNet-18 feature extractor (frozen)\n",
      "Using 5000 samples out of 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 40/40 [01:13<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (5000, 512)\n",
      "Feature dimension: 512\n",
      "Combined feature matrix shape: (10000, 512)\n",
      "Feature range after scaling: [0.0000, 1.0000]\n",
      "\n",
      "Training LDA model with 15 topics...\n",
      "iteration: 1 of max_iter: 100\n",
      "iteration: 2 of max_iter: 100\n",
      "iteration: 3 of max_iter: 100\n",
      "iteration: 4 of max_iter: 100\n",
      "iteration: 5 of max_iter: 100\n",
      "iteration: 6 of max_iter: 100\n",
      "iteration: 7 of max_iter: 100\n",
      "iteration: 8 of max_iter: 100\n",
      "iteration: 9 of max_iter: 100\n",
      "iteration: 10 of max_iter: 100, perplexity: 522.3162\n",
      "iteration: 11 of max_iter: 100\n",
      "iteration: 12 of max_iter: 100\n",
      "iteration: 13 of max_iter: 100\n",
      "iteration: 14 of max_iter: 100\n",
      "iteration: 15 of max_iter: 100\n",
      "iteration: 16 of max_iter: 100\n",
      "iteration: 17 of max_iter: 100\n",
      "iteration: 18 of max_iter: 100\n",
      "iteration: 19 of max_iter: 100\n",
      "iteration: 20 of max_iter: 100, perplexity: 522.3400\n",
      "LDA model trained successfully\n",
      "Topic distributions shape: (10000, 15)\n",
      "\n",
      "Computing difficulty scores using entropy metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.3739, Std: 0.2185\n",
      "\n",
      "Computing difficulty scores using max_prob metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.4163, Std: 0.2677\n",
      "\n",
      "Computing difficulty scores using composite metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.3806, Std: 0.2249\n",
      "Difficulty distribution plot saved to: results/tmcl/topic_models/cifar10_difficulty_entropy.png\n",
      "Difficulty distribution plot saved to: results/tmcl/topic_models/cifar10_difficulty_max_prob.png\n",
      "Results saved to: results/tmcl/topic_models/cifar10_tmcl_results.pkl\n",
      "\n",
      "Processing cifar100 with optimizations...\n",
      "\n",
      "==================================================\n",
      "Processing image dataset: cifar100\n",
      "==================================================\n",
      "Extracting image features...\n",
      "Using device: cpu\n",
      "Initialized ResNet-18 feature extractor (frozen)\n",
      "Using 5000 samples out of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 40/40 [01:15<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (5000, 512)\n",
      "Feature dimension: 512\n",
      "Extracting image features...\n",
      "Using device: cpu\n",
      "Initialized ResNet-18 feature extractor (frozen)\n",
      "Using 5000 samples out of 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 40/40 [01:10<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (5000, 512)\n",
      "Feature dimension: 512\n",
      "Combined feature matrix shape: (10000, 512)\n",
      "Feature range after scaling: [0.0000, 1.0000]\n",
      "\n",
      "Training LDA model with 15 topics...\n",
      "iteration: 1 of max_iter: 100\n",
      "iteration: 2 of max_iter: 100\n",
      "iteration: 3 of max_iter: 100\n",
      "iteration: 4 of max_iter: 100\n",
      "iteration: 5 of max_iter: 100\n",
      "iteration: 6 of max_iter: 100\n",
      "iteration: 7 of max_iter: 100\n",
      "iteration: 8 of max_iter: 100\n",
      "iteration: 9 of max_iter: 100\n",
      "iteration: 10 of max_iter: 100, perplexity: 536.0614\n",
      "iteration: 11 of max_iter: 100\n",
      "iteration: 12 of max_iter: 100\n",
      "iteration: 13 of max_iter: 100\n",
      "iteration: 14 of max_iter: 100\n",
      "iteration: 15 of max_iter: 100\n",
      "iteration: 16 of max_iter: 100\n",
      "iteration: 17 of max_iter: 100\n",
      "iteration: 18 of max_iter: 100\n",
      "iteration: 19 of max_iter: 100\n",
      "iteration: 20 of max_iter: 100, perplexity: 535.7940\n",
      "iteration: 21 of max_iter: 100\n",
      "iteration: 22 of max_iter: 100\n",
      "iteration: 23 of max_iter: 100\n",
      "iteration: 24 of max_iter: 100\n",
      "iteration: 25 of max_iter: 100\n",
      "iteration: 26 of max_iter: 100\n",
      "iteration: 27 of max_iter: 100\n",
      "iteration: 28 of max_iter: 100\n",
      "iteration: 29 of max_iter: 100\n",
      "iteration: 30 of max_iter: 100, perplexity: 535.4319\n",
      "iteration: 31 of max_iter: 100\n",
      "iteration: 32 of max_iter: 100\n",
      "iteration: 33 of max_iter: 100\n",
      "iteration: 34 of max_iter: 100\n",
      "iteration: 35 of max_iter: 100\n",
      "iteration: 36 of max_iter: 100\n",
      "iteration: 37 of max_iter: 100\n",
      "iteration: 38 of max_iter: 100\n",
      "iteration: 39 of max_iter: 100\n",
      "iteration: 40 of max_iter: 100, perplexity: 535.2640\n",
      "iteration: 41 of max_iter: 100\n",
      "iteration: 42 of max_iter: 100\n",
      "iteration: 43 of max_iter: 100\n",
      "iteration: 44 of max_iter: 100\n",
      "iteration: 45 of max_iter: 100\n",
      "iteration: 46 of max_iter: 100\n",
      "iteration: 47 of max_iter: 100\n",
      "iteration: 48 of max_iter: 100\n",
      "iteration: 49 of max_iter: 100\n",
      "iteration: 50 of max_iter: 100, perplexity: 535.1306\n",
      "iteration: 51 of max_iter: 100\n",
      "iteration: 52 of max_iter: 100\n",
      "iteration: 53 of max_iter: 100\n",
      "iteration: 54 of max_iter: 100\n",
      "iteration: 55 of max_iter: 100\n",
      "iteration: 56 of max_iter: 100\n",
      "iteration: 57 of max_iter: 100\n",
      "iteration: 58 of max_iter: 100\n",
      "iteration: 59 of max_iter: 100\n",
      "iteration: 60 of max_iter: 100, perplexity: 535.0226\n",
      "iteration: 61 of max_iter: 100\n",
      "iteration: 62 of max_iter: 100\n",
      "iteration: 63 of max_iter: 100\n",
      "iteration: 64 of max_iter: 100\n",
      "iteration: 65 of max_iter: 100\n",
      "iteration: 66 of max_iter: 100\n",
      "iteration: 67 of max_iter: 100\n",
      "iteration: 68 of max_iter: 100\n",
      "iteration: 69 of max_iter: 100\n",
      "iteration: 70 of max_iter: 100, perplexity: 534.9045\n",
      "iteration: 71 of max_iter: 100\n",
      "iteration: 72 of max_iter: 100\n",
      "iteration: 73 of max_iter: 100\n",
      "iteration: 74 of max_iter: 100\n",
      "iteration: 75 of max_iter: 100\n",
      "iteration: 76 of max_iter: 100\n",
      "iteration: 77 of max_iter: 100\n",
      "iteration: 78 of max_iter: 100\n",
      "iteration: 79 of max_iter: 100\n",
      "iteration: 80 of max_iter: 100, perplexity: 534.8022\n",
      "iteration: 81 of max_iter: 100\n",
      "iteration: 82 of max_iter: 100\n",
      "iteration: 83 of max_iter: 100\n",
      "iteration: 84 of max_iter: 100\n",
      "iteration: 85 of max_iter: 100\n",
      "iteration: 86 of max_iter: 100\n",
      "iteration: 87 of max_iter: 100\n",
      "iteration: 88 of max_iter: 100\n",
      "iteration: 89 of max_iter: 100\n",
      "iteration: 90 of max_iter: 100, perplexity: 534.7345\n",
      "LDA model trained successfully\n",
      "Topic distributions shape: (10000, 15)\n",
      "\n",
      "Computing difficulty scores using entropy metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.4144, Std: 0.2268\n",
      "\n",
      "Computing difficulty scores using max_prob metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.4369, Std: 0.2651\n",
      "\n",
      "Computing difficulty scores using composite metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.4214, Std: 0.2334\n",
      "Difficulty distribution plot saved to: results/tmcl/topic_models/cifar100_difficulty_entropy.png\n",
      "Difficulty distribution plot saved to: results/tmcl/topic_models/cifar100_difficulty_max_prob.png\n",
      "Results saved to: results/tmcl/topic_models/cifar100_tmcl_results.pkl\n",
      "\n",
      "Processing mnist with optimizations...\n",
      "\n",
      "==================================================\n",
      "Processing image dataset: mnist\n",
      "==================================================\n",
      "Extracting image features...\n",
      "Using device: cpu\n",
      "Initialized ResNet-18 feature extractor (frozen)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 469/469 [14:04<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (60000, 512)\n",
      "Feature dimension: 512\n",
      "Extracting image features...\n",
      "Using device: cpu\n",
      "Initialized ResNet-18 feature extractor (frozen)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 79/79 [02:20<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (10000, 512)\n",
      "Feature dimension: 512\n",
      "Combined feature matrix shape: (70000, 512)\n",
      "Feature range after scaling: [0.0000, 1.0000]\n",
      "\n",
      "Training LDA model with 15 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 100\n",
      "iteration: 2 of max_iter: 100\n",
      "iteration: 3 of max_iter: 100\n",
      "iteration: 4 of max_iter: 100\n",
      "iteration: 5 of max_iter: 100\n",
      "iteration: 6 of max_iter: 100\n",
      "iteration: 7 of max_iter: 100\n",
      "iteration: 8 of max_iter: 100\n",
      "iteration: 9 of max_iter: 100\n",
      "iteration: 10 of max_iter: 100, perplexity: 443.1813\n",
      "iteration: 11 of max_iter: 100\n",
      "iteration: 12 of max_iter: 100\n",
      "iteration: 13 of max_iter: 100\n",
      "iteration: 14 of max_iter: 100\n",
      "iteration: 15 of max_iter: 100\n",
      "iteration: 16 of max_iter: 100\n",
      "iteration: 17 of max_iter: 100\n",
      "iteration: 18 of max_iter: 100\n",
      "iteration: 19 of max_iter: 100\n",
      "iteration: 20 of max_iter: 100, perplexity: 443.2361\n",
      "LDA model trained successfully\n",
      "Topic distributions shape: (70000, 15)\n",
      "\n",
      "Computing difficulty scores using entropy metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.4061, Std: 0.2187\n",
      "\n",
      "Computing difficulty scores using max_prob metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.4283, Std: 0.2540\n",
      "\n",
      "Computing difficulty scores using composite metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.4119, Std: 0.2242\n",
      "Difficulty distribution plot saved to: results/tmcl/topic_models/mnist_difficulty_entropy.png\n",
      "Difficulty distribution plot saved to: results/tmcl/topic_models/mnist_difficulty_max_prob.png\n",
      "Results saved to: results/tmcl/topic_models/mnist_tmcl_results.pkl\n",
      "\n",
      "Processing fashion_mnist with optimizations...\n",
      "\n",
      "==================================================\n",
      "Processing image dataset: fashion_mnist\n",
      "==================================================\n",
      "Extracting image features...\n",
      "Using device: cpu\n",
      "Initialized ResNet-18 feature extractor (frozen)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 469/469 [14:03<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (60000, 512)\n",
      "Feature dimension: 512\n",
      "Extracting image features...\n",
      "Using device: cpu\n",
      "Initialized ResNet-18 feature extractor (frozen)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 79/79 [02:20<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (10000, 512)\n",
      "Feature dimension: 512\n",
      "Combined feature matrix shape: (70000, 512)\n",
      "Feature range after scaling: [0.0000, 1.0000]\n",
      "\n",
      "Training LDA model with 15 topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 100\n",
      "iteration: 2 of max_iter: 100\n",
      "iteration: 3 of max_iter: 100\n",
      "iteration: 4 of max_iter: 100\n",
      "iteration: 5 of max_iter: 100\n",
      "iteration: 6 of max_iter: 100\n",
      "iteration: 7 of max_iter: 100\n",
      "iteration: 8 of max_iter: 100\n",
      "iteration: 9 of max_iter: 100\n",
      "iteration: 10 of max_iter: 100, perplexity: 452.1410\n",
      "iteration: 11 of max_iter: 100\n",
      "iteration: 12 of max_iter: 100\n",
      "iteration: 13 of max_iter: 100\n",
      "iteration: 14 of max_iter: 100\n",
      "iteration: 15 of max_iter: 100\n",
      "iteration: 16 of max_iter: 100\n",
      "iteration: 17 of max_iter: 100\n",
      "iteration: 18 of max_iter: 100\n",
      "iteration: 19 of max_iter: 100\n",
      "iteration: 20 of max_iter: 100, perplexity: 452.0477\n",
      "LDA model trained successfully\n",
      "Topic distributions shape: (70000, 15)\n",
      "\n",
      "Computing difficulty scores using entropy metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.2874, Std: 0.2172\n",
      "\n",
      "Computing difficulty scores using max_prob metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.3122, Std: 0.2629\n",
      "\n",
      "Computing difficulty scores using composite metric...\n",
      "Difficulty scores computed. Range: [0.0000, 1.0000]\n",
      "Mean difficulty: 0.2918, Std: 0.2237\n",
      "Difficulty distribution plot saved to: results/tmcl/topic_models/fashion_mnist_difficulty_entropy.png\n",
      "Difficulty distribution plot saved to: results/tmcl/topic_models/fashion_mnist_difficulty_max_prob.png\n",
      "Results saved to: results/tmcl/topic_models/fashion_mnist_tmcl_results.pkl\n"
     ]
    }
   ],
   "source": [
    "# Process text datasets with optimizations (same as before)\n",
    "text_datasets = ['ag_news', 'imdb']\n",
    "text_results = {}\n",
    "\n",
    "# Use smaller sample sizes and reduced features for faster processing\n",
    "SAMPLE_SIZE_LIMIT = 20000\n",
    "MAX_FEATURES_TEXT = 2000\n",
    "N_TOPICS_TEXT = 20\n",
    "\n",
    "for dataset_name in text_datasets:\n",
    "    if dataset_name in datasets:\n",
    "        print(f\"\\nProcessing {dataset_name} with optimizations...\")\n",
    "        dataset_info = datasets[dataset_name]\n",
    "        train_df = dataset_info['train']\n",
    "        test_df = dataset_info['test']\n",
    "        all_texts = pd.concat([train_df, test_df])\n",
    "        if len(all_texts) > SAMPLE_SIZE_LIMIT:\n",
    "            all_texts = all_texts.sample(n=SAMPLE_SIZE_LIMIT, random_state=42)\n",
    "            print(f\"✓ Limited to {SAMPLE_SIZE_LIMIT} samples\")\n",
    "        dataset_info['train'] = all_texts[:len(train_df)]\n",
    "        dataset_info['test'] = all_texts[len(train_df):]\n",
    "        results = process_text_dataset(\n",
    "            dataset_info,\n",
    "            n_topics=N_TOPICS_TEXT,\n",
    "            max_features=MAX_FEATURES_TEXT\n",
    "        )\n",
    "        text_results[dataset_name] = results\n",
    "\n",
    "# Process image datasets with optimizations\n",
    "image_datasets = ['cifar10', 'cifar100', 'mnist', 'fashion_mnist']\n",
    "image_results = {}\n",
    "\n",
    "MAX_SAMPLES_IMAGE = 5000\n",
    "N_TOPICS_IMAGE = 15\n",
    "\n",
    "for dataset_name in image_datasets:\n",
    "    if dataset_name in datasets:\n",
    "        print(f\"\\nProcessing {dataset_name} with optimizations...\")\n",
    "        max_samples = MAX_SAMPLES_IMAGE if dataset_name in ['cifar10', 'cifar100'] else None\n",
    "        results = process_image_dataset(\n",
    "            datasets[dataset_name],\n",
    "            n_topics=N_TOPICS_IMAGE,\n",
    "            max_samples=max_samples\n",
    "        )\n",
    "        image_results[dataset_name] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3125a480",
   "metadata": {},
   "source": [
    "### 7. Analysis and Validation\n",
    " \n",
    "Let's perform some basic validation of the difficulty scores by checking their correlation with initial training loss. This validates our hypothesis that topic entropy correlates with sample difficulty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82927b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating difficulty scores for ag_news...\n",
      "Validation plot saved to: results/tmcl/topic_models/ag_news_diversity_validation.png\n",
      "Label diversity by difficulty bin: [0.3, 0.05194805194805195, 0.015209125475285171, 0.0073937153419593345, 0.03669724770642202]\n",
      "\n",
      "Validating difficulty scores for cifar10...\n",
      "Validation plot saved to: results/tmcl/topic_models/cifar10_diversity_validation.png\n",
      "Label diversity by difficulty bin: [0.04784688995215311, 0.03076923076923077, 0.03184713375796178, 0.06622516556291391, 1.0]\n",
      "\n",
      "Validating difficulty scores for imdb...\n",
      "Validation plot saved to: results/tmcl/topic_models/imdb_diversity_validation.png\n",
      "Label diversity by difficulty bin: [0.005277044854881266, 0.0038684719535783366, 0.024096385542168676, 0.1, 1.0]\n",
      "\n",
      "Validating difficulty scores for cifar100...\n",
      "Validation plot saved to: results/tmcl/topic_models/cifar100_diversity_validation.png\n",
      "Label diversity by difficulty bin: [0.3106796116504854, 0.33584905660377357, 0.304635761589404, 0.3619047619047619, 1.0]\n",
      "\n",
      "Validating difficulty scores for mnist...\n",
      "Validation plot saved to: results/tmcl/topic_models/mnist_diversity_validation.png\n",
      "Label diversity by difficulty bin: [0.060240963855421686, 0.03067484662576687, 0.03225806451612903, 0.049723756906077346, 0.47058823529411764]\n",
      "\n",
      "Validating difficulty scores for fashion_mnist...\n",
      "Validation plot saved to: results/tmcl/topic_models/fashion_mnist_diversity_validation.png\n",
      "Label diversity by difficulty bin: [0.03215434083601286, 0.02577319587628866, 0.04310344827586207, 0.13636363636363635, 0.6666666666666666]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def validate_difficulty_scores(dataset_name, results, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Validate difficulty scores by correlating with initial training loss.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset\n",
    "        results: Processing results containing difficulty scores\n",
    "        sample_size: Number of samples to use for validation\n",
    "    \n",
    "    Returns:\n",
    "        correlation_results: Dictionary of correlation results\n",
    "    \"\"\"\n",
    "    print(f\"\\nValidating difficulty scores for {dataset_name}...\")\n",
    "    \n",
    "    # Get difficulty scores and labels\n",
    "    difficulty_entropy = results['difficulty_scores']['entropy']\n",
    "    labels = results['labels']\n",
    "    \n",
    "    if labels is None:\n",
    "        print(\"No labels available for validation\")\n",
    "        return None\n",
    "    \n",
    "    # Sample a subset for validation\n",
    "    if len(difficulty_entropy) > sample_size:\n",
    "        indices = np.random.choice(len(difficulty_entropy), sample_size, replace=False)\n",
    "        difficulty_entropy = difficulty_entropy[indices]\n",
    "        labels = labels[indices]\n",
    "    \n",
    "    # Create simple validation: check if high difficulty samples have more diverse labels\n",
    "    # This is a proxy validation since we don't have actual training loss yet\n",
    "    \n",
    "    # Bin samples by difficulty\n",
    "    n_bins = 5\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_indices = np.digitize(difficulty_entropy, bins) - 1\n",
    "    \n",
    "    # Calculate label diversity per bin\n",
    "    bin_diversity = []\n",
    "    for i in range(n_bins):\n",
    "        bin_mask = (bin_indices == i)\n",
    "        if np.sum(bin_mask) > 0:\n",
    "            bin_labels = labels[bin_mask]\n",
    "            # Calculate number of unique labels / total samples\n",
    "            diversity = len(np.unique(bin_labels)) / len(bin_labels)\n",
    "            bin_diversity.append(diversity)\n",
    "        else:\n",
    "            bin_diversity.append(0)\n",
    "    \n",
    "    # Plot label diversity vs difficulty\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(n_bins), bin_diversity, 'o-', linewidth=2, markersize=8)\n",
    "    plt.title(f'Label Diversity vs Difficulty - {dataset_name}')\n",
    "    plt.xlabel('Difficulty Bin (0=easy, 4=hard)')\n",
    "    plt.ylabel('Label Diversity (Unique labels / Total samples)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    output_path = os.path.join(OUTPUT_DIR, f'{dataset_name}_diversity_validation.png')\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Validation plot saved to: {output_path}\")\n",
    "    print(f\"Label diversity by difficulty bin: {bin_diversity}\")\n",
    "    \n",
    "    return {\n",
    "        'dataset_name': dataset_name,\n",
    "        'bin_diversity': bin_diversity,\n",
    "        'correlation_plot': output_path\n",
    "    }\n",
    "\n",
    "# Run validation for a few datasets\n",
    "validation_results = {}\n",
    "for dataset_name in ['ag_news', 'cifar10' , 'imdb' , 'cifar100', 'mnist', 'fashion_mnist' ]:\n",
    "    if dataset_name in text_results:\n",
    "        validation_results[dataset_name] = validate_difficulty_scores(dataset_name, text_results[dataset_name])\n",
    "    elif dataset_name in image_results:\n",
    "        validation_results[dataset_name] = validate_difficulty_scores(dataset_name, image_results[dataset_name])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2997e749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING INTERACTIVE TOPIC VISUALIZATIONS\n",
      "================================================================================\n",
      "\n",
      "Processing ag_news...\n",
      "  Successfully loaded results from results/tmcl/topic_models/ag_news_tmcl_results.pkl\n",
      "  Using vectorizer vocabulary of size 2000\n",
      "\n",
      "Creating interactive topic network for ag_news...\n",
      "  Interactive network saved to: results/tmcl/topic_models/ag_news_interactive_network.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4_/mvw4dlxj1xbdr5dqwn8l47rh0000gn/T/ipykernel_22567/3918047744.py:1013: DeprecationWarning:\n",
      "\n",
      "\n",
      "Use of plotly.io.kaleido.scope.default_format is deprecated and support will be removed after September 2025.\n",
      "Please use plotly.io.defaults.default_format instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Static version saved to: results/tmcl/topic_models/ag_news_interactive_network_static.png\n",
      "  ✓ Created interactive topic network\n",
      "\n",
      "Creating interactive dashboard for ag_news...\n",
      "  Interactive dashboard saved to: results/tmcl/topic_models/ag_news_interactive_dashboard.html\n",
      "  Static dashboard saved to: results/tmcl/topic_models/ag_news_interactive_dashboard.png\n",
      "  ✓ Created interactive dashboard\n",
      "\n",
      "Creating topic evolution timeline for ag_news...\n",
      "  Topic timeline saved to: results/tmcl/topic_models/ag_news_topic_timeline.html\n",
      "  ✓ Created topic evolution timeline\n",
      "  Note: Enhanced analysis results not found at results/tmcl/topic_models/ag_news_enhanced_analysis.pkl\n",
      "\n",
      "Processing imdb...\n",
      "  Successfully loaded results from results/tmcl/topic_models/imdb_tmcl_results.pkl\n",
      "  Using vectorizer vocabulary of size 2000\n",
      "\n",
      "Creating interactive topic network for imdb...\n",
      "  Interactive network saved to: results/tmcl/topic_models/imdb_interactive_network.html\n",
      "  Static version saved to: results/tmcl/topic_models/imdb_interactive_network_static.png\n",
      "  ✓ Created interactive topic network\n",
      "\n",
      "Creating interactive dashboard for imdb...\n",
      "  Interactive dashboard saved to: results/tmcl/topic_models/imdb_interactive_dashboard.html\n",
      "  Static dashboard saved to: results/tmcl/topic_models/imdb_interactive_dashboard.png\n",
      "  ✓ Created interactive dashboard\n",
      "\n",
      "Creating topic evolution timeline for imdb...\n",
      "  Topic timeline saved to: results/tmcl/topic_models/imdb_topic_timeline.html\n",
      "  ✓ Created topic evolution timeline\n",
      "  Note: Enhanced analysis results not found at results/tmcl/topic_models/imdb_enhanced_analysis.pkl\n",
      "\n",
      "Processing cifar10...\n",
      "  Successfully loaded results from results/tmcl/topic_models/cifar10_tmcl_results.pkl\n",
      "\n",
      "Creating interactive topic network for cifar10...\n",
      "  Interactive network saved to: results/tmcl/topic_models/cifar10_interactive_network.html\n",
      "  Static version saved to: results/tmcl/topic_models/cifar10_interactive_network_static.png\n",
      "  ✓ Created interactive topic network\n",
      "\n",
      "Creating interactive dashboard for cifar10...\n",
      "  Interactive dashboard saved to: results/tmcl/topic_models/cifar10_interactive_dashboard.html\n",
      "  Static dashboard saved to: results/tmcl/topic_models/cifar10_interactive_dashboard.png\n",
      "  ✓ Created interactive dashboard\n",
      "\n",
      "Creating topic evolution timeline for cifar10...\n",
      "  Topic timeline saved to: results/tmcl/topic_models/cifar10_topic_timeline.html\n",
      "  ✓ Created topic evolution timeline\n",
      "  Note: Enhanced analysis results not found at results/tmcl/topic_models/cifar10_enhanced_analysis.pkl\n",
      "\n",
      "Processing cifar100...\n",
      "  Successfully loaded results from results/tmcl/topic_models/cifar100_tmcl_results.pkl\n",
      "\n",
      "Creating interactive topic network for cifar100...\n",
      "  Interactive network saved to: results/tmcl/topic_models/cifar100_interactive_network.html\n",
      "  Static version saved to: results/tmcl/topic_models/cifar100_interactive_network_static.png\n",
      "  ✓ Created interactive topic network\n",
      "\n",
      "Creating interactive dashboard for cifar100...\n",
      "  Interactive dashboard saved to: results/tmcl/topic_models/cifar100_interactive_dashboard.html\n",
      "  Static dashboard saved to: results/tmcl/topic_models/cifar100_interactive_dashboard.png\n",
      "  ✓ Created interactive dashboard\n",
      "\n",
      "Creating topic evolution timeline for cifar100...\n",
      "  Topic timeline saved to: results/tmcl/topic_models/cifar100_topic_timeline.html\n",
      "  ✓ Created topic evolution timeline\n",
      "  Note: Enhanced analysis results not found at results/tmcl/topic_models/cifar100_enhanced_analysis.pkl\n",
      "\n",
      "Processing mnist...\n",
      "  Successfully loaded results from results/tmcl/topic_models/mnist_tmcl_results.pkl\n",
      "\n",
      "Creating interactive topic network for mnist...\n",
      "  Interactive network saved to: results/tmcl/topic_models/mnist_interactive_network.html\n",
      "  Static version saved to: results/tmcl/topic_models/mnist_interactive_network_static.png\n",
      "  ✓ Created interactive topic network\n",
      "\n",
      "Creating interactive dashboard for mnist...\n",
      "  Interactive dashboard saved to: results/tmcl/topic_models/mnist_interactive_dashboard.html\n",
      "  Static dashboard saved to: results/tmcl/topic_models/mnist_interactive_dashboard.png\n",
      "  ✓ Created interactive dashboard\n",
      "\n",
      "Creating topic evolution timeline for mnist...\n",
      "  Topic timeline saved to: results/tmcl/topic_models/mnist_topic_timeline.html\n",
      "  ✓ Created topic evolution timeline\n",
      "  Note: Enhanced analysis results not found at results/tmcl/topic_models/mnist_enhanced_analysis.pkl\n",
      "\n",
      "Processing fashion_mnist...\n",
      "  Successfully loaded results from results/tmcl/topic_models/fashion_mnist_tmcl_results.pkl\n",
      "\n",
      "Creating interactive topic network for fashion_mnist...\n",
      "  Interactive network saved to: results/tmcl/topic_models/fashion_mnist_interactive_network.html\n",
      "  Static version saved to: results/tmcl/topic_models/fashion_mnist_interactive_network_static.png\n",
      "  ✓ Created interactive topic network\n",
      "\n",
      "Creating interactive dashboard for fashion_mnist...\n",
      "  Interactive dashboard saved to: results/tmcl/topic_models/fashion_mnist_interactive_dashboard.html\n",
      "  Static dashboard saved to: results/tmcl/topic_models/fashion_mnist_interactive_dashboard.png\n",
      "  ✓ Created interactive dashboard\n",
      "\n",
      "Creating topic evolution timeline for fashion_mnist...\n",
      "  Topic timeline saved to: results/tmcl/topic_models/fashion_mnist_topic_timeline.html\n",
      "  ✓ Created topic evolution timeline\n",
      "  Note: Enhanced analysis results not found at results/tmcl/topic_models/fashion_mnist_enhanced_analysis.pkl\n",
      "\n",
      "================================================================================\n",
      "INTERACTIVE VISUALIZATIONS COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Created interactive visualizations:\n",
      "----------------------------------------\n",
      "\n",
      "ag_news:\n",
      "  ✓ Network: results/tmcl/topic_models/ag_news_interactive_network.html\n",
      "  ✓ Dashboard: results/tmcl/topic_models/ag_news_interactive_dashboard.html\n",
      "  ✓ Timeline: results/tmcl/topic_models/ag_news_topic_timeline.html\n",
      "\n",
      "imdb:\n",
      "  ✓ Network: results/tmcl/topic_models/imdb_interactive_network.html\n",
      "  ✓ Dashboard: results/tmcl/topic_models/imdb_interactive_dashboard.html\n",
      "  ✓ Timeline: results/tmcl/topic_models/imdb_topic_timeline.html\n",
      "\n",
      "cifar10:\n",
      "  ✓ Network: results/tmcl/topic_models/cifar10_interactive_network.html\n",
      "  ✓ Dashboard: results/tmcl/topic_models/cifar10_interactive_dashboard.html\n",
      "  ✓ Timeline: results/tmcl/topic_models/cifar10_topic_timeline.html\n",
      "\n",
      "cifar100:\n",
      "  ✓ Network: results/tmcl/topic_models/cifar100_interactive_network.html\n",
      "  ✓ Dashboard: results/tmcl/topic_models/cifar100_interactive_dashboard.html\n",
      "  ✓ Timeline: results/tmcl/topic_models/cifar100_topic_timeline.html\n",
      "\n",
      "mnist:\n",
      "  ✓ Network: results/tmcl/topic_models/mnist_interactive_network.html\n",
      "  ✓ Dashboard: results/tmcl/topic_models/mnist_interactive_dashboard.html\n",
      "  ✓ Timeline: results/tmcl/topic_models/mnist_topic_timeline.html\n",
      "\n",
      "fashion_mnist:\n",
      "  ✓ Network: results/tmcl/topic_models/fashion_mnist_interactive_network.html\n",
      "  ✓ Dashboard: results/tmcl/topic_models/fashion_mnist_interactive_dashboard.html\n",
      "  ✓ Timeline: results/tmcl/topic_models/fashion_mnist_topic_timeline.html\n",
      "\n",
      "================================================================================\n",
      "To view interactive visualizations:\n",
      "1. Open the HTML files in a web browser\n",
      "2. Hover over nodes/points to see details\n",
      "3. Use zoom and pan to explore the visualizations\n",
      "================================================================================\n",
      "\n",
      "For text datasets (ag_news, imdb):\n",
      "• Topic labels show T0, T1, etc. with top words\n",
      "• Hover over nodes to see all top words for each topic\n",
      "• Word indices are shown if actual words are not available\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_interactive_topic_network(dataset_name, results, feature_names=None, top_n_words=10):\n",
    "    \"\"\"\n",
    "    Create interactive topic network visualization with actual topic labels\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset\n",
    "        results: Processing results containing LDA model and topic distributions\n",
    "        feature_names: Vocabulary for text datasets\n",
    "        top_n_words: Number of top words to show for each topic\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nCreating interactive topic network for {dataset_name}...\")\n",
    "    \n",
    "    # Extract components from results\n",
    "    lda_model = results['lda_model']\n",
    "    topic_distributions = results['topic_distributions']\n",
    "    difficulty_scores = results['difficulty_scores']\n",
    "    \n",
    "    # Get topic components\n",
    "    topic_vectors = lda_model.components_  # Shape: n_topics x n_features\n",
    "    topic_similarity = cosine_similarity(topic_vectors)\n",
    "    \n",
    "    # Calculate topic weights\n",
    "    topic_weights = np.sum(topic_distributions, axis=0)\n",
    "    topic_weights_norm = topic_weights / np.sum(topic_weights)\n",
    "    \n",
    "    # Create topic relationship graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Get actual topic labels (top words for each topic)\n",
    "    topic_labels = []\n",
    "    topic_full_labels = []  # Full label with all top words\n",
    "    topic_words_list = []   # List of top words for each topic\n",
    "    \n",
    "    for i in range(topic_vectors.shape[0]):\n",
    "        # Get top words for this topic\n",
    "        if feature_names is not None and len(feature_names) > 0:\n",
    "            topic_vector = topic_vectors[i]\n",
    "            # Get indices of top words for this topic\n",
    "            top_indices = np.argsort(topic_vector)[-top_n_words:][::-1]\n",
    "            top_words = []\n",
    "            for idx in top_indices:\n",
    "                if idx < len(feature_names):\n",
    "                    if isinstance(feature_names[idx], (int, np.integer)):\n",
    "                        # If it's an integer, it might be an index\n",
    "                        top_words.append(f\"word_{idx}\")\n",
    "                    else:\n",
    "                        top_words.append(str(feature_names[idx]))\n",
    "                else:\n",
    "                    top_words.append(f\"word_{idx}\")\n",
    "            \n",
    "            # Create short label (first 3 words)\n",
    "            short_label = f\"T{i}: {', '.join(top_words[:3])}\"\n",
    "            # Create full label\n",
    "            full_label = f\"Topic {i}<br>Top words: {', '.join(top_words)}\"\n",
    "            topic_labels.append(short_label)\n",
    "            topic_full_labels.append(full_label)\n",
    "            topic_words_list.append(top_words)\n",
    "        else:\n",
    "            # For image datasets or when no feature names\n",
    "            topic_labels.append(f\"Topic {i}\")\n",
    "            topic_full_labels.append(f\"Topic {i}\")\n",
    "            topic_words_list.append([])\n",
    "    \n",
    "    # Add nodes with topic information\n",
    "    for i in range(topic_vectors.shape[0]):\n",
    "        # Calculate average difficulty for documents where this topic is dominant\n",
    "        topic_doc_mask = np.argmax(topic_distributions, axis=1) == i\n",
    "        if np.sum(topic_doc_mask) > 0:\n",
    "            avg_difficulty = np.mean(difficulty_scores['entropy'][topic_doc_mask])\n",
    "            avg_composite = np.mean(difficulty_scores['composite'][topic_doc_mask])\n",
    "        else:\n",
    "            avg_difficulty = 0.5\n",
    "            avg_composite = 0.5\n",
    "        \n",
    "        # Node size based on topic weight\n",
    "        node_size = topic_weights_norm[i] * 100 + 10\n",
    "        \n",
    "        # Number of documents for this topic\n",
    "        n_docs = np.sum(topic_doc_mask)\n",
    "        \n",
    "        # Add to graph\n",
    "        G.add_node(i,\n",
    "                   label=topic_labels[i],\n",
    "                   full_label=topic_full_labels[i],\n",
    "                   top_words=topic_words_list[i],\n",
    "                   size=node_size,\n",
    "                   avg_difficulty=avg_difficulty,\n",
    "                   avg_composite=avg_composite,\n",
    "                   weight=topic_weights_norm[i],\n",
    "                   n_docs=n_docs,\n",
    "                   topic_id=i)\n",
    "    \n",
    "    # Add edges based on similarity\n",
    "    if len(topic_similarity) > 1:\n",
    "        # Flatten similarity matrix excluding diagonal\n",
    "        flat_similarities = topic_similarity[np.triu_indices_from(topic_similarity, k=1)]\n",
    "        if len(flat_similarities) > 0:\n",
    "            threshold = np.percentile(flat_similarities, 75)\n",
    "        else:\n",
    "            threshold = 0.5\n",
    "    else:\n",
    "        threshold = 0.5\n",
    "    \n",
    "    edges = []\n",
    "    edge_weights = []\n",
    "    \n",
    "    for i in range(topic_vectors.shape[0]):\n",
    "        for j in range(i+1, topic_vectors.shape[0]):\n",
    "            if topic_similarity[i, j] > threshold:\n",
    "                G.add_edge(i, j, weight=topic_similarity[i, j])\n",
    "                edges.append((i, j))\n",
    "                edge_weights.append(topic_similarity[i, j])\n",
    "    \n",
    "    # Use spring layout for node positions\n",
    "    if len(G.nodes()) > 0:\n",
    "        pos = nx.spring_layout(G, k=1.5, iterations=50, seed=42)\n",
    "    else:\n",
    "        print(f\"  Warning: No nodes in graph for {dataset_name}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Create edge traces\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    edge_hovertext = []\n",
    "    \n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "        \n",
    "        # Get edge weight\n",
    "        weight = G[edge[0]][edge[1]]['weight']\n",
    "        \n",
    "        # Edge hover text\n",
    "        edge_hovertext.append(\n",
    "            f\"Topic {edge[0]} ↔ Topic {edge[1]}<br>\"\n",
    "            f\"Similarity: {weight:.3f}\"\n",
    "        )\n",
    "    \n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=1.5, color='#888'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines',\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # Create node traces\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    node_text = []\n",
    "    node_hovertext = []\n",
    "    node_sizes = []\n",
    "    node_colors = []\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        \n",
    "        # Get node attributes\n",
    "        node_data = G.nodes[node]\n",
    "        node_text.append(node_data['label'])\n",
    "        \n",
    "        # Create hover text with topic words\n",
    "        if node_data['top_words']:\n",
    "            words_str = \"<br>\".join([f\"• {word}\" for word in node_data['top_words']])\n",
    "        else:\n",
    "            words_str = \"No words available\"\n",
    "        \n",
    "        hover_text = (\n",
    "            f\"<b>Topic {node_data['topic_id']}</b><br>\"\n",
    "            f\"{words_str}<br><br>\"\n",
    "            f\"Average Difficulty (Entropy): {node_data['avg_difficulty']:.3f}<br>\"\n",
    "            f\"Average Difficulty (Composite): {node_data['avg_composite']:.3f}<br>\"\n",
    "            f\"Topic Weight: {node_data['weight']:.3f}<br>\"\n",
    "            f\"Number of Documents: {node_data['n_docs']}\"\n",
    "        )\n",
    "        node_hovertext.append(hover_text)\n",
    "        \n",
    "        # Node size based on topic weight\n",
    "        node_sizes.append(node_data['size'])\n",
    "        \n",
    "        # Node color based on average difficulty\n",
    "        node_colors.append(node_data['avg_composite'])\n",
    "    \n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode='markers+text',\n",
    "        text=node_text,\n",
    "        textposition=\"top center\",\n",
    "        hoverinfo='text',\n",
    "        hovertext=node_hovertext,\n",
    "        marker=dict(\n",
    "            showscale=True,\n",
    "            colorscale='RdYlBu_r',  \n",
    "            color=node_colors,\n",
    "            size=node_sizes,\n",
    "            colorbar=dict(\n",
    "                thickness=15,\n",
    "                title='Average Difficulty<br>(Composite)',\n",
    "                xanchor='left',\n",
    "                title_side='right'\n",
    "            ),\n",
    "            line=dict(width=2, color='white')\n",
    "        ),\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                   layout=go.Layout(\n",
    "                       title=dict(\n",
    "                           text=f'Interactive Topic Network - {dataset_name}<br>'\n",
    "                                f'Node size ∝ Topic weight, Color ∝ Average difficulty',\n",
    "                           font=dict(size=16)\n",
    "                       ),\n",
    "                       showlegend=False,\n",
    "                       hovermode='closest',\n",
    "                       margin=dict(b=20, l=5, r=5, t=40),\n",
    "                       annotations=[dict(\n",
    "                           text=f\"Network Statistics:<br>\"\n",
    "                               f\"Nodes: {G.number_of_nodes()}<br>\"\n",
    "                               f\"Edges: {G.number_of_edges()}<br>\"\n",
    "                               f\"Edge threshold: {threshold:.3f}\",\n",
    "                           showarrow=False,\n",
    "                           xref=\"paper\", yref=\"paper\",\n",
    "                           x=0.02, y=0.02,\n",
    "                           bgcolor=\"white\",\n",
    "                           bordercolor=\"black\",\n",
    "                           borderwidth=1,\n",
    "                           borderpad=4\n",
    "                       )],\n",
    "                       xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                       yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
    "                   ))\n",
    "    \n",
    "    output_path = f\"results/tmcl/topic_models/{dataset_name}_interactive_network.html\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    fig.write_html(output_path)\n",
    "    print(f\"  Interactive network saved to: {output_path}\")\n",
    "    \n",
    "    # create a static version if kaleido is available\n",
    "    try:\n",
    "        fig.update_layout(\n",
    "            width=1200,\n",
    "            height=800,\n",
    "            font=dict(size=12)\n",
    "        )\n",
    "        static_output_path = f\"results/tmcl/topic_models/{dataset_name}_interactive_network_static.png\"\n",
    "        fig.write_image(static_output_path)\n",
    "        print(f\"  Static version saved to: {static_output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Note: Could not save static image. Install kaleido: pip install kaleido\")\n",
    "    \n",
    "    return fig, G\n",
    "\n",
    "def create_interactive_topic_dashboard(dataset_name, results, feature_names=None):\n",
    "    \"\"\"\n",
    "    Create comprehensive interactive dashboard for topic analysis\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset\n",
    "        results: Processing results\n",
    "        feature_names: Vocabulary for text datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nCreating interactive dashboard for {dataset_name}...\")\n",
    "    \n",
    "    # Extract components\n",
    "    lda_model = results['lda_model']\n",
    "    topic_distributions = results['topic_distributions']\n",
    "    difficulty_scores = results['difficulty_scores']\n",
    "    labels = results.get('labels', None)\n",
    "    \n",
    "    # Calculate topic statistics\n",
    "    topic_vectors = lda_model.components_\n",
    "    topic_weights = np.sum(topic_distributions, axis=0)\n",
    "    topic_weights_norm = topic_weights / np.sum(topic_weights)\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    analysis_data = []\n",
    "    for i in range(topic_vectors.shape[0]):\n",
    "        topic_doc_mask = np.argmax(topic_distributions, axis=1) == i\n",
    "        n_docs = np.sum(topic_doc_mask)\n",
    "        \n",
    "        if n_docs > 0:\n",
    "            avg_entropy = np.mean(difficulty_scores['entropy'][topic_doc_mask])\n",
    "            avg_max_prob = np.mean(difficulty_scores['max_prob'][topic_doc_mask])\n",
    "            avg_composite = np.mean(difficulty_scores['composite'][topic_doc_mask])\n",
    "            topic_strength = np.mean(topic_distributions[topic_doc_mask, i])\n",
    "            \n",
    "            # Get top words\n",
    "            top_words = []\n",
    "            if feature_names is not None and len(feature_names) > 0:\n",
    "                topic_vector = topic_vectors[i]\n",
    "                top_indices = np.argsort(topic_vector)[-10:][::-1]\n",
    "                for idx in top_indices:\n",
    "                    if idx < len(feature_names):\n",
    "                        if isinstance(feature_names[idx], (int, np.integer)):\n",
    "                            top_words.append(f\"word_{idx}\")\n",
    "                        else:\n",
    "                            top_words.append(str(feature_names[idx]))\n",
    "                    else:\n",
    "                        top_words.append(f\"word_{idx}\")\n",
    "            \n",
    "            analysis_data.append({\n",
    "                'Topic': f'T{i}',\n",
    "                'Topic_ID': i,\n",
    "                'N_Docs': n_docs,\n",
    "                'Doc_Proportion': n_docs / len(topic_distributions),\n",
    "                'Avg_Entropy_Difficulty': avg_entropy,\n",
    "                'Avg_MaxProb_Difficulty': avg_max_prob,\n",
    "                'Avg_Composite_Difficulty': avg_composite,\n",
    "                'Topic_Strength': topic_strength,\n",
    "                'Topic_Weight': topic_weights_norm[i],\n",
    "                'Top_Words': ', '.join(top_words) if top_words else 'N/A'\n",
    "            })\n",
    "    \n",
    "    analysis_df = pd.DataFrame(analysis_data)\n",
    "    \n",
    "    # Create interactive dashboard with multiple plots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Topic Difficulty Distribution',\n",
    "            'Topic Weight vs Difficulty',\n",
    "            'Difficulty Metrics by Topic',\n",
    "            'Topic Document Distribution'\n",
    "        ),\n",
    "        specs=[\n",
    "            [{'type': 'scatter'}, {'type': 'scatter'}],\n",
    "            [{'type': 'bar'}, {'type': 'pie'}]\n",
    "        ],\n",
    "        vertical_spacing=0.12,\n",
    "        horizontal_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    # Plot 1: Topic Difficulty Distribution (scatter plot)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=analysis_df['Topic'],\n",
    "            y=analysis_df['Avg_Composite_Difficulty'],\n",
    "            mode='markers+text',\n",
    "            text=analysis_df['Topic'],\n",
    "            textposition='top center',\n",
    "            marker=dict(\n",
    "                size=analysis_df['Topic_Weight'] * 100,\n",
    "                color=analysis_df['Avg_Composite_Difficulty'],\n",
    "                colorscale='RdYlBu_r',\n",
    "                showscale=True,\n",
    "                colorbar=dict(\n",
    "                    title='Difficulty',\n",
    "                    x=0.45,\n",
    "                    y=0.95,\n",
    "                    len=0.3\n",
    "                ),\n",
    "                line=dict(width=2, color='DarkSlateGrey')\n",
    "            ),\n",
    "            customdata=analysis_df[['Top_Words', 'N_Docs', 'Topic_Weight']].values,\n",
    "            hovertemplate=(\n",
    "                '<b>Topic: %{x}</b><br>'\n",
    "                'Difficulty: %{y:.3f}<br>'\n",
    "                'Top Words: %{customdata[0]}<br>'\n",
    "                'Documents: %{customdata[1]}<br>'\n",
    "                'Weight: %{customdata[2]:.3f}<br>'\n",
    "                '<extra></extra>'\n",
    "            ),\n",
    "            name='Topics'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add difficulty thresholds\n",
    "    if len(analysis_df) > 0:\n",
    "        mean_diff = analysis_df['Avg_Composite_Difficulty'].mean()\n",
    "        std_diff = analysis_df['Avg_Composite_Difficulty'].std()\n",
    "        \n",
    "        fig.add_hline(y=mean_diff, line_dash=\"dash\", line_color=\"gray\", \n",
    "                      annotation_text=f\"Mean: {mean_diff:.3f}\", \n",
    "                      annotation_position=\"top right\", row=1, col=1)\n",
    "        fig.add_hline(y=mean_diff + std_diff, line_dash=\"dot\", line_color=\"red\", \n",
    "                      annotation_text=f\"+1 std\", annotation_position=\"top right\", row=1, col=1)\n",
    "        fig.add_hline(y=mean_diff - std_diff, line_dash=\"dot\", line_color=\"green\", \n",
    "                      annotation_text=f\"-1 std\", annotation_position=\"top right\", row=1, col=1)\n",
    "    \n",
    "    # Plot 2: Topic Weight vs Difficulty (bubble chart)\n",
    "    if len(analysis_df) > 0:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=analysis_df['Topic_Weight'],\n",
    "                y=analysis_df['Avg_Composite_Difficulty'],\n",
    "                mode='markers+text',\n",
    "                text=analysis_df['Topic'],\n",
    "                textposition='top center',\n",
    "                marker=dict(\n",
    "                    size=analysis_df['N_Docs'] / max(analysis_df['N_Docs']) * 50 + 10 if max(analysis_df['N_Docs']) > 0 else 20,\n",
    "                    color=analysis_df['Avg_Composite_Difficulty'],\n",
    "                    colorscale='Viridis',\n",
    "                    showscale=False\n",
    "                ),\n",
    "                customdata=analysis_df[['Top_Words', 'N_Docs', 'Topic_ID']].values,\n",
    "                hovertemplate=(\n",
    "                    '<b>Topic: %{text}</b><br>'\n",
    "                    'Weight: %{x:.3f}<br>'\n",
    "                    'Difficulty: %{y:.3f}<br>'\n",
    "                    'Top Words: %{customdata[0]}<br>'\n",
    "                    'Documents: %{customdata[1]}<br>'\n",
    "                    'ID: %{customdata[2]}<br>'\n",
    "                    '<extra></extra>'\n",
    "                ),\n",
    "                name='Weight vs Difficulty'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(analysis_df) > 1:\n",
    "            z = np.polyfit(analysis_df['Topic_Weight'], analysis_df['Avg_Composite_Difficulty'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_range = np.linspace(analysis_df['Topic_Weight'].min(), analysis_df['Topic_Weight'].max(), 100)\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=x_range,\n",
    "                    y=p(x_range),\n",
    "                    mode='lines',\n",
    "                    line=dict(color='red', dash='dash'),\n",
    "                    name='Trend',\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "    \n",
    "    # Plot 3: Difficulty Metrics by Topic (grouped bar chart)\n",
    "    if len(analysis_df) > 0:\n",
    "        for i, metric in enumerate(['Avg_Entropy_Difficulty', 'Avg_MaxProb_Difficulty', 'Avg_Composite_Difficulty']):\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=analysis_df['Topic'],\n",
    "                    y=analysis_df[metric],\n",
    "                    name=metric.replace('Avg_', '').replace('_', ' '),\n",
    "                    marker_color=px.colors.qualitative.Set2[i],\n",
    "                    hovertemplate=(\n",
    "                        '<b>Topic: %{x}</b><br>'\n",
    "                        'Metric: %{fullData.name}<br>'\n",
    "                        'Value: %{y:.3f}<br>'\n",
    "                        '<extra></extra>'\n",
    "                    ),\n",
    "                    showlegend=True if i == 0 else False\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "    \n",
    "    # Plot 4: Topic Document Distribution (pie chart)\n",
    "    if len(analysis_df) > 0:\n",
    "        fig.add_trace(\n",
    "            go.Pie(\n",
    "                labels=analysis_df['Topic'],\n",
    "                values=analysis_df['N_Docs'],\n",
    "                textinfo='label+percent',\n",
    "                textposition='inside',\n",
    "                hole=0.4,\n",
    "                marker=dict(\n",
    "                    colors=px.colors.qualitative.Plotly,\n",
    "                    line=dict(color='white', width=2)\n",
    "                ),\n",
    "                hovertemplate=(\n",
    "                    '<b>Topic: %{label}</b><br>'\n",
    "                    'Documents: %{value}<br>'\n",
    "                    'Percentage: %{percent}<br>'\n",
    "                    '<extra></extra>'\n",
    "                ),\n",
    "                name='Document Distribution'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout with corrected title property\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f'Interactive Topic Analysis Dashboard - {dataset_name}',\n",
    "            font=dict(size=20)\n",
    "        ),\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02,\n",
    "            bgcolor='rgba(255, 255, 255, 0.8)',\n",
    "            bordercolor='black',\n",
    "            borderwidth=1\n",
    "        ),\n",
    "        hovermode='closest',\n",
    "        height=1000,\n",
    "        width=1400,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Topic\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Composite Difficulty\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Topic Weight\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Composite Difficulty\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Topic\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Difficulty Score\", row=2, col=1)\n",
    "    \n",
    "    # Save dashboard\n",
    "    dashboard_path = f\"results/tmcl/topic_models/{dataset_name}_interactive_dashboard.html\"\n",
    "    os.makedirs(os.path.dirname(dashboard_path), exist_ok=True)\n",
    "    fig.write_html(dashboard_path)\n",
    "    print(f\"  Interactive dashboard saved to: {dashboard_path}\")\n",
    "    \n",
    "    # Also save static version if kaleido is available\n",
    "    try:\n",
    "        static_dashboard_path = f\"results/tmcl/topic_models/{dataset_name}_interactive_dashboard.png\"\n",
    "        fig.write_image(static_dashboard_path, width=1400, height=1000)\n",
    "        print(f\"  Static dashboard saved to: {static_dashboard_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Note: Could not save static image. Install kaleido: pip install kaleido\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_topic_evolution_timeline(dataset_name, results, feature_names=None, n_samples_per_topic=100):\n",
    "    \"\"\"\n",
    "    Create interactive timeline showing topic evolution and difficulty\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset\n",
    "        results: Processing results\n",
    "        feature_names: Vocabulary for text datasets\n",
    "        n_samples_per_topic: Number of samples to show per topic\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nCreating topic evolution timeline for {dataset_name}...\")\n",
    "    \n",
    "    # Extract components\n",
    "    lda_model = results['lda_model']\n",
    "    topic_distributions = results['topic_distributions']\n",
    "    difficulty_scores = results['difficulty_scores']\n",
    "    \n",
    "    # Get topic components\n",
    "    topic_vectors = lda_model.components_\n",
    "    n_topics = topic_vectors.shape[0]\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Get top words for each topic for hover text\n",
    "    topic_words = []\n",
    "    for i in range(n_topics):\n",
    "        if feature_names is not None and len(feature_names) > 0:\n",
    "            topic_vector = topic_vectors[i]\n",
    "            top_indices = np.argsort(topic_vector)[-5:][::-1]\n",
    "            words = []\n",
    "            for idx in top_indices:\n",
    "                if idx < len(feature_names):\n",
    "                    if isinstance(feature_names[idx], (int, np.integer)):\n",
    "                        words.append(f\"word_{idx}\")\n",
    "                    else:\n",
    "                        words.append(str(feature_names[idx]))\n",
    "                else:\n",
    "                    words.append(f\"word_{idx}\")\n",
    "            topic_words.append(words)\n",
    "        else:\n",
    "            topic_words.append([])\n",
    "    \n",
    "    # For each topic, create a timeline trace\n",
    "    colors = px.colors.qualitative.Plotly\n",
    "    \n",
    "    for i in range(n_topics):\n",
    "        # Get indices of documents where this topic is dominant\n",
    "        topic_doc_mask = np.argmax(topic_distributions, axis=1) == i\n",
    "        \n",
    "        if np.sum(topic_doc_mask) > 0:\n",
    "            # Get subset of documents for this topic\n",
    "            topic_doc_indices = np.where(topic_doc_mask)[0]\n",
    "            \n",
    "            if len(topic_doc_indices) > n_samples_per_topic:\n",
    "                # Sample documents\n",
    "                sampled_indices = np.random.choice(topic_doc_indices, n_samples_per_topic, replace=False)\n",
    "            else:\n",
    "                sampled_indices = topic_doc_indices\n",
    "            \n",
    "            # Get topic strengths and difficulties for these documents\n",
    "            topic_strengths = topic_distributions[sampled_indices, i]\n",
    "            doc_difficulties = difficulty_scores['composite'][sampled_indices]\n",
    "            \n",
    "            # Sort by topic strength\n",
    "            sort_idx = np.argsort(topic_strengths)\n",
    "            topic_strengths = topic_strengths[sort_idx]\n",
    "            doc_difficulties = doc_difficulties[sort_idx]\n",
    "            \n",
    "            # Create hover text\n",
    "            hover_texts = []\n",
    "            for j, idx in enumerate(sampled_indices[sort_idx]):\n",
    "                hover_text = (\n",
    "                    f\"<b>Topic {i}</b><br>\"\n",
    "                    f\"Document {idx}<br>\"\n",
    "                    f\"Topic Strength: {topic_strengths[j]:.3f}<br>\"\n",
    "                    f\"Difficulty: {doc_difficulties[j]:.3f}\"\n",
    "                )\n",
    "                if topic_words[i]:\n",
    "                    hover_text += f\"<br>Top words: {', '.join(topic_words[i])}\"\n",
    "                hover_texts.append(hover_text)\n",
    "            \n",
    "            # Add trace for this topic\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=topic_strengths,\n",
    "                y=doc_difficulties,\n",
    "                mode='markers',\n",
    "                name=f'Topic {i}',\n",
    "                text=hover_texts,\n",
    "                hoverinfo='text',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    opacity=0.6,\n",
    "                    color=colors[i % len(colors)],\n",
    "                    line=dict(width=1, color='white')\n",
    "                ),\n",
    "                showlegend=True\n",
    "            ))\n",
    "    \n",
    "    # Update layout with corrected title property\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f'Topic Evolution Timeline - {dataset_name}',\n",
    "            font=dict(size=16)\n",
    "        ),\n",
    "        xaxis_title='Topic Strength (How dominant the topic is)',\n",
    "        yaxis_title='Document Difficulty (Composite)',\n",
    "        legend_title='Topics',\n",
    "        hovermode='closest',\n",
    "        height=800,\n",
    "        width=1200,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    # Add trend line for each topic\n",
    "    for i, trace in enumerate(fig.data):\n",
    "        topic_strengths = trace.x\n",
    "        difficulties = trace.y\n",
    "        \n",
    "        if len(topic_strengths) > 1:\n",
    "            # Calculate trend line\n",
    "            z = np.polyfit(topic_strengths, difficulties, 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_range = np.linspace(min(topic_strengths), max(topic_strengths), 100)\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x_range,\n",
    "                y=p(x_range),\n",
    "                mode='lines',\n",
    "                line=dict(color=trace.marker.color, dash='dash', width=1),\n",
    "                showlegend=False,\n",
    "                hoverinfo='skip',\n",
    "                name=f'Trend Topic {i}'\n",
    "            ))\n",
    "    \n",
    "    # Save timeline\n",
    "    timeline_path = f\"results/tmcl/topic_models/{dataset_name}_topic_timeline.html\"\n",
    "    os.makedirs(os.path.dirname(timeline_path), exist_ok=True)\n",
    "    fig.write_html(timeline_path)\n",
    "    print(f\"  Topic timeline saved to: {timeline_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_interactive_topic_comparison(all_analysis_results):\n",
    "    \"\"\"\n",
    "    Create interactive comparison of all datasets\n",
    "    \n",
    "    Args:\n",
    "        all_analysis_results: Dictionary containing analysis results for all datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nCreating interactive comparison across all datasets...\")\n",
    "    \n",
    "    # Collect comparison data\n",
    "    comparison_data = []\n",
    "    \n",
    "    for dataset_name, analysis in all_analysis_results.items():\n",
    "        if analysis is not None:\n",
    "            topic_analysis = analysis.get('topic_analysis', pd.DataFrame())\n",
    "            difficulty_stats = analysis.get('difficulty_stats', {})\n",
    "            network_stats = analysis.get('network_stats', {})\n",
    "            \n",
    "            if not topic_analysis.empty:\n",
    "                # Get difficulty range\n",
    "                if 'Avg_Composite_Difficulty' in topic_analysis.columns:\n",
    "                    difficulty_range = topic_analysis['Avg_Composite_Difficulty'].max() - topic_analysis['Avg_Composite_Difficulty'].min()\n",
    "                else:\n",
    "                    difficulty_range = 0\n",
    "                \n",
    "                comparison_data.append({\n",
    "                    'Dataset': dataset_name,\n",
    "                    'Type': 'Text' if dataset_name in ['ag_news', 'imdb'] else 'Image',\n",
    "                    'N_Topics': analysis.get('n_topics', 0),\n",
    "                    'N_Documents': analysis.get('n_documents', 0),\n",
    "                    'Perplexity': analysis.get('perplexity', 0),\n",
    "                    'Avg_Difficulty': difficulty_stats.get('overall_mean_composite', 0),\n",
    "                    'Std_Difficulty': difficulty_stats.get('overall_std_composite', 0),\n",
    "                    'Network_Density': network_stats.get('density', 0),\n",
    "                    'Difficulty_Range': difficulty_range\n",
    "                })\n",
    "    \n",
    "    if not comparison_data:\n",
    "        print(\"  No comparison data available\")\n",
    "        return None\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Create interactive comparison dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=3,\n",
    "        subplot_titles=(\n",
    "            'Average Difficulty by Dataset',\n",
    "            'Number of Topics vs Documents',\n",
    "            'Network Density vs Difficulty',\n",
    "            'Perplexity Comparison',\n",
    "            'Difficulty Range Comparison',\n",
    "            'Dataset Clustering'\n",
    "        ),\n",
    "        specs=[\n",
    "            [{'type': 'bar'}, {'type': 'scatter'}, {'type': 'scatter'}],\n",
    "            [{'type': 'bar'}, {'type': 'bar'}, {'type': 'scatter'}]\n",
    "        ],\n",
    "        vertical_spacing=0.15,\n",
    "        horizontal_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    # Plot 1: Average difficulty by dataset\n",
    "    colors = px.colors.qualitative.Set2\n",
    "    for i, (_, row) in enumerate(comparison_df.iterrows()):\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=[row['Dataset']],\n",
    "                y=[row['Avg_Difficulty']],\n",
    "                name=row['Dataset'],\n",
    "                marker_color=colors[i % len(colors)],\n",
    "                error_y=dict(\n",
    "                    type='data',\n",
    "                    array=[row['Std_Difficulty']],\n",
    "                    visible=True\n",
    "                ),\n",
    "                customdata=[[\n",
    "                    row['Type'],\n",
    "                    row['N_Topics'],\n",
    "                    row['N_Documents'],\n",
    "                    row['Perplexity']\n",
    "                ]],\n",
    "                hovertemplate=(\n",
    "                    '<b>%{x}</b><br>'\n",
    "                    'Type: %{customdata[0]}<br>'\n",
    "                    'Difficulty: %{y:.3f} ± %{error_y.array[0]:.3f}<br>'\n",
    "                    'Topics: %{customdata[1]}<br>'\n",
    "                    'Documents: %{customdata[2]:,}<br>'\n",
    "                    'Perplexity: %{customdata[3]:.2f}<br>'\n",
    "                    '<extra></extra>'\n",
    "                ),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Plot 2: Number of topics vs documents\n",
    "    if len(comparison_df) > 0:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=comparison_df['N_Topics'],\n",
    "                y=comparison_df['N_Documents'] / 1000,  # Convert to thousands\n",
    "                mode='markers+text',\n",
    "                text=comparison_df['Dataset'],\n",
    "                textposition='top center',\n",
    "                marker=dict(\n",
    "                    size=comparison_df['Avg_Difficulty'] * 50,\n",
    "                    color=comparison_df['Avg_Difficulty'],\n",
    "                    colorscale='RdYlBu_r',\n",
    "                    showscale=True,\n",
    "                    colorbar=dict(title='Avg Difficulty', x=0.47, y=0.95, len=0.25),\n",
    "                    line=dict(width=2, color='black')\n",
    "                ),\n",
    "                customdata=comparison_df[['Type', 'Perplexity', 'Network_Density']].values,\n",
    "                hovertemplate=(\n",
    "                    '<b>%{text}</b><br>'\n",
    "                    'Topics: %{x}<br>'\n",
    "                    'Documents (thousands): %{y:.1f}<br>'\n",
    "                    'Type: %{customdata[0]}<br>'\n",
    "                    'Perplexity: %{customdata[1]:.2f}<br>'\n",
    "                    'Network Density: %{customdata[2]:.3f}<br>'\n",
    "                    '<extra></extra>'\n",
    "                ),\n",
    "                name='Topics vs Documents'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Plot 3: Network density vs difficulty\n",
    "    if len(comparison_df) > 0:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=comparison_df['Network_Density'],\n",
    "                y=comparison_df['Avg_Difficulty'],\n",
    "                mode='markers+text',\n",
    "                text=comparison_df['Dataset'],\n",
    "                textposition='top center',\n",
    "                marker=dict(\n",
    "                    size=comparison_df['N_Topics'] * 10,\n",
    "                    color=comparison_df['Type'].map({'Text': 'blue', 'Image': 'red'}),\n",
    "                    symbol=comparison_df['Type'].map({'Text': 'circle', 'Image': 'square'}),\n",
    "                    line=dict(width=2, color='black')\n",
    "                ),\n",
    "                customdata=comparison_df[['N_Documents', 'Perplexity', 'Type']].values,\n",
    "                hovertemplate=(\n",
    "                    '<b>%{text}</b><br>'\n",
    "                    'Network Density: %{x:.3f}<br>'\n",
    "                    'Avg Difficulty: %{y:.3f}<br>'\n",
    "                    'Type: %{customdata[2]}<br>'\n",
    "                    'Documents: %{customdata[0]:,}<br>'\n",
    "                    'Perplexity: %{customdata[1]:.2f}<br>'\n",
    "                    '<extra></extra>'\n",
    "                ),\n",
    "                name='Network vs Difficulty'\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "    \n",
    "    # Plot 4: Perplexity comparison\n",
    "    if len(comparison_df) > 0:\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=comparison_df['Dataset'],\n",
    "                y=comparison_df['Perplexity'],\n",
    "                marker_color=comparison_df['Type'].map({'Text': 'lightblue', 'Image': 'lightcoral'}),\n",
    "                customdata=comparison_df[['Type', 'N_Topics', 'Avg_Difficulty']].values,\n",
    "                hovertemplate=(\n",
    "                    '<b>%{x}</b><br>'\n",
    "                    'Perplexity: %{y:.2f}<br>'\n",
    "                    'Type: %{customdata[0]}<br>'\n",
    "                    'Topics: %{customdata[1]}<br>'\n",
    "                    'Avg Difficulty: %{customdata[2]:.3f}<br>'\n",
    "                    '<extra></extra>'\n",
    "                ),\n",
    "                name='Perplexity',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Plot 5: Difficulty range comparison\n",
    "    if len(comparison_df) > 0:\n",
    "        # Normalize difficulty range for coloring\n",
    "        if comparison_df['Difficulty_Range'].max() > comparison_df['Difficulty_Range'].min():\n",
    "            norm_range = (comparison_df['Difficulty_Range'] - comparison_df['Difficulty_Range'].min()) / \\\n",
    "                        (comparison_df['Difficulty_Range'].max() - comparison_df['Difficulty_Range'].min())\n",
    "        else:\n",
    "            norm_range = pd.Series([0.5] * len(comparison_df))\n",
    "        \n",
    "        # Use Plotly colorscale\n",
    "        colorscale = px.colors.sequential.Viridis\n",
    "        colors = [colorscale[int(val * (len(colorscale)-1))] for val in norm_range]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=comparison_df['Dataset'],\n",
    "                y=comparison_df['Difficulty_Range'],\n",
    "                marker_color=colors,\n",
    "                customdata=comparison_df[['Type', 'Avg_Difficulty', 'Std_Difficulty']].values,\n",
    "                hovertemplate=(\n",
    "                    '<b>%{x}</b><br>'\n",
    "                    'Difficulty Range: %{y:.3f}<br>'\n",
    "                    'Type: %{customdata[0]}<br>'\n",
    "                    'Avg Difficulty: %{customdata[1]:.3f}<br>'\n",
    "                    'Std Difficulty: %{customdata[2]:.3f}<br>'\n",
    "                    '<extra></extra>'\n",
    "                ),\n",
    "                name='Difficulty Range',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Plot 6: Dataset clustering (simplified)\n",
    "    if len(comparison_df) > 1:\n",
    "        try:\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            from sklearn.decomposition import PCA\n",
    "            \n",
    "            # Prepare data for PCA\n",
    "            cluster_data = comparison_df[['Avg_Difficulty', 'Std_Difficulty', \n",
    "                                         'Network_Density', 'Difficulty_Range', \n",
    "                                         'Perplexity']].values\n",
    "            cluster_data = StandardScaler().fit_transform(cluster_data)\n",
    "            \n",
    "            # Apply PCA\n",
    "            pca = PCA(n_components=2)\n",
    "            pca_result = pca.fit_transform(cluster_data)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=pca_result[:, 0],\n",
    "                    y=pca_result[:, 1],\n",
    "                    mode='markers+text',\n",
    "                    text=comparison_df['Dataset'],\n",
    "                    textposition='top center',\n",
    "                    marker=dict(\n",
    "                        size=20,\n",
    "                        color=comparison_df['Type'].map({'Text': 'blue', 'Image': 'red'}),\n",
    "                        symbol=comparison_df['Type'].map({'Text': 'circle', 'Image': 'square'}),\n",
    "                        line=dict(width=2, color='black')\n",
    "                    ),\n",
    "                    customdata=comparison_df[['Type', 'N_Topics', 'N_Documents']].values,\n",
    "                    hovertemplate=(\n",
    "                        '<b>%{text}</b><br>'\n",
    "                        'PCA 1: %{x:.2f}<br>'\n",
    "                        'PCA 2: %{y:.2f}<br>'\n",
    "                        'Type: %{customdata[0]}<br>'\n",
    "                        'Topics: %{customdata[1]}<br>'\n",
    "                        'Documents: %{customdata[2]:,}<br>'\n",
    "                        '<extra></extra>'\n",
    "                    ),\n",
    "                    name='Dataset Clusters'\n",
    "                ),\n",
    "                row=2, col=3\n",
    "            )\n",
    "            \n",
    "            # Add variance explained to axis labels\n",
    "            fig.update_xaxes(\n",
    "                title_text=f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)',\n",
    "                row=2, col=3\n",
    "            )\n",
    "            fig.update_yaxes(\n",
    "                title_text=f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)',\n",
    "                row=2, col=3\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not create clustering plot: {e}\")\n",
    "            fig.add_annotation(\n",
    "                text=\"Clustering not available\",\n",
    "                xref=\"x domain\", yref=\"y domain\",\n",
    "                x=0.5, y=0.5,\n",
    "                showarrow=False,\n",
    "                font=dict(size=14),\n",
    "                row=2, col=3\n",
    "            )\n",
    "    else:\n",
    "        fig.add_annotation(\n",
    "            text=\"Need at least 2 datasets for clustering\",\n",
    "            xref=\"x domain\", yref=\"y domain\",\n",
    "            x=0.5, y=0.5,\n",
    "            showarrow=False,\n",
    "            font=dict(size=14),\n",
    "            row=2, col=3\n",
    "        )\n",
    "    \n",
    "    # Update layout with corrected title property\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text='Interactive Dataset Comparison Dashboard',\n",
    "            font=dict(size=20)\n",
    "        ),\n",
    "        showlegend=False,\n",
    "        height=1000,\n",
    "        width=1600,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Dataset\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Average Difficulty\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Number of Topics\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Documents (thousands)\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Network Density\", row=1, col=3)\n",
    "    fig.update_yaxes(title_text=\"Average Difficulty\", row=1, col=3)\n",
    "    fig.update_xaxes(title_text=\"Dataset\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Perplexity\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Dataset\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Difficulty Range\", row=2, col=2)\n",
    "    \n",
    "    # Save comparison results for the dashboard\n",
    "    comparison_path = \"results/tmcl/topic_models/interactive_dataset_comparison.html\"\n",
    "    os.makedirs(os.path.dirname(comparison_path), exist_ok=True)\n",
    "    fig.write_html(comparison_path)\n",
    "    print(f\"  Interactive comparison dashboard saved to: {comparison_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_all_interactive_visualizations():\n",
    "    \"\"\"\n",
    "    Create all interactive visualizations for the analyzed datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CREATING INTERACTIVE TOPIC VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Try to import plotly for image export\n",
    "    try:\n",
    "        import plotly.io as pio\n",
    "        pio.kaleido.scope.default_format = \"png\"\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Load all results\n",
    "    datasets = ['ag_news', 'imdb', 'cifar10', 'cifar100', 'mnist', 'fashion_mnist']\n",
    "    all_analysis_results = {}\n",
    "    \n",
    "    # Create visualizations for each dataset\n",
    "    for dataset_name in datasets:\n",
    "        print(f\"\\nProcessing {dataset_name}...\")\n",
    "        \n",
    "        # Load results\n",
    "        results_path = f\"results/tmcl/topic_models/{dataset_name}_tmcl_results.pkl\"\n",
    "        if os.path.exists(results_path):\n",
    "            try:\n",
    "                with open(results_path, 'rb') as f:\n",
    "                    results = pickle.load(f)\n",
    "                \n",
    "                print(f\"  Successfully loaded results from {results_path}\")\n",
    "                \n",
    "                # Get feature names for the text datasets\n",
    "                feature_names = None\n",
    "                if dataset_name in ['ag_news', 'imdb']:\n",
    "                    if 'vocabulary' in results:\n",
    "                        feature_names = results['vocabulary']\n",
    "                        print(f\"  Using saved vocabulary of size {len(feature_names)}\")\n",
    "                    elif 'vectorizer' in results:\n",
    "                        # Try to get vocabulary from the vectorizer\n",
    "                        vectorizer = results['vectorizer']\n",
    "                        if hasattr(vectorizer, 'get_feature_names_out'):\n",
    "                            feature_names = vectorizer.get_feature_names_out()\n",
    "                            print(f\"  Using vectorizer vocabulary of size {len(feature_names)}\")\n",
    "                        elif hasattr(vectorizer, 'get_feature_names'):\n",
    "                            feature_names = vectorizer.get_feature_names()\n",
    "                            print(f\"  Using vectorizer vocabulary of size {len(feature_names)}\")\n",
    "                    else:\n",
    "                        # Try to load from a vocabulary file\n",
    "                        vocab_path = f\"results/tmcl/topic_models/{dataset_name}_vocabulary.pkl\"\n",
    "                        if os.path.exists(vocab_path):\n",
    "                            with open(vocab_path, 'rb') as f:\n",
    "                                feature_names = pickle.load(f)\n",
    "                            print(f\"  Loaded vocabulary from file of size {len(feature_names)}\")\n",
    "                        else:\n",
    "                            # Create a sample vocabulary\n",
    "                            if 'feature_matrix' in results:\n",
    "                                n_features = results['feature_matrix'].shape[1]\n",
    "                            else:\n",
    "                                n_features = 1000\n",
    "                            feature_names = [f'feature_{i}' for i in range(n_features)]\n",
    "                            print(f\"  WARNING: Using placeholder feature names (n={n_features})\")\n",
    "                \n",
    "                # 1. Create interactive topic network\n",
    "                try:\n",
    "                    network_fig, network_graph = create_interactive_topic_network(\n",
    "                        dataset_name, results, feature_names\n",
    "                    )\n",
    "                    print(f\"  ✓ Created interactive topic network\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error creating network: {e}\")\n",
    "                \n",
    "                # 2. Create interactive dashboard\n",
    "                try:\n",
    "                    dashboard_fig = create_interactive_topic_dashboard(\n",
    "                        dataset_name, results, feature_names\n",
    "                    )\n",
    "                    print(f\"  ✓ Created interactive dashboard\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error creating dashboard: {e}\")\n",
    "                \n",
    "                # 3. Create topic evolution timeline\n",
    "                try:\n",
    "                    timeline_fig = create_topic_evolution_timeline(\n",
    "                        dataset_name, results, feature_names\n",
    "                    )\n",
    "                    print(f\"  ✓ Created topic evolution timeline\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error creating timeline: {e}\")\n",
    "                \n",
    "                enhanced_path = f\"results/tmcl/topic_models/{dataset_name}_enhanced_analysis.pkl\"\n",
    "                if os.path.exists(enhanced_path):\n",
    "                    try:\n",
    "                        with open(enhanced_path, 'rb') as f:\n",
    "                            all_analysis_results[dataset_name] = pickle.load(f)\n",
    "                        print(f\"  ✓ Loaded enhanced analysis results\")\n",
    "                    except:\n",
    "                        print(f\"  ✗ Could not load enhanced analysis results\")\n",
    "                else:\n",
    "                    print(f\"  Note: Enhanced analysis results not found at {enhanced_path}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error loading results for {dataset_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"  ✗ Results file not found for {dataset_name}\")\n",
    "    \n",
    "    # 4. Create interactive comparison across all the datasets \n",
    "    if all_analysis_results:\n",
    "        try:\n",
    "            comparison_fig = create_interactive_topic_comparison(all_analysis_results)\n",
    "            if comparison_fig:\n",
    "                print(f\"  ✓ Created interactive comparison across datasets\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error creating comparison: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INTERACTIVE VISUALIZATIONS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Print summary of the created files\n",
    "    print(\"\\nCreated interactive visualizations:\")\n",
    "    print(\"-\" * 40)\n",
    "    for dataset_name in datasets:\n",
    "        print(f\"\\n{dataset_name}:\")\n",
    "        network_file = f\"results/tmcl/topic_models/{dataset_name}_interactive_network.html\"\n",
    "        if os.path.exists(network_file):\n",
    "            print(f\"  ✓ Network: {network_file}\")\n",
    "        \n",
    "        dashboard_file = f\"results/tmcl/topic_models/{dataset_name}_interactive_dashboard.html\"\n",
    "        if os.path.exists(dashboard_file):\n",
    "            print(f\"  ✓ Dashboard: {dashboard_file}\")\n",
    "        \n",
    "        timeline_file = f\"results/tmcl/topic_models/{dataset_name}_topic_timeline.html\"\n",
    "        if os.path.exists(timeline_file):\n",
    "            print(f\"  ✓ Timeline: {timeline_file}\")\n",
    "    \n",
    "    comparison_file = \"results/tmcl/topic_models/interactive_dataset_comparison.html\"\n",
    "    if os.path.exists(comparison_file):\n",
    "        print(f\"\\nCross-dataset comparison: {comparison_file}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"To view interactive visualizations:\")\n",
    "    print(\"1. Open the HTML files in a web browser\")\n",
    "    print(\"2. Hover over nodes/points to see details\")\n",
    "    print(\"3. Use zoom and pan to explore the visualizations\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Additional instructions for text datasets\n",
    "    print(\"\\nFor text datasets (ag_news, imdb):\")\n",
    "    print(\"• Topic labels show T0, T1, etc. with top words\")\n",
    "    print(\"• Hover over nodes to see all top words for each topic\")\n",
    "    print(\"• Word indices are shown if actual words are not available\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Execute the interactive visualizations\n",
    "if __name__ == \"__main__\":\n",
    "    create_all_interactive_visualizations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedc1bef",
   "metadata": {},
   "source": [
    "\n",
    "### 8. Summary and Next Steps\n",
    " \n",
    "#### What We've Accomplished\n",
    " \n",
    "1. **Data Loading**: Successfully loaded all 6 datasets (4 vision, 2 NLP)\n",
    "2. **Feature Extraction**:\n",
    "    - Text: TF-IDF vectorization with n-grams and stopword removal\n",
    "    - Images: Deep feature extraction using pre-trained ResNet-18\n",
    "3. **Topic Modeling**: Applied LDA to learn latent topic structures\n",
    "4. **Difficulty Scoring**: Computed multiple difficulty metrics based on topic distributions\n",
    "5. **Validation**: Performed preliminary validation of difficulty scores\n",
    " \n",
    "#### Mathematical Transformations Applied\n",
    " \n",
    "1. **Text Processing**:\n",
    "\n",
    "$$\n",
    "  \\mathbf{x}_i \\rightarrow \\text{TF-IDF}(\\mathbf{x}_i) \\rightarrow \\text{LDA}(\\text{TF-IDF}(\\mathbf{x}_i)) \\rightarrow P(t|\\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "2. **Image Processing**:\n",
    "\n",
    "$$\n",
    "  \\mathbf{x}_i \\rightarrow \\text{ResNet-18}(\\mathbf{x}_i) \\rightarrow \\text{Normalize}(\\mathbf{f}_i) \\rightarrow \\text{LDA}(\\mathbf{f}_i) \\rightarrow P(t|\\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "3. **Difficulty Calculation**:\n",
    "\n",
    "$$\n",
    "  D_{\\text{entropy}}(\\mathbf{x}_i) = -\\sum_{t=1}^T P(t|\\mathbf{x}_i) \\log P(t|\\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "#### Convergence Toward TMCL Experiments\n",
    "\n",
    "The results generated in this notebook provide the foundation for the TMCL experiments described in the research plan:\n",
    " \n",
    "  1. **Curriculum Construction**: The difficulty scores can now be used to sort samples and create easy-to-hard curricula\n",
    "  2. **Training Comparison**: These topic-modeled difficulties can be compared against heuristic baselines\n",
    "  3. **Ablation Studies**: Different difficulty metrics (entropy, max_prob, composite) can be evaluated\n",
    "  4. **Cross-domain Analysis**: Results across vision and NLP domains can be compared\n",
    " \n",
    "#### Next Steps for Complete TMCL Implementation\n",
    " \n",
    "  1. **Neural Network Training**: Implement training loops with curriculum scheduling\n",
    "  2. **Schedule Functions**: Implement linear, root, and exponential curriculum schedules\n",
    "  3. **Baseline Comparisons**: Implement heuristic CL and self-paced learning baselines\n",
    "  4. **Comprehensive Evaluation**: Run full experiments measuring convergence speed, generalization gap, and training stability\n",
    "  5. **Hyperparameter Analysis**: Study the effect of number of topics, schedule parameters, etc.\n",
    " \n",
    "The topic modeling results saved in the `results/tmcl/topic_models/` directory contain all the necessary information to proceed with these next steps. Each pickle file contains:\n",
    "  - Topic distributions for every sample\n",
    "  - Multiple difficulty scores\n",
    "  - Feature matrices and models for reproducibility\n",
    "  - Label information for analysis\n",
    " \n",
    "This completes the first major phase of the TMCL research pipeline - creating the unsupervised, data-driven difficulty metric that will guide the curriculum learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81721a4",
   "metadata": {},
   "source": [
    "#### Unused Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "207c4040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import networkx as nx\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.manifold import TSNE\n",
    "# import warnings\n",
    "# import os\n",
    "# import pickle\n",
    "# from scipy import stats\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # For better visualization\n",
    "# plt.style.use('seaborn-v0_8-darkgrid')\n",
    "# sns.set_palette(\"husl\")\n",
    "\n",
    "# def enhanced_topic_analysis(dataset_name, results, feature_names=None, original_texts=None):\n",
    "#     \"\"\"\n",
    "#     Enhanced topic analysis with comprehensive visualizations\n",
    "    \n",
    "#     Args:\n",
    "#         dataset_name: Name of the dataset\n",
    "#         results: Processing results containing LDA model and topic distributions\n",
    "#         feature_names: Vocabulary for text datasets\n",
    "#         original_texts: Original text samples (for text datasets)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"ENHANCED TOPIC ANALYSIS: {dataset_name.upper()}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "    \n",
    "#     # Extract components from results\n",
    "#     lda_model = results['lda_model']\n",
    "#     topic_distributions = results['topic_distributions']\n",
    "#     difficulty_scores = results['difficulty_scores']\n",
    "#     labels = results.get('labels', None)\n",
    "    \n",
    "#     # 1. TOPIC QUALITY ANALYSIS\n",
    "#     print(\"\\n1. TOPIC QUALITY ANALYSIS\")\n",
    "#     print(\"-\" * 40)\n",
    "    \n",
    "#     # Try to get perplexity from the model or results\n",
    "#     perplexity = None\n",
    "#     if 'perplexity' in results:\n",
    "#         perplexity = results['perplexity']\n",
    "#         print(f\"Perplexity: {perplexity:.4f}\")\n",
    "#     elif hasattr(lda_model, 'perplexity'):\n",
    "#         # Try to calculate perplexity if possible\n",
    "#         try:\n",
    "#             if hasattr(lda_model, 'perplexity') and callable(lda_model.perplexity):\n",
    "#                 if 'feature_matrix' in results:\n",
    "#                     dtm = results['feature_matrix']\n",
    "#                     perplexity = lda_model.perplexity(dtm)\n",
    "#                     print(f\"Perplexity: {perplexity:.4f}\")\n",
    "#         except:\n",
    "#             pass\n",
    "    \n",
    "#     # Topic significance (based on topic distribution)\n",
    "#     topic_weights = np.sum(topic_distributions, axis=0)\n",
    "#     topic_weights_norm = topic_weights / np.sum(topic_weights)\n",
    "#     print(f\"\\nTopic Weight Distribution:\")\n",
    "#     print(f\"  Mean weight: {np.mean(topic_weights_norm):.4f}\")\n",
    "#     print(f\"  Std weight: {np.std(topic_weights_norm):.4f}\")\n",
    "#     print(f\"  Min weight: {np.min(topic_weights_norm):.4f}\")\n",
    "#     print(f\"  Max weight: {np.max(topic_weights_norm):.4f}\")\n",
    "    \n",
    "#     # 2. TOPIC VISUALIZATION WITH pyLDAvis (for text datasets)\n",
    "#     if feature_names is not None:\n",
    "#         print(\"\\n2. GENERATING INTERACTIVE TOPIC VISUALIZATION...\")\n",
    "#         try:\n",
    "#             # Try to import pyLDAvis\n",
    "#             import pyLDAvis\n",
    "#             import pyLDAvis.sklearn as ldavis\n",
    "            \n",
    "#             # Prepare data for pyLDAvis\n",
    "#             dtm = results.get('feature_matrix')\n",
    "#             if dtm is not None and hasattr(lda_model, 'components_'):\n",
    "#                 # Create visualization data using sklearn's prepare function\n",
    "#                 vis_data = ldavis.prepare(\n",
    "#                     lda_model, \n",
    "#                     dtm, \n",
    "#                     feature_names,\n",
    "#                     mds='tsne'\n",
    "#                 )\n",
    "                \n",
    "#                 # Save as HTML for interactive visualization\n",
    "#                 output_path = f\"results/tmcl/topic_models/{dataset_name}_lda_vis.html\"\n",
    "#                 pyLDAvis.save_html(vis_data, output_path)\n",
    "#                 print(f\"  Interactive visualization saved to: {output_path}\")\n",
    "                \n",
    "#                 # Extract topic coordinates for static visualization\n",
    "#                 topic_coordinates = vis_data.topic_coordinates\n",
    "                \n",
    "#                 # Create static visualization of topics\n",
    "#                 plt.figure(figsize=(12, 10))\n",
    "#                 plt.scatter(topic_coordinates['x'], topic_coordinates['y'], \n",
    "#                           s=topic_weights_norm*5000, alpha=0.7, \n",
    "#                           c=range(len(topic_coordinates)), cmap='tab20')\n",
    "                \n",
    "#                 # Add topic labels\n",
    "#                 for i, (x, y) in enumerate(zip(topic_coordinates['x'], topic_coordinates['y'])):\n",
    "#                     plt.text(x, y, f'T{i}', fontsize=12, ha='center', va='center',\n",
    "#                            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "                \n",
    "#                 plt.title(f'Topic Visualization - {dataset_name}', fontsize=16)\n",
    "#                 plt.xlabel('Dimension 1', fontsize=12)\n",
    "#                 plt.ylabel('Dimension 2', fontsize=12)\n",
    "#                 plt.colorbar(label='Topic Index')\n",
    "#                 plt.grid(True, alpha=0.3)\n",
    "                \n",
    "#                 output_path = f\"results/tmcl/topic_models/{dataset_name}_topic_scatter.png\"\n",
    "#                 plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "#                 plt.close()\n",
    "#                 print(f\"  Topic scatter plot saved to: {output_path}\")\n",
    "                \n",
    "#         except ImportError:\n",
    "#             print(\"  pyLDAvis not installed. Skipping interactive visualization.\")\n",
    "#             print(\"  Install with: pip install pyLDAvis\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"  Could not generate pyLDAvis visualization: {type(e).__name__}: {str(e)}\")\n",
    "    \n",
    "#     # 3. TOPIC RELATIONSHIP NETWORK\n",
    "#     print(\"\\n3. ANALYZING TOPIC RELATIONSHIPS...\")\n",
    "    \n",
    "#     # Calculate topic-topic similarity matrix\n",
    "#     topic_vectors = lda_model.components_  # Shape: n_topics x n_features\n",
    "#     topic_similarity = cosine_similarity(topic_vectors)\n",
    "    \n",
    "#     # Create topic relationship graph\n",
    "#     G = nx.Graph()\n",
    "    \n",
    "#     # Add nodes with topic information\n",
    "#     for i in range(topic_vectors.shape[0]):\n",
    "#         # Calculate average difficulty for documents where this topic is dominant\n",
    "#         topic_doc_mask = np.argmax(topic_distributions, axis=1) == i\n",
    "#         if np.sum(topic_doc_mask) > 0:\n",
    "#             avg_difficulty = np.mean(difficulty_scores['entropy'][topic_doc_mask])\n",
    "#         else:\n",
    "#             avg_difficulty = 0.5\n",
    "        \n",
    "#         # Node size based on topic weight\n",
    "#         node_size = topic_weights_norm[i] * 1000 + 300\n",
    "        \n",
    "#         G.add_node(f'T{i}', \n",
    "#                    size=node_size,\n",
    "#                    avg_difficulty=avg_difficulty,\n",
    "#                    weight=topic_weights_norm[i])\n",
    "    \n",
    "#     # Add edges based on similarity (only if we have more than 1 topic)\n",
    "#     if topic_vectors.shape[0] > 1:\n",
    "#         # Calculate threshold for edges (only consider similarities less than 1 to exclude self-similarity)\n",
    "#         similarity_values = topic_similarity[topic_similarity < 0.999]  # Exclude self-similarity\n",
    "#         if len(similarity_values) > 0:\n",
    "#             threshold = np.percentile(similarity_values, 75)\n",
    "            \n",
    "#             for i in range(topic_vectors.shape[0]):\n",
    "#                 for j in range(i+1, topic_vectors.shape[0]):\n",
    "#                     if topic_similarity[i, j] > threshold:\n",
    "#                         G.add_edge(f'T{i}', f'T{j}', \n",
    "#                                   weight=topic_similarity[i, j],\n",
    "#                                   width=topic_similarity[i, j] * 3)\n",
    "#         else:\n",
    "#             threshold = 0\n",
    "#     else:\n",
    "#         threshold = 0\n",
    "    \n",
    "#     # Visualize the network\n",
    "#     if G.number_of_nodes() > 0:\n",
    "#         fig, ax = plt.subplots(figsize=(14, 12))\n",
    "        \n",
    "#         # Get node positions using spring layout\n",
    "#         pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "        \n",
    "#         # Extract node attributes\n",
    "#         node_sizes = [G.nodes[n]['size'] for n in G.nodes()]\n",
    "#         avg_difficulties = [G.nodes[n]['avg_difficulty'] for n in G.nodes()]\n",
    "        \n",
    "#         # Normalize difficulties for coloring\n",
    "#         if len(set(avg_difficulties)) > 1:\n",
    "#             normalized_difficulties = (avg_difficulties - np.min(avg_difficulties)) / (np.max(avg_difficulties) - np.min(avg_difficulties))\n",
    "#         else:\n",
    "#             normalized_difficulties = [0.5] * len(avg_difficulties)\n",
    "        \n",
    "#         # Create colormap for difficulties\n",
    "#         cmap = plt.cm.RdYlBu\n",
    "#         node_colors = [cmap(d) for d in normalized_difficulties]\n",
    "        \n",
    "#         # Extract edge attributes\n",
    "#         if G.number_of_edges() > 0:\n",
    "#             edge_weights = [G[u][v]['width'] for u, v in G.edges()]\n",
    "#         else:\n",
    "#             edge_weights = []\n",
    "        \n",
    "#         # Draw network\n",
    "#         nx.draw_networkx_nodes(G, pos, \n",
    "#                               node_size=node_sizes,\n",
    "#                               node_color=node_colors,\n",
    "#                               alpha=0.8,\n",
    "#                               edgecolors='black',\n",
    "#                               linewidths=1,\n",
    "#                               ax=ax)\n",
    "        \n",
    "#         if G.number_of_edges() > 0:\n",
    "#             nx.draw_networkx_edges(G, pos, \n",
    "#                                   width=edge_weights,\n",
    "#                                   alpha=0.3,\n",
    "#                                   edge_color='gray',\n",
    "#                                   ax=ax)\n",
    "        \n",
    "#         # Add labels\n",
    "#         nx.draw_networkx_labels(G, pos, \n",
    "#                                font_size=10,\n",
    "#                                font_weight='bold',\n",
    "#                                ax=ax)\n",
    "        \n",
    "#         # Add colorbar for difficulty\n",
    "#         sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=min(avg_difficulties), vmax=max(avg_difficulties)))\n",
    "#         sm.set_array([])\n",
    "#         cbar = plt.colorbar(sm, ax=ax, shrink=0.8)\n",
    "#         cbar.set_label('Average Difficulty (Entropy)', fontsize=12)\n",
    "        \n",
    "#         ax.set_title(f'Topic Relationship Network - {dataset_name}\\n'\n",
    "#                      f'Node size ∝ Topic weight, Color ∝ Average difficulty',\n",
    "#                      fontsize=16, pad=20)\n",
    "        \n",
    "#         # Add statistics to plot\n",
    "#         ax.text(0.02, 0.02, \n",
    "#                 f\"Nodes: {G.number_of_nodes()}\\n\"\n",
    "#                 f\"Edges: {G.number_of_edges()}\\n\"\n",
    "#                 f\"Edge threshold: {threshold:.3f}\",\n",
    "#                 fontsize=10,\n",
    "#                 transform=ax.transAxes,\n",
    "#                 bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "        \n",
    "#         output_path = f\"results/tmcl/topic_models/{dataset_name}_topic_network.png\"\n",
    "#         plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "#         plt.close()\n",
    "        \n",
    "#         print(f\"  Topic network saved to: {output_path}\")\n",
    "#         print(f\"  Network stats: {G.number_of_nodes()} topics, {G.number_of_edges()} connections\")\n",
    "#     else:\n",
    "#         print(\"  Not enough topics to create network visualization\")\n",
    "    \n",
    "#     # 4. TOPIC-DIFFICULTY ANALYSIS\n",
    "#     print(\"\\n4. TOPIC-DIFFICULTY ANALYSIS\")\n",
    "    \n",
    "#     # Create a DataFrame for analysis\n",
    "#     analysis_data = []\n",
    "#     for i in range(topic_vectors.shape[0]):\n",
    "#         topic_doc_mask = np.argmax(topic_distributions, axis=1) == i\n",
    "#         n_docs = np.sum(topic_doc_mask)\n",
    "        \n",
    "#         if n_docs > 0:\n",
    "#             avg_entropy = np.mean(difficulty_scores['entropy'][topic_doc_mask])\n",
    "#             avg_max_prob = np.mean(difficulty_scores['max_prob'][topic_doc_mask])\n",
    "#             avg_composite = np.mean(difficulty_scores['composite'][topic_doc_mask])\n",
    "            \n",
    "#             # Calculate topic purity (how dominant is the main topic)\n",
    "#             topic_strength = np.mean(topic_distributions[topic_doc_mask, i])\n",
    "            \n",
    "#             # Get top words for this topic (if available)\n",
    "#             top_words = []\n",
    "#             if feature_names is not None and hasattr(lda_model, 'components_'):\n",
    "#                 # Get indices of top words for this topic\n",
    "#                 topic_vector = lda_model.components_[i]\n",
    "#                 top_indices = np.argsort(topic_vector)[-5:][::-1]  # Top 5 words\n",
    "#                 top_words = [feature_names[idx] for idx in top_indices]\n",
    "            \n",
    "#             analysis_data.append({\n",
    "#                 'Topic': f'T{i}',\n",
    "#                 'N_Docs': n_docs,\n",
    "#                 'Doc_Proportion': n_docs / len(topic_distributions),\n",
    "#                 'Avg_Entropy_Difficulty': avg_entropy,\n",
    "#                 'Avg_MaxProb_Difficulty': avg_max_prob,\n",
    "#                 'Avg_Composite_Difficulty': avg_composite,\n",
    "#                 'Topic_Strength': topic_strength,\n",
    "#                 'Topic_Weight': topic_weights_norm[i],\n",
    "#                 'Top_Words': ', '.join(top_words) if top_words else ''\n",
    "#             })\n",
    "    \n",
    "#     if analysis_data:  # Check if we have data\n",
    "#         analysis_df = pd.DataFrame(analysis_data)\n",
    "        \n",
    "#         # Create visualization of topic difficulty distribution\n",
    "#         fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "#         fig.suptitle(f'Topic Analysis - {dataset_name}', fontsize=20, y=1.02)\n",
    "        \n",
    "#         # Plot 1: Topic weights\n",
    "#         axes[0, 0].bar(analysis_df['Topic'], analysis_df['Topic_Weight'])\n",
    "#         axes[0, 0].set_title('Topic Weights', fontsize=14)\n",
    "#         axes[0, 0].set_xlabel('Topic')\n",
    "#         axes[0, 0].set_ylabel('Weight')\n",
    "#         axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "#         # Plot 2: Number of documents per topic\n",
    "#         axes[0, 1].bar(analysis_df['Topic'], analysis_df['N_Docs'])\n",
    "#         axes[0, 1].set_title('Documents per Topic', fontsize=14)\n",
    "#         axes[0, 1].set_xlabel('Topic')\n",
    "#         axes[0, 1].set_ylabel('Number of Documents')\n",
    "#         axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "#         # Plot 3: Average difficulty per topic (entropy)\n",
    "#         if len(analysis_df) > 1:\n",
    "#             axes[0, 2].scatter(analysis_df['Topic_Weight'], analysis_df['Avg_Entropy_Difficulty'],\n",
    "#                               s=analysis_df['N_Docs']/10, alpha=0.6)\n",
    "#             for idx, row in analysis_df.iterrows():\n",
    "#                 axes[0, 2].annotate(row['Topic'], \n",
    "#                                   (row['Topic_Weight'], row['Avg_Entropy_Difficulty']),\n",
    "#                                   fontsize=9, alpha=0.8)\n",
    "#         axes[0, 2].set_title('Topic Weight vs Difficulty', fontsize=14)\n",
    "#         axes[0, 2].set_xlabel('Topic Weight')\n",
    "#         axes[0, 2].set_ylabel('Average Entropy Difficulty')\n",
    "        \n",
    "#         # Plot 4: Topic strength distribution\n",
    "#         axes[1, 0].hist(analysis_df['Topic_Strength'], bins=20, alpha=0.7)\n",
    "#         axes[1, 0].axvline(analysis_df['Topic_Strength'].mean(), color='red', \n",
    "#                           linestyle='--', label=f'Mean: {analysis_df[\"Topic_Strength\"].mean():.3f}')\n",
    "#         axes[1, 0].set_title('Topic Strength Distribution', fontsize=14)\n",
    "#         axes[1, 0].set_xlabel('Topic Strength')\n",
    "#         axes[1, 0].set_ylabel('Frequency')\n",
    "#         axes[1, 0].legend()\n",
    "        \n",
    "#         # Plot 5: Difficulty metrics comparison\n",
    "#         x = np.arange(len(analysis_df))\n",
    "#         width = 0.25\n",
    "#         axes[1, 1].bar(x - width, analysis_df['Avg_Entropy_Difficulty'], width, label='Entropy')\n",
    "#         axes[1, 1].bar(x, analysis_df['Avg_MaxProb_Difficulty'], width, label='Max Prob')\n",
    "#         axes[1, 1].bar(x + width, analysis_df['Avg_Composite_Difficulty'], width, label='Composite')\n",
    "#         axes[1, 1].set_title('Difficulty Metrics by Topic', fontsize=14)\n",
    "#         axes[1, 1].set_xlabel('Topic')\n",
    "#         axes[1, 1].set_ylabel('Difficulty Score')\n",
    "#         axes[1, 1].set_xticks(x)\n",
    "#         axes[1, 1].set_xticklabels(analysis_df['Topic'], rotation=45)\n",
    "#         axes[1, 1].legend()\n",
    "        \n",
    "#         # Plot 6: Topic-difficulty heatmap\n",
    "#         difficulty_matrix = np.array([\n",
    "#             analysis_df['Avg_Entropy_Difficulty'].values,\n",
    "#             analysis_df['Avg_MaxProb_Difficulty'].values,\n",
    "#             analysis_df['Avg_Composite_Difficulty'].values\n",
    "#         ])\n",
    "#         im = axes[1, 2].imshow(difficulty_matrix, aspect='auto', cmap='YlOrRd')\n",
    "#         axes[1, 2].set_title('Difficulty Metrics Heatmap', fontsize=14)\n",
    "#         axes[1, 2].set_xlabel('Topic')\n",
    "#         axes[1, 2].set_ylabel('Difficulty Metric')\n",
    "#         axes[1, 2].set_xticks(range(len(analysis_df)))\n",
    "#         axes[1, 2].set_xticklabels(analysis_df['Topic'], rotation=45)\n",
    "#         axes[1, 2].set_yticks(range(3))\n",
    "#         axes[1, 2].set_yticklabels(['Entropy', 'MaxProb', 'Composite'])\n",
    "#         plt.colorbar(im, ax=axes[1, 2])\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         output_path = f\"results/tmcl/topic_models/{dataset_name}_topic_analysis.png\"\n",
    "#         plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "#         plt.close()\n",
    "        \n",
    "#         print(f\"  Topic analysis plot saved to: {output_path}\")\n",
    "        \n",
    "#         # 5. TOPIC EVOLUTION IN DIFFICULTY SPACE\n",
    "#         print(\"\\n5. TOPIC EVOLUTION IN DIFFICULTY SPACE\")\n",
    "        \n",
    "#         # Use t-SNE to visualize topics in 2D based on difficulty patterns\n",
    "#         if len(analysis_df) > 2:\n",
    "#             # Prepare data for t-SNE\n",
    "#             tsne_data = analysis_df[['Avg_Entropy_Difficulty', 'Avg_MaxProb_Difficulty', \n",
    "#                                     'Avg_Composite_Difficulty', 'Topic_Strength', 'Topic_Weight']].values\n",
    "            \n",
    "#             # Normalize\n",
    "#             tsne_data = (tsne_data - tsne_data.mean(axis=0)) / (tsne_data.std(axis=0) + 1e-8)\n",
    "            \n",
    "#             # Apply t-SNE\n",
    "#             tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(analysis_df)-1))\n",
    "#             tsne_results = tsne.fit_transform(tsne_data)\n",
    "            \n",
    "#             # Plot\n",
    "#             plt.figure(figsize=(12, 10))\n",
    "#             scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], \n",
    "#                                 c=analysis_df['Avg_Composite_Difficulty'], \n",
    "#                                 s=analysis_df['Topic_Weight'] * 2000,\n",
    "#                                 alpha=0.7, cmap='coolwarm', edgecolors='black')\n",
    "            \n",
    "#             # Add labels\n",
    "#             for i, (x, y) in enumerate(tsne_results):\n",
    "#                 plt.text(x, y, f\"T{i}\", fontsize=11, ha='center', va='center',\n",
    "#                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "            \n",
    "#             plt.colorbar(scatter, label='Composite Difficulty')\n",
    "#             plt.title(f'Topic Clustering by Difficulty Patterns - {dataset_name}', fontsize=16)\n",
    "#             plt.xlabel('t-SNE Component 1')\n",
    "#             plt.ylabel('t-SNE Component 2')\n",
    "#             plt.grid(True, alpha=0.3)\n",
    "            \n",
    "#             # Add statistics\n",
    "#             plt.figtext(0.02, 0.98, \n",
    "#                        f\"Topics: {len(analysis_df)}\\n\"\n",
    "#                        f\"Difficulty range: [{analysis_df['Avg_Composite_Difficulty'].min():.3f}, \"\n",
    "#                        f\"{analysis_df['Avg_Composite_Difficulty'].max():.3f}]\",\n",
    "#                        fontsize=10,\n",
    "#                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "            \n",
    "#             output_path = f\"results/tmcl/topic_models/{dataset_name}_topic_clustering.png\"\n",
    "#             plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "#             plt.close()\n",
    "            \n",
    "#             print(f\"  Topic clustering plot saved to: {output_path}\")\n",
    "#         else:\n",
    "#             print(\"  Not enough topics for t-SNE clustering (need at least 3)\")\n",
    "        \n",
    "#         # 6. COMPREHENSIVE STATISTICAL ANALYSIS\n",
    "#         print(\"\\n6. COMPREHENSIVE STATISTICAL ANALYSIS\")\n",
    "#         print(\"-\" * 40)\n",
    "        \n",
    "#         # Calculate correlations\n",
    "#         if len(analysis_df) > 2:\n",
    "#             correlation_matrix = analysis_df[['Avg_Entropy_Difficulty', 'Avg_MaxProb_Difficulty',\n",
    "#                                              'Avg_Composite_Difficulty', 'Topic_Strength',\n",
    "#                                              'Topic_Weight', 'N_Docs']].corr()\n",
    "            \n",
    "#             print(\"\\nCorrelation Matrix:\")\n",
    "#             print(correlation_matrix.round(3))\n",
    "            \n",
    "#             # Plot correlation heatmap\n",
    "#             plt.figure(figsize=(10, 8))\n",
    "#             sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "#                        square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "#             plt.title(f'Feature Correlations - {dataset_name}', fontsize=16)\n",
    "#             plt.tight_layout()\n",
    "            \n",
    "#             output_path = f\"results/tmcl/topic_models/{dataset_name}_correlation_heatmap.png\"\n",
    "#             plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "#             plt.close()\n",
    "            \n",
    "#             print(f\"  Correlation heatmap saved to: {output_path}\")\n",
    "        \n",
    "#         # Calculate difficulty statistics by topic\n",
    "#         print(\"\\nDifficulty Statistics by Topic (Top 5 most difficult):\")\n",
    "#         difficult_topics = analysis_df.sort_values('Avg_Composite_Difficulty', ascending=False).head(5)\n",
    "#         for _, row in difficult_topics.iterrows():\n",
    "#             print(f\"  {row['Topic']}: {row['Avg_Composite_Difficulty']:.3f} \"\n",
    "#                   f\"(Entropy: {row['Avg_Entropy_Difficulty']:.3f}, \"\n",
    "#                   f\"MaxProb: {row['Avg_MaxProb_Difficulty']:.3f})\")\n",
    "#             if row['Top_Words']:\n",
    "#                 print(f\"     Top words: {row['Top_Words']}\")\n",
    "        \n",
    "#         print(\"\\nDifficulty Statistics by Topic (Top 5 least difficult):\")\n",
    "#         easy_topics = analysis_df.sort_values('Avg_Composite_Difficulty').head(5)\n",
    "#         for _, row in easy_topics.iterrows():\n",
    "#             print(f\"  {row['Topic']}: {row['Avg_Composite_Difficulty']:.3f} \"\n",
    "#                   f\"(Entropy: {row['Avg_Entropy_Difficulty']:.3f}, \"\n",
    "#                   f\"MaxProb: {row['Avg_MaxProb_Difficulty']:.3f})\")\n",
    "#             if row['Top_Words']:\n",
    "#                 print(f\"     Top words: {row['Top_Words']}\")\n",
    "#     else:\n",
    "#         print(\"  No valid topic data for analysis\")\n",
    "#         analysis_df = pd.DataFrame()\n",
    "    \n",
    "#     # 7. SAVE COMPREHENSIVE ANALYSIS REPORT\n",
    "#     print(\"\\n7. GENERATING ANALYSIS REPORT...\")\n",
    "    \n",
    "#     if not analysis_df.empty:\n",
    "#         # Calculate network density\n",
    "#         if G.number_of_nodes() > 0:\n",
    "#             density = nx.density(G)\n",
    "#         else:\n",
    "#             density = 0\n",
    "        \n",
    "#         report = {\n",
    "#             'dataset_name': dataset_name,\n",
    "#             'n_topics': topic_vectors.shape[0],\n",
    "#             'n_documents': len(topic_distributions),\n",
    "#             'perplexity': perplexity,\n",
    "#             'topic_analysis': analysis_df,\n",
    "#             'network_stats': {\n",
    "#                 'n_nodes': G.number_of_nodes(),\n",
    "#                 'n_edges': G.number_of_edges(),\n",
    "#                 'density': density\n",
    "#             },\n",
    "#             'difficulty_stats': {\n",
    "#                 'overall_mean_entropy': np.mean(difficulty_scores['entropy']),\n",
    "#                 'overall_std_entropy': np.std(difficulty_scores['entropy']),\n",
    "#                 'topic_mean_entropy': analysis_df['Avg_Entropy_Difficulty'].mean() if not analysis_df.empty else 0,\n",
    "#                 'topic_std_entropy': analysis_df['Avg_Entropy_Difficulty'].std() if not analysis_df.empty else 0,\n",
    "#                 'overall_mean_composite': np.mean(difficulty_scores['composite']),\n",
    "#                 'overall_std_composite': np.std(difficulty_scores['composite'])\n",
    "#             }\n",
    "#         }\n",
    "#     else:\n",
    "#         report = {\n",
    "#             'dataset_name': dataset_name,\n",
    "#             'n_topics': topic_vectors.shape[0],\n",
    "#             'n_documents': len(topic_distributions),\n",
    "#             'perplexity': perplexity,\n",
    "#             'topic_analysis': pd.DataFrame(),\n",
    "#             'network_stats': {\n",
    "#                 'n_nodes': 0,\n",
    "#                 'n_edges': 0,\n",
    "#                 'density': 0\n",
    "#             },\n",
    "#             'difficulty_stats': {\n",
    "#                 'overall_mean_entropy': np.mean(difficulty_scores['entropy']),\n",
    "#                 'overall_std_entropy': np.std(difficulty_scores['entropy']),\n",
    "#                 'topic_mean_entropy': 0,\n",
    "#                 'topic_std_entropy': 0,\n",
    "#                 'overall_mean_composite': np.mean(difficulty_scores['composite']),\n",
    "#                 'overall_std_composite': np.std(difficulty_scores['composite'])\n",
    "#             }\n",
    "#         }\n",
    "    \n",
    "#     # Save report\n",
    "#     output_path = f\"results/tmcl/topic_models/{dataset_name}_enhanced_analysis.pkl\"\n",
    "#     with open(output_path, 'wb') as f:\n",
    "#         pickle.dump(report, f)\n",
    "    \n",
    "#     print(f\"  Analysis report saved to: {output_path}\")\n",
    "    \n",
    "#     # Generate summary text file\n",
    "#     summary_path = f\"results/tmcl/topic_models/{dataset_name}_analysis_summary.txt\"\n",
    "#     with open(summary_path, 'w') as f:\n",
    "#         f.write(f\"ENHANCED TOPIC ANALYSIS REPORT - {dataset_name.upper()}\\n\")\n",
    "#         f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        \n",
    "#         f.write(\"1. DATASET STATISTICS\\n\")\n",
    "#         f.write(\"-\" * 40 + \"\\n\")\n",
    "#         f.write(f\"   Number of topics: {topic_vectors.shape[0]}\\n\")\n",
    "#         f.write(f\"   Number of documents: {len(topic_distributions)}\\n\")\n",
    "#         if perplexity:\n",
    "#             f.write(f\"   Perplexity: {perplexity:.4f}\\n\")\n",
    "        \n",
    "#         if not analysis_df.empty:\n",
    "#             f.write(f\"\\n   Topic weight distribution:\\n\")\n",
    "#             f.write(f\"     Mean: {analysis_df['Topic_Weight'].mean():.4f}\\n\")\n",
    "#             f.write(f\"     Std: {analysis_df['Topic_Weight'].std():.4f}\\n\")\n",
    "#             f.write(f\"     Min: {analysis_df['Topic_Weight'].min():.4f}\\n\")\n",
    "#             f.write(f\"     Max: {analysis_df['Topic_Weight'].max():.4f}\\n\")\n",
    "        \n",
    "#         f.write(\"\\n2. DIFFICULTY ANALYSIS\\n\")\n",
    "#         f.write(\"-\" * 40 + \"\\n\")\n",
    "#         f.write(f\"   Overall difficulty (entropy): {np.mean(difficulty_scores['entropy']):.4f} \"\n",
    "#                 f\"± {np.std(difficulty_scores['entropy']):.4f}\\n\")\n",
    "#         f.write(f\"   Overall difficulty (composite): {np.mean(difficulty_scores['composite']):.4f} \"\n",
    "#                 f\"± {np.std(difficulty_scores['composite']):.4f}\\n\")\n",
    "        \n",
    "#         if not analysis_df.empty:\n",
    "#             f.write(f\"   Topic-level difficulty (composite): {analysis_df['Avg_Composite_Difficulty'].mean():.4f} \"\n",
    "#                     f\"± {analysis_df['Avg_Composite_Difficulty'].std():.4f}\\n\")\n",
    "        \n",
    "#         if not analysis_df.empty:\n",
    "#             f.write(\"\\n3. MOST DIFFICULT TOPICS\\n\")\n",
    "#             f.write(\"-\" * 40 + \"\\n\")\n",
    "#             for _, row in difficult_topics.iterrows():\n",
    "#                 f.write(f\"   {row['Topic']}: {row['Avg_Composite_Difficulty']:.3f} \"\n",
    "#                        f\"(docs: {row['N_Docs']}, weight: {row['Topic_Weight']:.3f})\\n\")\n",
    "#                 if row['Top_Words']:\n",
    "#                     f.write(f\"     Top words: {row['Top_Words']}\\n\")\n",
    "            \n",
    "#             f.write(\"\\n4. LEAST DIFFICULT TOPICS\\n\")\n",
    "#             f.write(\"-\" * 40 + \"\\n\")\n",
    "#             for _, row in easy_topics.iterrows():\n",
    "#                 f.write(f\"   {row['Topic']}: {row['Avg_Composite_Difficulty']:.3f} \"\n",
    "#                        f\"(docs: {row['N_Docs']}, weight: {row['Topic_Weight']:.3f})\\n\")\n",
    "#                 if row['Top_Words']:\n",
    "#                     f.write(f\"     Top words: {row['Top_Words']}\\n\")\n",
    "        \n",
    "#         f.write(\"\\n5. TOPIC NETWORK STATISTICS\\n\")\n",
    "#         f.write(\"-\" * 40 + \"\\n\")\n",
    "#         f.write(f\"   Number of topic nodes: {G.number_of_nodes()}\\n\")\n",
    "#         f.write(f\"   Number of connections: {G.number_of_edges()}\\n\")\n",
    "#         f.write(f\"   Network density: {nx.density(G) if G.number_of_nodes() > 0 else 0:.4f}\\n\")\n",
    "        \n",
    "#         # Calculate degree centrality\n",
    "#         if G.number_of_nodes() > 0:\n",
    "#             degree_centrality = nx.degree_centrality(G)\n",
    "#             f.write(f\"   Average degree centrality: {np.mean(list(degree_centrality.values())):.4f}\\n\")\n",
    "        \n",
    "#         f.write(\"\\n6. KEY INSIGHTS\\n\")\n",
    "#         f.write(\"-\" * 40 + \"\\n\")\n",
    "#         if not analysis_df.empty and 'Topic_Weight' in analysis_df.columns and 'Avg_Composite_Difficulty' in analysis_df.columns:\n",
    "#             try:\n",
    "#                 corr = analysis_df['Topic_Weight'].corr(analysis_df['Avg_Composite_Difficulty'])\n",
    "#                 trend = \"higher\" if corr > 0 else \"lower\" if corr < 0 else \"no clear relationship with\"\n",
    "#                 f.write(f\"   - Topics with higher weights tend to have {trend} difficulty (correlation: {corr:.3f})\\n\")\n",
    "#             except:\n",
    "#                 f.write(f\"   - Could not calculate correlation between topic weight and difficulty\\n\")\n",
    "        \n",
    "#         if not analysis_df.empty:\n",
    "#             f.write(f\"   - Document distribution across topics: {analysis_df['N_Docs'].min()} to {analysis_df['N_Docs'].max()} documents\\n\")\n",
    "#             f.write(f\"   - Topic strength varies from {analysis_df['Topic_Strength'].min():.3f} to {analysis_df['Topic_Strength'].max():.3f}\\n\")\n",
    "        \n",
    "#         # Add dataset-specific insights\n",
    "#         if dataset_name == 'ag_news':\n",
    "#             f.write(f\"   - AG News has relatively high average difficulty ({np.mean(difficulty_scores['composite']):.3f}), suggesting complex topic structure\\n\")\n",
    "#         elif dataset_name == 'imdb':\n",
    "#             f.write(f\"   - IMDB has relatively low average difficulty ({np.mean(difficulty_scores['composite']):.3f}), suggesting clearer topic separation\\n\")\n",
    "    \n",
    "#     print(f\"  Analysis summary saved to: {summary_path}\")\n",
    "    \n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"ANALYSIS COMPLETE FOR {dataset_name.upper()}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "    \n",
    "#     return report\n",
    "\n",
    "# def enhanced_validation_analysis(dataset_name, results, sample_size=2000):\n",
    "#     \"\"\"\n",
    "#     Enhanced validation analysis with comprehensive metrics\n",
    "    \n",
    "#     Args:\n",
    "#         dataset_name: Name of the dataset\n",
    "#         results: Processing results containing difficulty scores\n",
    "#         sample_size: Number of samples to use for validation\n",
    "#     \"\"\"\n",
    "    \n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"ENHANCED VALIDATION: {dataset_name.upper()}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "    \n",
    "#     # Get difficulty scores and labels\n",
    "#     difficulty_entropy = results['difficulty_scores']['entropy']\n",
    "#     difficulty_max_prob = results['difficulty_scores']['max_prob']\n",
    "#     difficulty_composite = results['difficulty_scores']['composite']\n",
    "#     labels = results.get('labels', None)\n",
    "#     topic_distributions = results.get('topic_distributions', None)\n",
    "    \n",
    "#     if labels is None:\n",
    "#         print(\"No labels available for validation\")\n",
    "#         return None\n",
    "    \n",
    "#     # Sample a subset for validation\n",
    "#     if len(difficulty_entropy) > sample_size:\n",
    "#         indices = np.random.choice(len(difficulty_entropy), sample_size, replace=False)\n",
    "#         difficulty_entropy = difficulty_entropy[indices]\n",
    "#         difficulty_max_prob = difficulty_max_prob[indices]\n",
    "#         difficulty_composite = difficulty_composite[indices]\n",
    "#         labels = labels[indices]\n",
    "#         if topic_distributions is not None:\n",
    "#             topic_distributions = topic_distributions[indices]\n",
    "    \n",
    "#     # Create comprehensive validation analysis\n",
    "#     fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "#     fig.suptitle(f'Enhanced Validation Analysis - {dataset_name}', fontsize=20, y=1.02)\n",
    "    \n",
    "#     # 1. Label diversity vs difficulty (original)\n",
    "#     n_bins = 10\n",
    "#     bins = np.linspace(0, 1, n_bins + 1)\n",
    "#     bin_indices = np.digitize(difficulty_composite, bins) - 1\n",
    "    \n",
    "#     bin_diversity = []\n",
    "#     bin_samples = []\n",
    "#     for i in range(n_bins):\n",
    "#         bin_mask = (bin_indices == i)\n",
    "#         if np.sum(bin_mask) > 10:  # Minimum samples threshold\n",
    "#             bin_labels = labels[bin_mask]\n",
    "#             diversity = len(np.unique(bin_labels)) / len(bin_labels)\n",
    "#             bin_diversity.append(diversity)\n",
    "#             bin_samples.append(np.sum(bin_mask))\n",
    "#         else:\n",
    "#             bin_diversity.append(0)\n",
    "#             bin_samples.append(0)\n",
    "    \n",
    "#     # Plot 1: Label diversity vs difficulty\n",
    "#     axes[0, 0].bar(range(n_bins), bin_diversity, alpha=0.7, color='steelblue')\n",
    "#     axes[0, 0].set_title('Label Diversity vs Difficulty', fontsize=14)\n",
    "#     axes[0, 0].set_xlabel('Difficulty Bin (0=easy, 9=hard)')\n",
    "#     axes[0, 0].set_ylabel('Label Diversity')\n",
    "#     axes[0, 0].set_xticks(range(n_bins))\n",
    "#     axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "#     # Add sample counts\n",
    "#     for i, (div, count) in enumerate(zip(bin_diversity, bin_samples)):\n",
    "#         if count > 0:\n",
    "#             axes[0, 0].text(i, div + 0.02, str(count), ha='center', fontsize=8)\n",
    "    \n",
    "#     # 2. Difficulty distribution by label\n",
    "#     unique_labels = np.unique(labels)\n",
    "#     if len(unique_labels) <= 20:  # Only plot if reasonable number of labels\n",
    "#         label_difficulties = []\n",
    "#         for label in unique_labels:\n",
    "#             mask = labels == label\n",
    "#             if np.sum(mask) > 0:\n",
    "#                 label_difficulties.append(np.mean(difficulty_composite[mask]))\n",
    "#             else:\n",
    "#                 label_difficulties.append(0)\n",
    "        \n",
    "#         axes[0, 1].bar(range(len(unique_labels)), label_difficulties, alpha=0.7)\n",
    "#         axes[0, 1].set_title('Average Difficulty by Label', fontsize=14)\n",
    "#         axes[0, 1].set_xlabel('Label')\n",
    "#         axes[0, 1].set_ylabel('Average Difficulty')\n",
    "#         axes[0, 1].set_xticks(range(len(unique_labels)))\n",
    "#         axes[0, 1].set_xticklabels([f'L{i}' for i in unique_labels], rotation=45)\n",
    "#         axes[0, 1].grid(True, alpha=0.3)\n",
    "#     else:\n",
    "#         axes[0, 1].text(0.5, 0.5, f'Too many labels ({len(unique_labels)})', \n",
    "#                        ha='center', va='center', fontsize=12)\n",
    "#         axes[0, 1].set_title('Average Difficulty by Label', fontsize=14)\n",
    "    \n",
    "#     # 3. Topic concentration vs difficulty\n",
    "#     if topic_distributions is not None:\n",
    "#         # Calculate topic concentration (max probability in topic distribution)\n",
    "#         topic_concentration = np.max(topic_distributions, axis=1)\n",
    "        \n",
    "#         # Scatter plot\n",
    "#         sc = axes[0, 2].scatter(topic_concentration, difficulty_composite, \n",
    "#                                c=labels, alpha=0.5, cmap='tab20')\n",
    "#         axes[0, 2].set_title('Topic Concentration vs Difficulty', fontsize=14)\n",
    "#         axes[0, 2].set_xlabel('Topic Concentration (Max Probability)')\n",
    "#         axes[0, 2].set_ylabel('Difficulty Score')\n",
    "#         axes[0, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "#         # Add correlation line\n",
    "#         if len(topic_concentration) > 1:\n",
    "#             z = np.polyfit(topic_concentration, difficulty_composite, 1)\n",
    "#             p = np.poly1d(z)\n",
    "#             axes[0, 2].plot(np.sort(topic_concentration), p(np.sort(topic_concentration)), \n",
    "#                            \"r--\", alpha=0.8, label=f'Corr: {np.corrcoef(topic_concentration, difficulty_composite)[0,1]:.3f}')\n",
    "#             axes[0, 2].legend()\n",
    "#     else:\n",
    "#         axes[0, 2].text(0.5, 0.5, 'No topic distributions available', \n",
    "#                        ha='center', va='center', fontsize=12)\n",
    "#         axes[0, 2].set_title('Topic Concentration vs Difficulty', fontsize=14)\n",
    "    \n",
    "#     # 4. Difficulty metrics comparison\n",
    "#     scatter = axes[1, 0].scatter(difficulty_entropy, difficulty_max_prob, \n",
    "#                                 c=difficulty_composite, alpha=0.5, cmap='viridis')\n",
    "#     axes[1, 0].set_title('Entropy vs MaxProb Difficulty', fontsize=14)\n",
    "#     axes[1, 0].set_xlabel('Entropy Difficulty')\n",
    "#     axes[1, 0].set_ylabel('MaxProb Difficulty')\n",
    "#     axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "#     # Add identity line\n",
    "#     axes[1, 0].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    \n",
    "#     # Add colorbar\n",
    "#     plt.colorbar(scatter, ax=axes[1, 0], label='Composite Difficulty')\n",
    "    \n",
    "#     # 5. Cumulative difficulty distribution\n",
    "#     sorted_difficulty = np.sort(difficulty_composite)\n",
    "#     cumulative = np.arange(1, len(sorted_difficulty) + 1) / len(sorted_difficulty)\n",
    "    \n",
    "#     axes[1, 1].plot(sorted_difficulty, cumulative, linewidth=2, color='darkgreen')\n",
    "#     axes[1, 1].set_title('Cumulative Difficulty Distribution', fontsize=14)\n",
    "#     axes[1, 1].set_xlabel('Difficulty Score')\n",
    "#     axes[1, 1].set_ylabel('Cumulative Proportion')\n",
    "#     axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "#     # Add quartile lines\n",
    "#     for q in [0.25, 0.5, 0.75]:\n",
    "#         q_value = np.percentile(sorted_difficulty, q * 100)\n",
    "#         axes[1, 1].axvline(q_value, color='red', linestyle='--', alpha=0.7)\n",
    "#         axes[1, 1].text(q_value, 0.5, f'Q{q*100}%', rotation=90, va='center')\n",
    "    \n",
    "#     # 6. Difficulty histogram with KDE\n",
    "#     axes[1, 2].hist(difficulty_composite, bins=30, alpha=0.7, density=True, \n",
    "#                    color='skyblue', edgecolor='black')\n",
    "    \n",
    "#     # Add KDE\n",
    "#     kde = stats.gaussian_kde(difficulty_composite)\n",
    "#     x_range = np.linspace(0, 1, 100)\n",
    "#     axes[1, 2].plot(x_range, kde(x_range), color='darkblue', linewidth=2)\n",
    "    \n",
    "#     axes[1, 2].set_title('Difficulty Distribution with KDE', fontsize=14)\n",
    "#     axes[1, 2].set_xlabel('Difficulty Score')\n",
    "#     axes[1, 2].set_ylabel('Density')\n",
    "#     axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "#     # Add statistics\n",
    "#     stats_text = (f'Mean: {np.mean(difficulty_composite):.3f}\\n'\n",
    "#                   f'Std: {np.std(difficulty_composite):.3f}\\n'\n",
    "#                   f'Skew: {stats.skew(difficulty_composite):.3f}\\n'\n",
    "#                   f'Kurtosis: {stats.kurtosis(difficulty_composite):.3f}')\n",
    "#     axes[1, 2].text(0.02, 0.98, stats_text, transform=axes[1, 2].transAxes,\n",
    "#                    fontsize=10, verticalalignment='top',\n",
    "#                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     # Save the comprehensive validation plot\n",
    "#     output_path = f\"results/tmcl/topic_models/{dataset_name}_enhanced_validation.png\"\n",
    "#     plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "#     plt.close()\n",
    "    \n",
    "#     print(f\"Enhanced validation plot saved to: {output_path}\")\n",
    "    \n",
    "#     # Calculate and print validation statistics\n",
    "#     print(\"\\nVALIDATION STATISTICS:\")\n",
    "#     print(\"-\" * 40)\n",
    "    \n",
    "#     # Calculate correlation between difficulty metrics\n",
    "#     corr_entropy_maxprob = np.corrcoef(difficulty_entropy, difficulty_max_prob)[0, 1]\n",
    "#     corr_entropy_composite = np.corrcoef(difficulty_entropy, difficulty_composite)[0, 1]\n",
    "#     corr_maxprob_composite = np.corrcoef(difficulty_max_prob, difficulty_composite)[0, 1]\n",
    "    \n",
    "#     print(f\"Correlation between difficulty metrics:\")\n",
    "#     print(f\"  Entropy vs MaxProb: {corr_entropy_maxprob:.4f}\")\n",
    "#     print(f\"  Entropy vs Composite: {corr_entropy_composite:.4f}\")\n",
    "#     print(f\"  MaxProb vs Composite: {corr_maxprob_composite:.4f}\")\n",
    "    \n",
    "#     # Calculate difficulty by label (if not too many labels)\n",
    "#     if len(unique_labels) <= 20:\n",
    "#         print(f\"\\nDifficulty by label (top 5 most difficult):\")\n",
    "#         label_stats = []\n",
    "#         for label in unique_labels:\n",
    "#             mask = labels == label\n",
    "#             if np.sum(mask) > 0:\n",
    "#                 avg_diff = np.mean(difficulty_composite[mask])\n",
    "#                 label_stats.append((label, avg_diff, np.sum(mask)))\n",
    "        \n",
    "#         # Sort by difficulty\n",
    "#         label_stats.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "#         for label, avg_diff, count in label_stats[:5]:\n",
    "#             print(f\"  Label {label}: {avg_diff:.3f} (n={count})\")\n",
    "    \n",
    "#     # Calculate topic concentration statistics\n",
    "#     if topic_distributions is not None:\n",
    "#         print(f\"\\nTopic concentration statistics:\")\n",
    "#         print(f\"  Mean concentration: {np.mean(topic_concentration):.4f}\")\n",
    "#         print(f\"  Std concentration: {np.std(topic_concentration):.4f}\")\n",
    "#         corr_concentration = np.corrcoef(topic_concentration, difficulty_composite)[0,1] if len(topic_concentration) > 1 else 0\n",
    "#         print(f\"  Correlation with difficulty: {corr_concentration:.4f}\")\n",
    "    \n",
    "#     # Calculate label diversity statistics\n",
    "#     valid_bins = [d for d, s in zip(bin_diversity, bin_samples) if s > 0]\n",
    "#     if valid_bins:\n",
    "#         print(f\"\\nLabel diversity statistics:\")\n",
    "#         print(f\"  Mean diversity: {np.mean(valid_bins):.4f}\")\n",
    "#         print(f\"  Std diversity: {np.std(valid_bins):.4f}\")\n",
    "        \n",
    "#         # Calculate correlation between bin index and diversity\n",
    "#         valid_indices = [i for i, s in enumerate(bin_samples) if s > 0]\n",
    "#         if len(valid_indices) > 1:\n",
    "#             corr_diversity_difficulty = np.corrcoef(valid_indices, \n",
    "#                                                    [bin_diversity[i] for i in valid_indices])[0, 1]\n",
    "#             print(f\"  Correlation with difficulty bin: {corr_diversity_difficulty:.4f}\")\n",
    "    \n",
    "#     # Generate validation report\n",
    "#     validation_report = {\n",
    "#         'dataset_name': dataset_name,\n",
    "#         'n_samples': len(difficulty_composite),\n",
    "#         'difficulty_stats': {\n",
    "#             'mean_entropy': float(np.mean(difficulty_entropy)),\n",
    "#             'std_entropy': float(np.std(difficulty_entropy)),\n",
    "#             'mean_maxprob': float(np.mean(difficulty_max_prob)),\n",
    "#             'std_maxprob': float(np.std(difficulty_max_prob)),\n",
    "#             'mean_composite': float(np.mean(difficulty_composite)),\n",
    "#             'std_composite': float(np.std(difficulty_composite)),\n",
    "#         },\n",
    "#         'correlations': {\n",
    "#             'entropy_maxprob': float(corr_entropy_maxprob),\n",
    "#             'entropy_composite': float(corr_entropy_composite),\n",
    "#             'maxprob_composite': float(corr_maxprob_composite),\n",
    "#         },\n",
    "#         'label_diversity_by_bin': [float(d) for d in bin_diversity],\n",
    "#         'validation_plot': output_path\n",
    "#     }\n",
    "    \n",
    "#     if topic_distributions is not None:\n",
    "#         validation_report['topic_concentration'] = {\n",
    "#             'mean': float(np.mean(topic_concentration)),\n",
    "#             'std': float(np.std(topic_concentration)),\n",
    "#             'corr_with_difficulty': float(corr_concentration)\n",
    "#         }\n",
    "    \n",
    "#     # Save validation report\n",
    "#     report_path = f\"results/tmcl/topic_models/{dataset_name}_validation_report.pkl\"\n",
    "#     with open(report_path, 'wb') as f:\n",
    "#         pickle.dump(validation_report, f)\n",
    "    \n",
    "#     print(f\"\\nValidation report saved to: {report_path}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "    \n",
    "#     return validation_report\n",
    "\n",
    "# # Main execution\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"COMPREHENSIVE TOPIC MODELING ANALYSIS AND VALIDATION\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Define datasets to analyze\n",
    "# datasets = ['ag_news', 'imdb', 'cifar10', 'cifar100', 'mnist', 'fashion_mnist']\n",
    "\n",
    "# # Store all analysis results\n",
    "# all_analysis_results = {}\n",
    "# all_validation_results = {}\n",
    "\n",
    "# # First, analyze text datasets with enhanced topic analysis\n",
    "# print(\"\\nANALYZING TEXT DATASETS WITH TOPIC VISUALIZATION...\")\n",
    "\n",
    "# for dataset_name in ['ag_news', 'imdb']:\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"Processing: {dataset_name}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "    \n",
    "#     # Load results\n",
    "#     results_path = f\"results/tmcl/topic_models/{dataset_name}_tmcl_results.pkl\"\n",
    "#     if os.path.exists(results_path):\n",
    "#         with open(results_path, 'rb') as f:\n",
    "#             results = pickle.load(f)\n",
    "        \n",
    "#         # Get feature names for text datasets\n",
    "#         if 'vocabulary' in results:\n",
    "#             feature_names = results['vocabulary']\n",
    "#             print(f\"  Found vocabulary with {len(feature_names)} words\")\n",
    "#         else:\n",
    "#             # Try to get from feature matrix shape\n",
    "#             n_features = results.get('feature_matrix', np.zeros((1, 2000))).shape[1]\n",
    "#             feature_names = [f'word_{i}' for i in range(min(n_features, 2000))]\n",
    "#             print(f\"  Created dummy feature names (n={len(feature_names)})\")\n",
    "        \n",
    "#         # Run enhanced topic analysis\n",
    "#         analysis_result = enhanced_topic_analysis(\n",
    "#             dataset_name, \n",
    "#             results, \n",
    "#             feature_names=feature_names\n",
    "#         )\n",
    "        \n",
    "#         all_analysis_results[dataset_name] = analysis_result\n",
    "        \n",
    "#         # Run enhanced validation\n",
    "#         validation_result = enhanced_validation_analysis(dataset_name, results)\n",
    "#         all_validation_results[dataset_name] = validation_result\n",
    "        \n",
    "#     else:\n",
    "#         print(f\"Results file not found for {dataset_name}\")\n",
    "\n",
    "# # Now analyze image datasets\n",
    "# print(\"\\nANALYZING IMAGE DATASETS...\")\n",
    "\n",
    "# for dataset_name in ['cifar10', 'cifar100', 'mnist', 'fashion_mnist']:\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"Processing: {dataset_name}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "    \n",
    "#     # Load results\n",
    "#     results_path = f\"results/tmcl/topic_models/{dataset_name}_tmcl_results.pkl\"\n",
    "#     if os.path.exists(results_path):\n",
    "#         with open(results_path, 'rb') as f:\n",
    "#             results = pickle.load(f)\n",
    "        \n",
    "#         # For image datasets, we don't have feature names\n",
    "#         # Run analysis without feature names\n",
    "#         analysis_result = enhanced_topic_analysis(dataset_name, results)\n",
    "#         all_analysis_results[dataset_name] = analysis_result\n",
    "        \n",
    "#         # Run enhanced validation\n",
    "#         validation_result = enhanced_validation_analysis(dataset_name, results)\n",
    "#         all_validation_results[dataset_name] = validation_result\n",
    "        \n",
    "#     else:\n",
    "#         print(f\"Results file not found for {dataset_name}\")\n",
    "\n",
    "# # 8. CROSS-DATASET COMPARATIVE ANALYSIS\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"CROSS-DATASET COMPARATIVE ANALYSIS\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Create comparative analysis\n",
    "# comparison_data = []\n",
    "\n",
    "# for dataset_name, analysis in all_analysis_results.items():\n",
    "#     if analysis is not None:\n",
    "#         # Safely extract values with defaults\n",
    "#         topic_analysis = analysis.get('topic_analysis', pd.DataFrame())\n",
    "#         difficulty_stats = analysis.get('difficulty_stats', {})\n",
    "#         network_stats = analysis.get('network_stats', {})\n",
    "        \n",
    "#         # Get Avg_Composite_Difficulty with fallback\n",
    "#         if not topic_analysis.empty and 'Avg_Composite_Difficulty' in topic_analysis.columns:\n",
    "#             difficulty_range = topic_analysis['Avg_Composite_Difficulty'].max() - topic_analysis['Avg_Composite_Difficulty'].min()\n",
    "#         else:\n",
    "#             difficulty_range = 0\n",
    "        \n",
    "#         comparison_data.append({\n",
    "#             'Dataset': dataset_name,\n",
    "#             'N_Topics': analysis.get('n_topics', 0),\n",
    "#             'N_Documents': analysis.get('n_documents', 0),\n",
    "#             'Perplexity': analysis.get('perplexity', 0),\n",
    "#             'Avg_Topic_Weight': topic_analysis['Topic_Weight'].mean() if not topic_analysis.empty and 'Topic_Weight' in topic_analysis.columns else 0,\n",
    "#             'Std_Topic_Weight': topic_analysis['Topic_Weight'].std() if not topic_analysis.empty and 'Topic_Weight' in topic_analysis.columns else 0,\n",
    "#             'Avg_Difficulty': difficulty_stats.get('overall_mean_composite', 0),\n",
    "#             'Std_Difficulty': difficulty_stats.get('overall_std_composite', 0),\n",
    "#             'Network_Density': network_stats.get('density', 0),\n",
    "#             'Difficulty_Range': difficulty_range\n",
    "#         })\n",
    "\n",
    "# if comparison_data:\n",
    "#     comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "#     # Create comparative visualizations\n",
    "#     fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "#     fig.suptitle('Cross-Dataset Comparative Analysis', fontsize=20, y=1.02)\n",
    "    \n",
    "#     # Plot 1: Average difficulty by dataset\n",
    "#     if len(comparison_df) > 0:\n",
    "#         axes[0, 0].bar(comparison_df['Dataset'], comparison_df['Avg_Difficulty'], \n",
    "#                        alpha=0.7, color='steelblue')\n",
    "#         axes[0, 0].set_title('Average Difficulty by Dataset', fontsize=14)\n",
    "#         axes[0, 0].set_xlabel('Dataset')\n",
    "#         axes[0, 0].set_ylabel('Average Difficulty')\n",
    "#         axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "#         axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "#         # Add error bars\n",
    "#         axes[0, 0].errorbar(comparison_df['Dataset'], comparison_df['Avg_Difficulty'], \n",
    "#                            yerr=comparison_df['Std_Difficulty'], \n",
    "#                            fmt='none', color='black', capsize=5)\n",
    "    \n",
    "#     # Plot 2: Topic statistics\n",
    "#     if len(comparison_df) > 0:\n",
    "#         x = np.arange(len(comparison_df))\n",
    "#         width = 0.35\n",
    "#         axes[0, 1].bar(x - width/2, comparison_df['N_Topics'], width, label='Number of Topics')\n",
    "#         axes[0, 1].bar(x + width/2, comparison_df['N_Documents']/1000, width, label='Documents (thousands)')\n",
    "#         axes[0, 1].set_title('Dataset Scale Comparison', fontsize=14)\n",
    "#         axes[0, 1].set_xlabel('Dataset')\n",
    "#         axes[0, 1].set_ylabel('Count')\n",
    "#         axes[0, 1].set_xticks(x)\n",
    "#         axes[0, 1].set_xticklabels(comparison_df['Dataset'], rotation=45)\n",
    "#         axes[0, 1].legend()\n",
    "#         axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "#     # Plot 3: Network density vs difficulty\n",
    "#     if len(comparison_df) > 0:\n",
    "#         scatter = axes[0, 2].scatter(comparison_df['Network_Density'], comparison_df['Avg_Difficulty'],\n",
    "#                                     s=comparison_df['N_Topics']*50, alpha=0.7,\n",
    "#                                     c=range(len(comparison_df)), cmap='tab20')\n",
    "#         axes[0, 2].set_title('Network Density vs Average Difficulty', fontsize=14)\n",
    "#         axes[0, 2].set_xlabel('Network Density')\n",
    "#         axes[0, 2].set_ylabel('Average Difficulty')\n",
    "#         axes[0, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "#         # Add dataset labels\n",
    "#         for i, row in comparison_df.iterrows():\n",
    "#             axes[0, 2].annotate(row['Dataset'], \n",
    "#                                (row['Network_Density'], row['Avg_Difficulty']),\n",
    "#                                fontsize=9, alpha=0.8)\n",
    "    \n",
    "#     # Plot 4: Perplexity comparison\n",
    "#     if len(comparison_df) > 0:\n",
    "#         axes[1, 0].bar(comparison_df['Dataset'], comparison_df['Perplexity'], \n",
    "#                        alpha=0.7, color='purple')\n",
    "#         axes[1, 0].set_title('Model Perplexity by Dataset', fontsize=14)\n",
    "#         axes[1, 0].set_xlabel('Dataset')\n",
    "#         axes[1, 0].set_ylabel('Perplexity')\n",
    "#         axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "#         axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "#     # Plot 5: Difficulty range comparison\n",
    "#     if len(comparison_df) > 0:\n",
    "#         axes[1, 1].bar(comparison_df['Dataset'], comparison_df['Difficulty_Range'], \n",
    "#                        alpha=0.7, color='coral')\n",
    "#         axes[1, 1].set_title('Difficulty Range by Dataset', fontsize=14)\n",
    "#         axes[1, 1].set_xlabel('Dataset')\n",
    "#         axes[1, 1].set_ylabel('Difficulty Range (Max - Min)')\n",
    "#         axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "#         axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "#     # Plot 6: Dataset clustering based on metrics\n",
    "#     if len(comparison_df) > 1:\n",
    "#         try:\n",
    "#             from sklearn.preprocessing import StandardScaler\n",
    "#             from sklearn.cluster import KMeans\n",
    "#             from sklearn.decomposition import PCA\n",
    "            \n",
    "#             # Prepare data for clustering\n",
    "#             cluster_data = comparison_df[['Avg_Difficulty', 'Std_Difficulty', \n",
    "#                                          'Network_Density', 'Difficulty_Range', 'Perplexity']].values\n",
    "            \n",
    "#             # Handle NaN values\n",
    "#             cluster_data = np.nan_to_num(cluster_data)\n",
    "#             cluster_data = StandardScaler().fit_transform(cluster_data)\n",
    "            \n",
    "#             # Apply K-means clustering\n",
    "#             kmeans = KMeans(n_clusters=min(3, len(cluster_data)), random_state=42)\n",
    "#             clusters = kmeans.fit_predict(cluster_data)\n",
    "            \n",
    "#             # Use PCA for 2D visualization\n",
    "#             pca = PCA(n_components=2)\n",
    "#             pca_result = pca.fit_transform(cluster_data)\n",
    "            \n",
    "#             scatter = axes[1, 2].scatter(pca_result[:, 0], pca_result[:, 1], \n",
    "#                                         c=clusters, s=100, alpha=0.7, cmap='Set2')\n",
    "            \n",
    "#             # Add dataset labels\n",
    "#             for i, (x, y) in enumerate(pca_result):\n",
    "#                 axes[1, 2].annotate(comparison_df.iloc[i]['Dataset'], \n",
    "#                                    (x, y), fontsize=10, alpha=0.8)\n",
    "            \n",
    "#             axes[1, 2].set_title('Dataset Clustering Based on Metrics', fontsize=14)\n",
    "#             axes[1, 2].set_xlabel(f'PCA Component 1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "#             axes[1, 2].set_ylabel(f'PCA Component 2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "#             axes[1, 2].grid(True, alpha=0.3)\n",
    "            \n",
    "#             # Add legend for clusters\n",
    "#             from matplotlib.lines import Line2D\n",
    "#             legend_elements = [Line2D([0], [0], marker='o', color='w', \n",
    "#                                      markerfacecolor=plt.cm.Set2(i), markersize=10,\n",
    "#                                      label=f'Cluster {i}') \n",
    "#                               for i in range(len(np.unique(clusters)))]\n",
    "#             axes[1, 2].legend(handles=legend_elements, loc='best')\n",
    "#         except Exception as e:\n",
    "#             axes[1, 2].text(0.5, 0.5, f'Clustering error: {str(e)}', \n",
    "#                            ha='center', va='center', fontsize=10)\n",
    "#             axes[1, 2].set_title('Dataset Clustering Based on Metrics', fontsize=14)\n",
    "#     else:\n",
    "#         axes[1, 2].text(0.5, 0.5, 'Insufficient data for clustering', \n",
    "#                        ha='center', va='center', fontsize=12)\n",
    "#         axes[1, 2].set_title('Dataset Clustering Based on Metrics', fontsize=14)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     # Save comparative analysis\n",
    "#     output_path = \"results/tmcl/topic_models/cross_dataset_comparison.png\"\n",
    "#     plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "#     plt.close()\n",
    "    \n",
    "#     print(f\"Cross-dataset comparison plot saved to: {output_path}\")\n",
    "    \n",
    "#     # Create comprehensive summary report\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"COMPREHENSIVE SUMMARY REPORT\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     print(\"\\n1. DATASET OVERVIEW:\")\n",
    "#     print(\"-\" * 40)\n",
    "#     for i, row in comparison_df.iterrows():\n",
    "#         print(f\"{row['Dataset']}:\")\n",
    "#         print(f\"  Topics: {row['N_Topics']}, Documents: {row['N_Documents']:,}\")\n",
    "#         print(f\"  Perplexity: {row['Perplexity']:.2f}\")\n",
    "#         print(f\"  Avg Difficulty: {row['Avg_Difficulty']:.3f} ± {row['Std_Difficulty']:.3f}\")\n",
    "#         print(f\"  Network Density: {row['Network_Density']:.3f}\")\n",
    "#         print()\n",
    "    \n",
    "#     print(\"\\n2. KEY FINDINGS:\")\n",
    "#     print(\"-\" * 40)\n",
    "    \n",
    "#     # Find hardest and easiest datasets\n",
    "#     if not comparison_df.empty:\n",
    "#         hardest_dataset = comparison_df.loc[comparison_df['Avg_Difficulty'].idxmax()]\n",
    "#         easiest_dataset = comparison_df.loc[comparison_df['Avg_Difficulty'].idxmin()]\n",
    "        \n",
    "#         print(f\"Hardest Dataset: {hardest_dataset['Dataset']} \"\n",
    "#               f\"(Difficulty: {hardest_dataset['Avg_Difficulty']:.3f})\")\n",
    "#         print(f\"Easiest Dataset: {easiest_dataset['Dataset']} \"\n",
    "#               f\"(Difficulty: {easiest_dataset['Avg_Difficulty']:.3f})\")\n",
    "#         print(f\"Difficulty Range Across Datasets: {comparison_df['Avg_Difficulty'].max() - comparison_df['Avg_Difficulty'].min():.3f}\")\n",
    "        \n",
    "#         # Calculate average network density\n",
    "#         avg_density = comparison_df['Network_Density'].mean()\n",
    "#         print(f\"\\nAverage Network Density: {avg_density:.3f}\")\n",
    "#         print(\"  Higher density indicates more interconnected topics\")\n",
    "#         print(\"  Lower density indicates more distinct, separate topics\")\n",
    "        \n",
    "#         # Text vs Image dataset comparison\n",
    "#         text_datasets = [d for d in comparison_df['Dataset'] if d in ['ag_news', 'imdb']]\n",
    "#         image_datasets = [d for d in comparison_df['Dataset'] if d not in ['ag_news', 'imdb']]\n",
    "        \n",
    "#         if text_datasets and image_datasets:\n",
    "#             text_avg_diff = comparison_df[comparison_df['Dataset'].isin(text_datasets)]['Avg_Difficulty'].mean()\n",
    "#             image_avg_diff = comparison_df[comparison_df['Dataset'].isin(image_datasets)]['Avg_Difficulty'].mean()\n",
    "            \n",
    "#             print(f\"\\nText vs Image Dataset Comparison:\")\n",
    "#             print(f\"  Text datasets avg difficulty: {text_avg_diff:.3f}\")\n",
    "#             print(f\"  Image datasets avg difficulty: {image_avg_diff:.3f}\")\n",
    "#             print(f\"  Difference: {abs(text_avg_diff - image_avg_diff):.3f}\")\n",
    "    \n",
    "#     print(\"\\n3. RECOMMENDATIONS FOR TMCL:\")\n",
    "#     print(\"-\" * 40)\n",
    "#     print(\"Based on the analysis, consider the following:\")\n",
    "#     print(\"1. For datasets with high average difficulty:\")\n",
    "#     print(\"   - Implement stronger regularization\")\n",
    "#     print(\"   - Consider curriculum learning with gradual difficulty increase\")\n",
    "#     print(\"   - Use more sophisticated difficulty metrics\")\n",
    "    \n",
    "#     print(\"\\n2. For datasets with low network density:\")\n",
    "#     print(\"   - Topics are more distinct, which is good for interpretability\")\n",
    "#     print(\"   - Consider topic-guided data augmentation\")\n",
    "    \n",
    "#     print(\"\\n3. For datasets with high difficulty variance:\")\n",
    "#     print(\"   - Implement adaptive sampling strategies\")\n",
    "#     print(\"   - Consider multi-stage training approaches\")\n",
    "    \n",
    "#     print(\"\\n4. GENERAL RECOMMENDATIONS:\")\n",
    "#     print(\"   - Use composite difficulty metric for balanced assessment\")\n",
    "#     print(\"   - Monitor topic evolution during training\")\n",
    "#     print(\"   - Validate difficulty scores with downstream task performance\")\n",
    "    \n",
    "#     # Save comprehensive summary\n",
    "#     summary_path = \"results/tmcl/topic_models/comprehensive_analysis_summary.txt\"\n",
    "#     with open(summary_path, 'w') as f:\n",
    "#         f.write(\"=\"*80 + \"\\n\")\n",
    "#         f.write(\"COMPREHENSIVE TOPIC MODELING ANALYSIS SUMMARY\\n\")\n",
    "#         f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "#         f.write(\"1. DATASET OVERVIEW\\n\")\n",
    "#         f.write(\"-\" * 40 + \"\\n\")\n",
    "#         for i, row in comparison_df.iterrows():\n",
    "#             f.write(f\"{row['Dataset'].upper()}:\\n\")\n",
    "#             f.write(f\"  Topics: {row['N_Topics']}\\n\")\n",
    "#             f.write(f\"  Documents: {row['N_Documents']:,}\\n\")\n",
    "#             f.write(f\"  Perplexity: {row['Perplexity']:.2f}\\n\")\n",
    "#             f.write(f\"  Average Difficulty: {row['Avg_Difficulty']:.3f} ± {row['Std_Difficulty']:.3f}\\n\")\n",
    "#             f.write(f\"  Difficulty Range: {row['Difficulty_Range']:.3f}\\n\")\n",
    "#             f.write(f\"  Network Density: {row['Network_Density']:.3f}\\n\")\n",
    "#             f.write(f\"  Average Topic Weight: {row['Avg_Topic_Weight']:.3f}\\n\")\n",
    "#             f.write(f\"  Topic Weight Std: {row['Std_Topic_Weight']:.3f}\\n\\n\")\n",
    "        \n",
    "#         f.write(\"2. KEY FINDINGS\\n\")\n",
    "#         f.write(\"-\" * 40 + \"\\n\")\n",
    "#         if not comparison_df.empty:\n",
    "#             f.write(f\"Hardest Dataset: {hardest_dataset['Dataset']} \"\n",
    "#                    f\"(Difficulty: {hardest_dataset['Avg_Difficulty']:.3f})\\n\")\n",
    "#             f.write(f\"Easiest Dataset: {easiest_dataset['Dataset']} \"\n",
    "#                    f\"(Difficulty: {easiest_dataset['Avg_Difficulty']:.3f})\\n\")\n",
    "#             f.write(f\"Overall Difficulty Range: {comparison_df['Avg_Difficulty'].max() - comparison_df['Avg_Difficulty'].min():.3f}\\n\")\n",
    "#             f.write(f\"Average Network Density: {avg_density:.3f}\\n\")\n",
    "            \n",
    "#             if text_datasets and image_datasets:\n",
    "#                 f.write(f\"\\nText vs Image Comparison:\\n\")\n",
    "#                 f.write(f\"  Text datasets: {text_avg_diff:.3f}\\n\")\n",
    "#                 f.write(f\"  Image datasets: {image_avg_diff:.3f}\\n\")\n",
    "#                 f.write(f\"  Difference: {abs(text_avg_diff - image_avg_diff):.3f}\\n\")\n",
    "        \n",
    "#         f.write(\"\\n3. RECOMMENDATIONS FOR TMCL\\n\")\n",
    "#         f.write(\"-\" * 40 + \"\\n\")\n",
    "#         f.write(\"For Topic Modeling Curriculum Learning implementation:\\n\")\n",
    "#         f.write(\"1. Use adaptive difficulty sampling based on topic distributions\\n\")\n",
    "#         f.write(\"2. Monitor topic evolution during training\\n\")\n",
    "#         f.write(\"3. Validate with multiple difficulty metrics (entropy, max_prob, composite)\\n\")\n",
    "#         f.write(\"4. Consider dataset-specific difficulty thresholds\\n\")\n",
    "#         f.write(\"5. Implement curriculum learning for high-difficulty datasets\\n\")\n",
    "#         f.write(\"6. Use topic networks to understand relationships between concepts\\n\")\n",
    "#         f.write(\"7. For text data, leverage topic-word distributions for interpretability\\n\")\n",
    "#         f.write(\"8. For image data, consider feature-space clustering for topic assignment\\n\")\n",
    "    \n",
    "#     print(f\"\\nComprehensive summary saved to: {summary_path}\")\n",
    "    \n",
    "#     # Save comparison dataframe for future analysis\n",
    "#     comparison_df_path = \"results/tmcl/topic_models/dataset_comparison.csv\"\n",
    "#     comparison_df.to_csv(comparison_df_path, index=False)\n",
    "#     print(f\"Comparison data saved to: {comparison_df_path}\")\n",
    "# else:\n",
    "#     print(\"No valid analysis data available for cross-dataset comparison\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"ANALYSIS COMPLETE!\")\n",
    "# print(f\"All visualizations and reports saved to: results/tmcl/topic_models/\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Print final summary\n",
    "# print(\"\\nFINAL SUMMARY:\")\n",
    "# print(\"-\" * 40)\n",
    "# for dataset_name in datasets:\n",
    "#     if dataset_name in all_analysis_results:\n",
    "#         analysis = all_analysis_results[dataset_name]\n",
    "#         if analysis and 'difficulty_stats' in analysis:\n",
    "#             stats = analysis['difficulty_stats']\n",
    "#             print(f\"{dataset_name}:\")\n",
    "#             print(f\"  Composite Difficulty: {stats.get('overall_mean_composite', 0):.3f} ± {stats.get('overall_std_composite', 0):.3f}\")\n",
    "#             print(f\"  Topics: {analysis.get('n_topics', 0)}, Documents: {analysis.get('n_documents', 0):,}\")\n",
    "#             print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8823cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_interactive_topic_network(dataset_name, results, feature_names=None, top_n_words=10):\n",
    "#     \"\"\"\n",
    "#     Create interactive topic network visualization with actual topic labels\n",
    "    \n",
    "#     Args:\n",
    "#         dataset_name: Name of the dataset\n",
    "#         results: Processing results containing LDA model and topic distributions\n",
    "#         feature_names: Vocabulary for text datasets\n",
    "#         top_n_words: Number of top words to show for each topic\n",
    "#     \"\"\"\n",
    "    \n",
    "#     print(f\"\\nCreating interactive topic network for {dataset_name}...\")\n",
    "    \n",
    "#     # Extract components from results\n",
    "#     lda_model = results['lda_model']\n",
    "#     topic_distributions = results['topic_distributions']\n",
    "#     difficulty_scores = results['difficulty_scores']\n",
    "    \n",
    "#     # Get topic components\n",
    "#     topic_vectors = lda_model.components_  # Shape: n_topics x n_features\n",
    "#     topic_similarity = cosine_similarity(topic_vectors)\n",
    "    \n",
    "#     # Calculate topic weights\n",
    "#     topic_weights = np.sum(topic_distributions, axis=0)\n",
    "#     topic_weights_norm = topic_weights / np.sum(topic_weights)\n",
    "    \n",
    "#     # Create topic relationship graph\n",
    "#     G = nx.Graph()\n",
    "    \n",
    "#     # Get actual topic labels (top words for each topic)\n",
    "#     topic_labels = []\n",
    "#     topic_full_labels = []  # Full label with all top words\n",
    "#     topic_words_list = []   # List of top words for each topic\n",
    "    \n",
    "#     for i in range(topic_vectors.shape[0]):\n",
    "#         # Get top words for this topic\n",
    "#         if feature_names is not None and len(feature_names) > 0:\n",
    "#             topic_vector = topic_vectors[i]\n",
    "#             # Get indices of top words for this topic\n",
    "#             top_indices = np.argsort(topic_vector)[-top_n_words:][::-1]\n",
    "#             top_words = []\n",
    "#             for idx in top_indices:\n",
    "#                 if idx < len(feature_names):\n",
    "#                     if isinstance(feature_names[idx], (int, np.integer)):\n",
    "#                         # If it's an integer, it might be an index\n",
    "#                         top_words.append(f\"word_{idx}\")\n",
    "#                     else:\n",
    "#                         top_words.append(str(feature_names[idx]))\n",
    "#                 else:\n",
    "#                     top_words.append(f\"word_{idx}\")\n",
    "            \n",
    "#             # Create short label (first 3 words)\n",
    "#             short_label = f\"T{i}: {', '.join(top_words[:3])}\"\n",
    "#             # Create full label\n",
    "#             full_label = f\"Topic {i}<br>Top words: {', '.join(top_words)}\"\n",
    "#             topic_labels.append(short_label)\n",
    "#             topic_full_labels.append(full_label)\n",
    "#             topic_words_list.append(top_words)\n",
    "#         else:\n",
    "#             # For image datasets or when no feature names\n",
    "#             topic_labels.append(f\"Topic {i}\")\n",
    "#             topic_full_labels.append(f\"Topic {i}\")\n",
    "#             topic_words_list.append([])\n",
    "    \n",
    "#     # Add nodes with topic information\n",
    "#     for i in range(topic_vectors.shape[0]):\n",
    "#         # Calculate average difficulty for documents where this topic is dominant\n",
    "#         topic_doc_mask = np.argmax(topic_distributions, axis=1) == i\n",
    "#         if np.sum(topic_doc_mask) > 0:\n",
    "#             avg_difficulty = np.mean(difficulty_scores['entropy'][topic_doc_mask])\n",
    "#             avg_composite = np.mean(difficulty_scores['composite'][topic_doc_mask])\n",
    "#         else:\n",
    "#             avg_difficulty = 0.5\n",
    "#             avg_composite = 0.5\n",
    "        \n",
    "#         # Node size based on topic weight\n",
    "#         node_size = topic_weights_norm[i] * 100 + 10\n",
    "        \n",
    "#         # Number of documents for this topic\n",
    "#         n_docs = np.sum(topic_doc_mask)\n",
    "        \n",
    "#         # Add to graph\n",
    "#         G.add_node(i,\n",
    "#                    label=topic_labels[i],\n",
    "#                    full_label=topic_full_labels[i],\n",
    "#                    top_words=topic_words_list[i],\n",
    "#                    size=node_size,\n",
    "#                    avg_difficulty=avg_difficulty,\n",
    "#                    avg_composite=avg_composite,\n",
    "#                    weight=topic_weights_norm[i],\n",
    "#                    n_docs=n_docs,\n",
    "#                    topic_id=i)\n",
    "    \n",
    "#     # Add edges based on similarity\n",
    "#     if len(topic_similarity) > 1:\n",
    "#         # Flatten similarity matrix excluding diagonal\n",
    "#         flat_similarities = topic_similarity[np.triu_indices_from(topic_similarity, k=1)]\n",
    "#         if len(flat_similarities) > 0:\n",
    "#             threshold = np.percentile(flat_similarities, 75)\n",
    "#         else:\n",
    "#             threshold = 0.5\n",
    "#     else:\n",
    "#         threshold = 0.5\n",
    "    \n",
    "#     edges = []\n",
    "#     edge_weights = []\n",
    "    \n",
    "#     for i in range(topic_vectors.shape[0]):\n",
    "#         for j in range(i+1, topic_vectors.shape[0]):\n",
    "#             if topic_similarity[i, j] > threshold:\n",
    "#                 G.add_edge(i, j, weight=topic_similarity[i, j])\n",
    "#                 edges.append((i, j))\n",
    "#                 edge_weights.append(topic_similarity[i, j])\n",
    "    \n",
    "#     # Use spring layout for node positions\n",
    "#     if len(G.nodes()) > 0:\n",
    "#         pos = nx.spring_layout(G, k=1.5, iterations=50, seed=42)\n",
    "#     else:\n",
    "#         print(f\"  Warning: No nodes in graph for {dataset_name}\")\n",
    "#         return None, None\n",
    "    \n",
    "#     # Create edge traces\n",
    "#     edge_x = []\n",
    "#     edge_y = []\n",
    "#     edge_hovertext = []\n",
    "    \n",
    "#     for edge in G.edges():\n",
    "#         x0, y0 = pos[edge[0]]\n",
    "#         x1, y1 = pos[edge[1]]\n",
    "#         edge_x.extend([x0, x1, None])\n",
    "#         edge_y.extend([y0, y1, None])\n",
    "        \n",
    "#         # Get edge weight\n",
    "#         weight = G[edge[0]][edge[1]]['weight']\n",
    "        \n",
    "#         # Edge hover text\n",
    "#         edge_hovertext.append(\n",
    "#             f\"Topic {edge[0]} ↔ Topic {edge[1]}<br>\"\n",
    "#             f\"Similarity: {weight:.3f}\"\n",
    "#         )\n",
    "    \n",
    "#     edge_trace = go.Scatter(\n",
    "#         x=edge_x, y=edge_y,\n",
    "#         line=dict(width=1.5, color='#888'),\n",
    "#         hoverinfo='none',\n",
    "#         mode='lines',\n",
    "#         showlegend=False\n",
    "#     )\n",
    "    \n",
    "#     # Create node traces\n",
    "#     node_x = []\n",
    "#     node_y = []\n",
    "#     node_text = []\n",
    "#     node_hovertext = []\n",
    "#     node_sizes = []\n",
    "#     node_colors = []\n",
    "    \n",
    "#     for node in G.nodes():\n",
    "#         x, y = pos[node]\n",
    "#         node_x.append(x)\n",
    "#         node_y.append(y)\n",
    "        \n",
    "#         # Get node attributes\n",
    "#         node_data = G.nodes[node]\n",
    "#         node_text.append(node_data['label'])\n",
    "        \n",
    "#         # Create hover text with topic words\n",
    "#         if node_data['top_words']:\n",
    "#             words_str = \"<br>\".join([f\"• {word}\" for word in node_data['top_words']])\n",
    "#         else:\n",
    "#             words_str = \"No words available\"\n",
    "        \n",
    "#         hover_text = (\n",
    "#             f\"<b>Topic {node_data['topic_id']}</b><br>\"\n",
    "#             f\"{words_str}<br><br>\"\n",
    "#             f\"Average Difficulty (Entropy): {node_data['avg_difficulty']:.3f}<br>\"\n",
    "#             f\"Average Difficulty (Composite): {node_data['avg_composite']:.3f}<br>\"\n",
    "#             f\"Topic Weight: {node_data['weight']:.3f}<br>\"\n",
    "#             f\"Number of Documents: {node_data['n_docs']}\"\n",
    "#         )\n",
    "#         node_hovertext.append(hover_text)\n",
    "        \n",
    "#         # Node size based on topic weight\n",
    "#         node_sizes.append(node_data['size'])\n",
    "        \n",
    "#         # Node color based on average difficulty\n",
    "#         node_colors.append(node_data['avg_composite'])\n",
    "    \n",
    "#     node_trace = go.Scatter(\n",
    "#         x=node_x, y=node_y,\n",
    "#         mode='markers+text',\n",
    "#         text=node_text,\n",
    "#         textposition=\"top center\",\n",
    "#         hoverinfo='text',\n",
    "#         hovertext=node_hovertext,\n",
    "#         marker=dict(\n",
    "#             showscale=True,\n",
    "#             colorscale='RdYlBu_r',  # Reversed: red=hard, blue=easy\n",
    "#             color=node_colors,\n",
    "#             size=node_sizes,\n",
    "#             colorbar=dict(\n",
    "#                 thickness=15,\n",
    "#                 title='Average Difficulty<br>(Composite)',\n",
    "#                 xanchor='left',\n",
    "#                 title_side='right'  # Fixed: changed from titleside to title_side\n",
    "#             ),\n",
    "#             line=dict(width=2, color='white')\n",
    "#         ),\n",
    "#         showlegend=False\n",
    "#     )\n",
    "    \n",
    "#     # Create the figure\n",
    "#     fig = go.Figure(data=[edge_trace, node_trace],\n",
    "#                    layout=go.Layout(\n",
    "#                        title=f'Interactive Topic Network - {dataset_name}<br>'\n",
    "#                             f'Node size ∝ Topic weight, Color ∝ Average difficulty',\n",
    "#                        titlefont_size=16,\n",
    "#                        showlegend=False,\n",
    "#                        hovermode='closest',\n",
    "#                        margin=dict(b=20, l=5, r=5, t=40),\n",
    "#                        annotations=[dict(\n",
    "#                            text=f\"Network Statistics:<br>\"\n",
    "#                                f\"Nodes: {G.number_of_nodes()}<br>\"\n",
    "#                                f\"Edges: {G.number_of_edges()}<br>\"\n",
    "#                                f\"Edge threshold: {threshold:.3f}\",\n",
    "#                            showarrow=False,\n",
    "#                            xref=\"paper\", yref=\"paper\",\n",
    "#                            x=0.02, y=0.02,\n",
    "#                            bgcolor=\"white\",\n",
    "#                            bordercolor=\"black\",\n",
    "#                            borderwidth=1,\n",
    "#                            borderpad=4\n",
    "#                        )],\n",
    "#                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "#                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
    "#                    ))\n",
    "    \n",
    "#     # Save as HTML\n",
    "#     output_path = f\"results/tmcl/topic_models/{dataset_name}_interactive_network.html\"\n",
    "#     os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "#     fig.write_html(output_path)\n",
    "#     print(f\"  Interactive network saved to: {output_path}\")\n",
    "    \n",
    "#     # Also create a static version if kaleido is available\n",
    "#     try:\n",
    "#         fig.update_layout(\n",
    "#             width=1200,\n",
    "#             height=800,\n",
    "#             font=dict(size=12)\n",
    "#         )\n",
    "#         static_output_path = f\"results/tmcl/topic_models/{dataset_name}_interactive_network_static.png\"\n",
    "#         fig.write_image(static_output_path)\n",
    "#         print(f\"  Static version saved to: {static_output_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"  Note: Could not save static image. Install kaleido: pip install kaleido\")\n",
    "    \n",
    "#     return fig, G\n",
    "\n",
    "# def create_interactive_topic_dashboard(dataset_name, results, feature_names=None):\n",
    "#     \"\"\"\n",
    "#     Create comprehensive interactive dashboard for topic analysis\n",
    "    \n",
    "#     Args:\n",
    "#         dataset_name: Name of the dataset\n",
    "#         results: Processing results\n",
    "#         feature_names: Vocabulary for text datasets\n",
    "#     \"\"\"\n",
    "    \n",
    "#     print(f\"\\nCreating interactive dashboard for {dataset_name}...\")\n",
    "    \n",
    "#     # Extract components\n",
    "#     lda_model = results['lda_model']\n",
    "#     topic_distributions = results['topic_distributions']\n",
    "#     difficulty_scores = results['difficulty_scores']\n",
    "#     labels = results.get('labels', None)\n",
    "    \n",
    "#     # Calculate topic statistics\n",
    "#     topic_vectors = lda_model.components_\n",
    "#     topic_weights = np.sum(topic_distributions, axis=0)\n",
    "#     topic_weights_norm = topic_weights / np.sum(topic_weights)\n",
    "    \n",
    "#     # Create DataFrame for analysis\n",
    "#     analysis_data = []\n",
    "#     for i in range(topic_vectors.shape[0]):\n",
    "#         topic_doc_mask = np.argmax(topic_distributions, axis=1) == i\n",
    "#         n_docs = np.sum(topic_doc_mask)\n",
    "        \n",
    "#         if n_docs > 0:\n",
    "#             avg_entropy = np.mean(difficulty_scores['entropy'][topic_doc_mask])\n",
    "#             avg_max_prob = np.mean(difficulty_scores['max_prob'][topic_doc_mask])\n",
    "#             avg_composite = np.mean(difficulty_scores['composite'][topic_doc_mask])\n",
    "#             topic_strength = np.mean(topic_distributions[topic_doc_mask, i])\n",
    "            \n",
    "#             # Get top words\n",
    "#             top_words = []\n",
    "#             if feature_names is not None and len(feature_names) > 0:\n",
    "#                 topic_vector = topic_vectors[i]\n",
    "#                 top_indices = np.argsort(topic_vector)[-10:][::-1]\n",
    "#                 for idx in top_indices:\n",
    "#                     if idx < len(feature_names):\n",
    "#                         if isinstance(feature_names[idx], (int, np.integer)):\n",
    "#                             top_words.append(f\"word_{idx}\")\n",
    "#                         else:\n",
    "#                             top_words.append(str(feature_names[idx]))\n",
    "#                     else:\n",
    "#                         top_words.append(f\"word_{idx}\")\n",
    "            \n",
    "#             analysis_data.append({\n",
    "#                 'Topic': f'T{i}',\n",
    "#                 'Topic_ID': i,\n",
    "#                 'N_Docs': n_docs,\n",
    "#                 'Doc_Proportion': n_docs / len(topic_distributions),\n",
    "#                 'Avg_Entropy_Difficulty': avg_entropy,\n",
    "#                 'Avg_MaxProb_Difficulty': avg_max_prob,\n",
    "#                 'Avg_Composite_Difficulty': avg_composite,\n",
    "#                 'Topic_Strength': topic_strength,\n",
    "#                 'Topic_Weight': topic_weights_norm[i],\n",
    "#                 'Top_Words': ', '.join(top_words) if top_words else 'N/A'\n",
    "#             })\n",
    "    \n",
    "#     analysis_df = pd.DataFrame(analysis_data)\n",
    "    \n",
    "#     # Create interactive dashboard with multiple plots\n",
    "#     fig = make_subplots(\n",
    "#         rows=2, cols=2,\n",
    "#         subplot_titles=(\n",
    "#             'Topic Difficulty Distribution',\n",
    "#             'Topic Weight vs Difficulty',\n",
    "#             'Difficulty Metrics by Topic',\n",
    "#             'Topic Document Distribution'\n",
    "#         ),\n",
    "#         specs=[\n",
    "#             [{'type': 'scatter'}, {'type': 'scatter'}],\n",
    "#             [{'type': 'bar'}, {'type': 'pie'}]\n",
    "#         ],\n",
    "#         vertical_spacing=0.12,\n",
    "#         horizontal_spacing=0.1\n",
    "#     )\n",
    "    \n",
    "#     # Plot 1: Topic Difficulty Distribution (scatter plot)\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=analysis_df['Topic'],\n",
    "#             y=analysis_df['Avg_Composite_Difficulty'],\n",
    "#             mode='markers+text',\n",
    "#             text=analysis_df['Topic'],\n",
    "#             textposition='top center',\n",
    "#             marker=dict(\n",
    "#                 size=analysis_df['Topic_Weight'] * 100,\n",
    "#                 color=analysis_df['Avg_Composite_Difficulty'],\n",
    "#                 colorscale='RdYlBu_r',\n",
    "#                 showscale=True,\n",
    "#                 colorbar=dict(\n",
    "#                     title='Difficulty',\n",
    "#                     x=0.45,\n",
    "#                     y=0.95,\n",
    "#                     len=0.3\n",
    "#                 ),\n",
    "#                 line=dict(width=2, color='DarkSlateGrey')\n",
    "#             ),\n",
    "#             customdata=analysis_df[['Top_Words', 'N_Docs', 'Topic_Weight']].values,\n",
    "#             hovertemplate=(\n",
    "#                 '<b>Topic: %{x}</b><br>'\n",
    "#                 'Difficulty: %{y:.3f}<br>'\n",
    "#                 'Top Words: %{customdata[0]}<br>'\n",
    "#                 'Documents: %{customdata[1]}<br>'\n",
    "#                 'Weight: %{customdata[2]:.3f}<br>'\n",
    "#                 '<extra></extra>'\n",
    "#             ),\n",
    "#             name='Topics'\n",
    "#         ),\n",
    "#         row=1, col=1\n",
    "#     )\n",
    "    \n",
    "#     # Add difficulty thresholds\n",
    "#     if len(analysis_df) > 0:\n",
    "#         mean_diff = analysis_df['Avg_Composite_Difficulty'].mean()\n",
    "#         std_diff = analysis_df['Avg_Composite_Difficulty'].std()\n",
    "        \n",
    "#         fig.add_hline(y=mean_diff, line_dash=\"dash\", line_color=\"gray\", \n",
    "#                       annotation_text=f\"Mean: {mean_diff:.3f}\", \n",
    "#                       annotation_position=\"top right\", row=1, col=1)\n",
    "#         fig.add_hline(y=mean_diff + std_diff, line_dash=\"dot\", line_color=\"red\", \n",
    "#                       annotation_text=f\"+1 std\", annotation_position=\"top right\", row=1, col=1)\n",
    "#         fig.add_hline(y=mean_diff - std_diff, line_dash=\"dot\", line_color=\"green\", \n",
    "#                       annotation_text=f\"-1 std\", annotation_position=\"top right\", row=1, col=1)\n",
    "    \n",
    "#     # Plot 2: Topic Weight vs Difficulty (bubble chart)\n",
    "#     if len(analysis_df) > 0:\n",
    "#         fig.add_trace(\n",
    "#             go.Scatter(\n",
    "#                 x=analysis_df['Topic_Weight'],\n",
    "#                 y=analysis_df['Avg_Composite_Difficulty'],\n",
    "#                 mode='markers+text',\n",
    "#                 text=analysis_df['Topic'],\n",
    "#                 textposition='top center',\n",
    "#                 marker=dict(\n",
    "#                     size=analysis_df['N_Docs'] / max(analysis_df['N_Docs']) * 50 + 10 if max(analysis_df['N_Docs']) > 0 else 20,\n",
    "#                     color=analysis_df['Avg_Composite_Difficulty'],\n",
    "#                     colorscale='Viridis',\n",
    "#                     showscale=False\n",
    "#                 ),\n",
    "#                 customdata=analysis_df[['Top_Words', 'N_Docs', 'Topic_ID']].values,\n",
    "#                 hovertemplate=(\n",
    "#                     '<b>Topic: %{text}</b><br>'\n",
    "#                     'Weight: %{x:.3f}<br>'\n",
    "#                     'Difficulty: %{y:.3f}<br>'\n",
    "#                     'Top Words: %{customdata[0]}<br>'\n",
    "#                     'Documents: %{customdata[1]}<br>'\n",
    "#                     'ID: %{customdata[2]}<br>'\n",
    "#                     '<extra></extra>'\n",
    "#                 ),\n",
    "#                 name='Weight vs Difficulty'\n",
    "#             ),\n",
    "#             row=1, col=2\n",
    "#         )\n",
    "        \n",
    "#         # Add trend line\n",
    "#         if len(analysis_df) > 1:\n",
    "#             z = np.polyfit(analysis_df['Topic_Weight'], analysis_df['Avg_Composite_Difficulty'], 1)\n",
    "#             p = np.poly1d(z)\n",
    "#             x_range = np.linspace(analysis_df['Topic_Weight'].min(), analysis_df['Topic_Weight'].max(), 100)\n",
    "#             fig.add_trace(\n",
    "#                 go.Scatter(\n",
    "#                     x=x_range,\n",
    "#                     y=p(x_range),\n",
    "#                     mode='lines',\n",
    "#                     line=dict(color='red', dash='dash'),\n",
    "#                     name='Trend',\n",
    "#                     showlegend=False\n",
    "#                 ),\n",
    "#                 row=1, col=2\n",
    "#             )\n",
    "    \n",
    "#     # Plot 3: Difficulty Metrics by Topic (grouped bar chart)\n",
    "#     if len(analysis_df) > 0:\n",
    "#         for i, metric in enumerate(['Avg_Entropy_Difficulty', 'Avg_MaxProb_Difficulty', 'Avg_Composite_Difficulty']):\n",
    "#             fig.add_trace(\n",
    "#                 go.Bar(\n",
    "#                     x=analysis_df['Topic'],\n",
    "#                     y=analysis_df[metric],\n",
    "#                     name=metric.replace('Avg_', '').replace('_', ' '),\n",
    "#                     marker_color=px.colors.qualitative.Set2[i],\n",
    "#                     hovertemplate=(\n",
    "#                         '<b>Topic: %{x}</b><br>'\n",
    "#                         'Metric: %{fullData.name}<br>'\n",
    "#                         'Value: %{y:.3f}<br>'\n",
    "#                         '<extra></extra>'\n",
    "#                     ),\n",
    "#                     showlegend=True if i == 0 else False\n",
    "#                 ),\n",
    "#                 row=2, col=1\n",
    "#             )\n",
    "    \n",
    "#     # Plot 4: Topic Document Distribution (pie chart)\n",
    "#     if len(analysis_df) > 0:\n",
    "#         fig.add_trace(\n",
    "#             go.Pie(\n",
    "#                 labels=analysis_df['Topic'],\n",
    "#                 values=analysis_df['N_Docs'],\n",
    "#                 textinfo='label+percent',\n",
    "#                 textposition='inside',\n",
    "#                 hole=0.4,\n",
    "#                 marker=dict(\n",
    "#                     colors=px.colors.qualitative.Plotly,\n",
    "#                     line=dict(color='white', width=2)\n",
    "#                 ),\n",
    "#                 hovertemplate=(\n",
    "#                     '<b>Topic: %{label}</b><br>'\n",
    "#                     'Documents: %{value}<br>'\n",
    "#                     'Percentage: %{percent}<br>'\n",
    "#                     '<extra></extra>'\n",
    "#                 ),\n",
    "#                 name='Document Distribution'\n",
    "#             ),\n",
    "#             row=2, col=2\n",
    "#         )\n",
    "    \n",
    "#     # Update layout\n",
    "#     fig.update_layout(\n",
    "#         title=f'Interactive Topic Analysis Dashboard - {dataset_name}',\n",
    "#         title_font_size=20,\n",
    "#         showlegend=True,\n",
    "#         legend=dict(\n",
    "#             yanchor=\"top\",\n",
    "#             y=0.99,\n",
    "#             xanchor=\"left\",\n",
    "#             x=1.02,\n",
    "#             bgcolor='rgba(255, 255, 255, 0.8)',\n",
    "#             bordercolor='black',\n",
    "#             borderwidth=1\n",
    "#         ),\n",
    "#         hovermode='closest',\n",
    "#         height=1000,\n",
    "#         width=1400,\n",
    "#         template='plotly_white'\n",
    "#     )\n",
    "    \n",
    "#     # Update axes labels\n",
    "#     fig.update_xaxes(title_text=\"Topic\", row=1, col=1)\n",
    "#     fig.update_yaxes(title_text=\"Composite Difficulty\", row=1, col=1)\n",
    "#     fig.update_xaxes(title_text=\"Topic Weight\", row=1, col=2)\n",
    "#     fig.update_yaxes(title_text=\"Composite Difficulty\", row=1, col=2)\n",
    "#     fig.update_xaxes(title_text=\"Topic\", row=2, col=1)\n",
    "#     fig.update_yaxes(title_text=\"Difficulty Score\", row=2, col=1)\n",
    "    \n",
    "#     # Save dashboard\n",
    "#     dashboard_path = f\"results/tmcl/topic_models/{dataset_name}_interactive_dashboard.html\"\n",
    "#     os.makedirs(os.path.dirname(dashboard_path), exist_ok=True)\n",
    "#     fig.write_html(dashboard_path)\n",
    "#     print(f\"  Interactive dashboard saved to: {dashboard_path}\")\n",
    "    \n",
    "#     # Also save static version if kaleido is available\n",
    "#     try:\n",
    "#         static_dashboard_path = f\"results/tmcl/topic_models/{dataset_name}_interactive_dashboard.png\"\n",
    "#         fig.write_image(static_dashboard_path, width=1400, height=1000)\n",
    "#         print(f\"  Static dashboard saved to: {static_dashboard_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"  Note: Could not save static image. Install kaleido: pip install kaleido\")\n",
    "    \n",
    "#     return fig\n",
    "\n",
    "# def create_topic_evolution_timeline(dataset_name, results, feature_names=None, n_samples_per_topic=100):\n",
    "#     \"\"\"\n",
    "#     Create interactive timeline showing topic evolution and difficulty\n",
    "    \n",
    "#     Args:\n",
    "#         dataset_name: Name of the dataset\n",
    "#         results: Processing results\n",
    "#         feature_names: Vocabulary for text datasets\n",
    "#         n_samples_per_topic: Number of samples to show per topic\n",
    "#     \"\"\"\n",
    "    \n",
    "#     print(f\"\\nCreating topic evolution timeline for {dataset_name}...\")\n",
    "    \n",
    "#     # Extract components\n",
    "#     lda_model = results['lda_model']\n",
    "#     topic_distributions = results['topic_distributions']\n",
    "#     difficulty_scores = results['difficulty_scores']\n",
    "    \n",
    "#     # Get topic components\n",
    "#     topic_vectors = lda_model.components_\n",
    "#     n_topics = topic_vectors.shape[0]\n",
    "    \n",
    "#     # Create figure\n",
    "#     fig = go.Figure()\n",
    "    \n",
    "#     # Get top words for each topic for hover text\n",
    "#     topic_words = []\n",
    "#     for i in range(n_topics):\n",
    "#         if feature_names is not None and len(feature_names) > 0:\n",
    "#             topic_vector = topic_vectors[i]\n",
    "#             top_indices = np.argsort(topic_vector)[-5:][::-1]\n",
    "#             words = []\n",
    "#             for idx in top_indices:\n",
    "#                 if idx < len(feature_names):\n",
    "#                     if isinstance(feature_names[idx], (int, np.integer)):\n",
    "#                         words.append(f\"word_{idx}\")\n",
    "#                     else:\n",
    "#                         words.append(str(feature_names[idx]))\n",
    "#                 else:\n",
    "#                     words.append(f\"word_{idx}\")\n",
    "#             topic_words.append(words)\n",
    "#         else:\n",
    "#             topic_words.append([])\n",
    "    \n",
    "#     # For each topic, create a timeline trace\n",
    "#     colors = px.colors.qualitative.Plotly\n",
    "    \n",
    "#     for i in range(n_topics):\n",
    "#         # Get indices of documents where this topic is dominant\n",
    "#         topic_doc_mask = np.argmax(topic_distributions, axis=1) == i\n",
    "        \n",
    "#         if np.sum(topic_doc_mask) > 0:\n",
    "#             # Get subset of documents for this topic\n",
    "#             topic_doc_indices = np.where(topic_doc_mask)[0]\n",
    "            \n",
    "#             if len(topic_doc_indices) > n_samples_per_topic:\n",
    "#                 # Sample documents\n",
    "#                 sampled_indices = np.random.choice(topic_doc_indices, n_samples_per_topic, replace=False)\n",
    "#             else:\n",
    "#                 sampled_indices = topic_doc_indices\n",
    "            \n",
    "#             # Get topic strengths and difficulties for these documents\n",
    "#             topic_strengths = topic_distributions[sampled_indices, i]\n",
    "#             doc_difficulties = difficulty_scores['composite'][sampled_indices]\n",
    "            \n",
    "#             # Sort by topic strength\n",
    "#             sort_idx = np.argsort(topic_strengths)\n",
    "#             topic_strengths = topic_strengths[sort_idx]\n",
    "#             doc_difficulties = doc_difficulties[sort_idx]\n",
    "            \n",
    "#             # Create hover text\n",
    "#             hover_texts = []\n",
    "#             for j, idx in enumerate(sampled_indices[sort_idx]):\n",
    "#                 hover_text = (\n",
    "#                     f\"<b>Topic {i}</b><br>\"\n",
    "#                     f\"Document {idx}<br>\"\n",
    "#                     f\"Topic Strength: {topic_strengths[j]:.3f}<br>\"\n",
    "#                     f\"Difficulty: {doc_difficulties[j]:.3f}\"\n",
    "#                 )\n",
    "#                 if topic_words[i]:\n",
    "#                     hover_text += f\"<br>Top words: {', '.join(topic_words[i])}\"\n",
    "#                 hover_texts.append(hover_text)\n",
    "            \n",
    "#             # Add trace for this topic\n",
    "#             fig.add_trace(go.Scatter(\n",
    "#                 x=topic_strengths,\n",
    "#                 y=doc_difficulties,\n",
    "#                 mode='markers',\n",
    "#                 name=f'Topic {i}',\n",
    "#                 text=hover_texts,\n",
    "#                 hoverinfo='text',\n",
    "#                 marker=dict(\n",
    "#                     size=8,\n",
    "#                     opacity=0.6,\n",
    "#                     color=colors[i % len(colors)],\n",
    "#                     line=dict(width=1, color='white')\n",
    "#                 ),\n",
    "#                 showlegend=True\n",
    "#             ))\n",
    "    \n",
    "#     # Update layout\n",
    "#     fig.update_layout(\n",
    "#         title=f'Topic Evolution Timeline - {dataset_name}',\n",
    "#         xaxis_title='Topic Strength (How dominant the topic is)',\n",
    "#         yaxis_title='Document Difficulty (Composite)',\n",
    "#         legend_title='Topics',\n",
    "#         hovermode='closest',\n",
    "#         height=800,\n",
    "#         width=1200,\n",
    "#         template='plotly_white'\n",
    "#     )\n",
    "    \n",
    "#     # Add trend line for each topic\n",
    "#     for i, trace in enumerate(fig.data):\n",
    "#         topic_strengths = trace.x\n",
    "#         difficulties = trace.y\n",
    "        \n",
    "#         if len(topic_strengths) > 1:\n",
    "#             # Calculate trend line\n",
    "#             z = np.polyfit(topic_strengths, difficulties, 1)\n",
    "#             p = np.poly1d(z)\n",
    "#             x_range = np.linspace(min(topic_strengths), max(topic_strengths), 100)\n",
    "            \n",
    "#             fig.add_trace(go.Scatter(\n",
    "#                 x=x_range,\n",
    "#                 y=p(x_range),\n",
    "#                 mode='lines',\n",
    "#                 line=dict(color=trace.marker.color, dash='dash', width=1),\n",
    "#                 showlegend=False,\n",
    "#                 hoverinfo='skip',\n",
    "#                 name=f'Trend Topic {i}'\n",
    "#             ))\n",
    "    \n",
    "#     # Save timeline\n",
    "#     timeline_path = f\"results/tmcl/topic_models/{dataset_name}_topic_timeline.html\"\n",
    "#     os.makedirs(os.path.dirname(timeline_path), exist_ok=True)\n",
    "#     fig.write_html(timeline_path)\n",
    "#     print(f\"  Topic timeline saved to: {timeline_path}\")\n",
    "    \n",
    "#     return fig\n",
    "\n",
    "# def create_interactive_topic_comparison(all_analysis_results):\n",
    "#     \"\"\"\n",
    "#     Create interactive comparison of all datasets\n",
    "    \n",
    "#     Args:\n",
    "#         all_analysis_results: Dictionary containing analysis results for all datasets\n",
    "#     \"\"\"\n",
    "    \n",
    "#     print(f\"\\nCreating interactive comparison across all datasets...\")\n",
    "    \n",
    "#     # Collect comparison data\n",
    "#     comparison_data = []\n",
    "    \n",
    "#     for dataset_name, analysis in all_analysis_results.items():\n",
    "#         if analysis is not None:\n",
    "#             topic_analysis = analysis.get('topic_analysis', pd.DataFrame())\n",
    "#             difficulty_stats = analysis.get('difficulty_stats', {})\n",
    "#             network_stats = analysis.get('network_stats', {})\n",
    "            \n",
    "#             if not topic_analysis.empty:\n",
    "#                 # Get difficulty range\n",
    "#                 if 'Avg_Composite_Difficulty' in topic_analysis.columns:\n",
    "#                     difficulty_range = topic_analysis['Avg_Composite_Difficulty'].max() - topic_analysis['Avg_Composite_Difficulty'].min()\n",
    "#                 else:\n",
    "#                     difficulty_range = 0\n",
    "                \n",
    "#                 comparison_data.append({\n",
    "#                     'Dataset': dataset_name,\n",
    "#                     'Type': 'Text' if dataset_name in ['ag_news', 'imdb'] else 'Image',\n",
    "#                     'N_Topics': analysis.get('n_topics', 0),\n",
    "#                     'N_Documents': analysis.get('n_documents', 0),\n",
    "#                     'Perplexity': analysis.get('perplexity', 0),\n",
    "#                     'Avg_Difficulty': difficulty_stats.get('overall_mean_composite', 0),\n",
    "#                     'Std_Difficulty': difficulty_stats.get('overall_std_composite', 0),\n",
    "#                     'Network_Density': network_stats.get('density', 0),\n",
    "#                     'Difficulty_Range': difficulty_range\n",
    "#                 })\n",
    "    \n",
    "#     if not comparison_data:\n",
    "#         print(\"  No comparison data available\")\n",
    "#         return None\n",
    "    \n",
    "#     comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "#     # Create interactive comparison dashboard\n",
    "#     fig = make_subplots(\n",
    "#         rows=2, cols=3,\n",
    "#         subplot_titles=(\n",
    "#             'Average Difficulty by Dataset',\n",
    "#             'Number of Topics vs Documents',\n",
    "#             'Network Density vs Difficulty',\n",
    "#             'Perplexity Comparison',\n",
    "#             'Difficulty Range Comparison',\n",
    "#             'Dataset Clustering'\n",
    "#         ),\n",
    "#         specs=[\n",
    "#             [{'type': 'bar'}, {'type': 'scatter'}, {'type': 'scatter'}],\n",
    "#             [{'type': 'bar'}, {'type': 'bar'}, {'type': 'scatter'}]\n",
    "#         ],\n",
    "#         vertical_spacing=0.15,\n",
    "#         horizontal_spacing=0.1\n",
    "#     )\n",
    "    \n",
    "#     # Plot 1: Average difficulty by dataset\n",
    "#     colors = px.colors.qualitative.Set2\n",
    "#     for i, (_, row) in enumerate(comparison_df.iterrows()):\n",
    "#         fig.add_trace(\n",
    "#             go.Bar(\n",
    "#                 x=[row['Dataset']],\n",
    "#                 y=[row['Avg_Difficulty']],\n",
    "#                 name=row['Dataset'],\n",
    "#                 marker_color=colors[i % len(colors)],\n",
    "#                 error_y=dict(\n",
    "#                     type='data',\n",
    "#                     array=[row['Std_Difficulty']],\n",
    "#                     visible=True\n",
    "#                 ),\n",
    "#                 customdata=[[\n",
    "#                     row['Type'],\n",
    "#                     row['N_Topics'],\n",
    "#                     row['N_Documents'],\n",
    "#                     row['Perplexity']\n",
    "#                 ]],\n",
    "#                 hovertemplate=(\n",
    "#                     '<b>%{x}</b><br>'\n",
    "#                     'Type: %{customdata[0]}<br>'\n",
    "#                     'Difficulty: %{y:.3f} ± %{error_y.array[0]:.3f}<br>'\n",
    "#                     'Topics: %{customdata[1]}<br>'\n",
    "#                     'Documents: %{customdata[2]:,}<br>'\n",
    "#                     'Perplexity: %{customdata[3]:.2f}<br>'\n",
    "#                     '<extra></extra>'\n",
    "#                 ),\n",
    "#                 showlegend=False\n",
    "#             ),\n",
    "#             row=1, col=1\n",
    "#         )\n",
    "    \n",
    "#     # Plot 2: Number of topics vs documents\n",
    "#     if len(comparison_df) > 0:\n",
    "#         fig.add_trace(\n",
    "#             go.Scatter(\n",
    "#                 x=comparison_df['N_Topics'],\n",
    "#                 y=comparison_df['N_Documents'] / 1000,  # Convert to thousands\n",
    "#                 mode='markers+text',\n",
    "#                 text=comparison_df['Dataset'],\n",
    "#                 textposition='top center',\n",
    "#                 marker=dict(\n",
    "#                     size=comparison_df['Avg_Difficulty'] * 50,\n",
    "#                     color=comparison_df['Avg_Difficulty'],\n",
    "#                     colorscale='RdYlBu_r',\n",
    "#                     showscale=True,\n",
    "#                     colorbar=dict(title='Avg Difficulty', x=0.47, y=0.95, len=0.25),\n",
    "#                     line=dict(width=2, color='black')\n",
    "#                 ),\n",
    "#                 customdata=comparison_df[['Type', 'Perplexity', 'Network_Density']].values,\n",
    "#                 hovertemplate=(\n",
    "#                     '<b>%{text}</b><br>'\n",
    "#                     'Topics: %{x}<br>'\n",
    "#                     'Documents (thousands): %{y:.1f}<br>'\n",
    "#                     'Type: %{customdata[0]}<br>'\n",
    "#                     'Perplexity: %{customdata[1]:.2f}<br>'\n",
    "#                     'Network Density: %{customdata[2]:.3f}<br>'\n",
    "#                     '<extra></extra>'\n",
    "#                 ),\n",
    "#                 name='Topics vs Documents'\n",
    "#             ),\n",
    "#             row=1, col=2\n",
    "#         )\n",
    "    \n",
    "#     # Plot 3: Network density vs difficulty\n",
    "#     if len(comparison_df) > 0:\n",
    "#         fig.add_trace(\n",
    "#             go.Scatter(\n",
    "#                 x=comparison_df['Network_Density'],\n",
    "#                 y=comparison_df['Avg_Difficulty'],\n",
    "#                 mode='markers+text',\n",
    "#                 text=comparison_df['Dataset'],\n",
    "#                 textposition='top center',\n",
    "#                 marker=dict(\n",
    "#                     size=comparison_df['N_Topics'] * 10,\n",
    "#                     color=comparison_df['Type'].map({'Text': 'blue', 'Image': 'red'}),\n",
    "#                     symbol=comparison_df['Type'].map({'Text': 'circle', 'Image': 'square'}),\n",
    "#                     line=dict(width=2, color='black')\n",
    "#                 ),\n",
    "#                 customdata=comparison_df[['N_Documents', 'Perplexity', 'Type']].values,\n",
    "#                 hovertemplate=(\n",
    "#                     '<b>%{text}</b><br>'\n",
    "#                     'Network Density: %{x:.3f}<br>'\n",
    "#                     'Avg Difficulty: %{y:.3f}<br>'\n",
    "#                     'Type: %{customdata[2]}<br>'\n",
    "#                     'Documents: %{customdata[0]:,}<br>'\n",
    "#                     'Perplexity: %{customdata[1]:.2f}<br>'\n",
    "#                     '<extra></extra>'\n",
    "#                 ),\n",
    "#                 name='Network vs Difficulty'\n",
    "#             ),\n",
    "#             row=1, col=3\n",
    "#         )\n",
    "    \n",
    "#     # Plot 4: Perplexity comparison\n",
    "#     if len(comparison_df) > 0:\n",
    "#         fig.add_trace(\n",
    "#             go.Bar(\n",
    "#                 x=comparison_df['Dataset'],\n",
    "#                 y=comparison_df['Perplexity'],\n",
    "#                 marker_color=comparison_df['Type'].map({'Text': 'lightblue', 'Image': 'lightcoral'}),\n",
    "#                 customdata=comparison_df[['Type', 'N_Topics', 'Avg_Difficulty']].values,\n",
    "#                 hovertemplate=(\n",
    "#                     '<b>%{x}</b><br>'\n",
    "#                     'Perplexity: %{y:.2f}<br>'\n",
    "#                     'Type: %{customdata[0]}<br>'\n",
    "#                     'Topics: %{customdata[1]}<br>'\n",
    "#                     'Avg Difficulty: %{customdata[2]:.3f}<br>'\n",
    "#                     '<extra></extra>'\n",
    "#                 ),\n",
    "#                 name='Perplexity',\n",
    "#                 showlegend=False\n",
    "#             ),\n",
    "#             row=2, col=1\n",
    "#         )\n",
    "    \n",
    "#     # Plot 5: Difficulty range comparison - FIXED: removed colorscale from Bar trace\n",
    "#     if len(comparison_df) > 0:\n",
    "#         # Normalize difficulty range for coloring\n",
    "#         if comparison_df['Difficulty_Range'].max() > comparison_df['Difficulty_Range'].min():\n",
    "#             norm_range = (comparison_df['Difficulty_Range'] - comparison_df['Difficulty_Range'].min()) / \\\n",
    "#                         (comparison_df['Difficulty_Range'].max() - comparison_df['Difficulty_Range'].min())\n",
    "#         else:\n",
    "#             norm_range = pd.Series([0.5] * len(comparison_df))\n",
    "        \n",
    "#         # Use Plotly colorscale\n",
    "#         colorscale = px.colors.sequential.Viridis\n",
    "#         colors = [colorscale[int(val * (len(colorscale)-1))] for val in norm_range]\n",
    "        \n",
    "#         fig.add_trace(\n",
    "#             go.Bar(\n",
    "#                 x=comparison_df['Dataset'],\n",
    "#                 y=comparison_df['Difficulty_Range'],\n",
    "#                 marker_color=colors,\n",
    "#                 customdata=comparison_df[['Type', 'Avg_Difficulty', 'Std_Difficulty']].values,\n",
    "#                 hovertemplate=(\n",
    "#                     '<b>%{x}</b><br>'\n",
    "#                     'Difficulty Range: %{y:.3f}<br>'\n",
    "#                     'Type: %{customdata[0]}<br>'\n",
    "#                     'Avg Difficulty: %{customdata[1]:.3f}<br>'\n",
    "#                     'Std Difficulty: %{customdata[2]:.3f}<br>'\n",
    "#                     '<extra></extra>'\n",
    "#                 ),\n",
    "#                 name='Difficulty Range',\n",
    "#                 showlegend=False\n",
    "#             ),\n",
    "#             row=2, col=2\n",
    "#         )\n",
    "    \n",
    "#     # Plot 6: Dataset clustering (simplified)\n",
    "#     if len(comparison_df) > 1:\n",
    "#         try:\n",
    "#             from sklearn.preprocessing import StandardScaler\n",
    "#             from sklearn.decomposition import PCA\n",
    "            \n",
    "#             # Prepare data for PCA\n",
    "#             cluster_data = comparison_df[['Avg_Difficulty', 'Std_Difficulty', \n",
    "#                                          'Network_Density', 'Difficulty_Range', \n",
    "#                                          'Perplexity']].values\n",
    "#             cluster_data = StandardScaler().fit_transform(cluster_data)\n",
    "            \n",
    "#             # Apply PCA\n",
    "#             pca = PCA(n_components=2)\n",
    "#             pca_result = pca.fit_transform(cluster_data)\n",
    "            \n",
    "#             fig.add_trace(\n",
    "#                 go.Scatter(\n",
    "#                     x=pca_result[:, 0],\n",
    "#                     y=pca_result[:, 1],\n",
    "#                     mode='markers+text',\n",
    "#                     text=comparison_df['Dataset'],\n",
    "#                     textposition='top center',\n",
    "#                     marker=dict(\n",
    "#                         size=20,\n",
    "#                         color=comparison_df['Type'].map({'Text': 'blue', 'Image': 'red'}),\n",
    "#                         symbol=comparison_df['Type'].map({'Text': 'circle', 'Image': 'square'}),\n",
    "#                         line=dict(width=2, color='black')\n",
    "#                     ),\n",
    "#                     customdata=comparison_df[['Type', 'N_Topics', 'N_Documents']].values,\n",
    "#                     hovertemplate=(\n",
    "#                         '<b>%{text}</b><br>'\n",
    "#                         'PCA 1: %{x:.2f}<br>'\n",
    "#                         'PCA 2: %{y:.2f}<br>'\n",
    "#                         'Type: %{customdata[0]}<br>'\n",
    "#                         'Topics: %{customdata[1]}<br>'\n",
    "#                         'Documents: %{customdata[2]:,}<br>'\n",
    "#                         '<extra></extra>'\n",
    "#                     ),\n",
    "#                     name='Dataset Clusters'\n",
    "#                 ),\n",
    "#                 row=2, col=3\n",
    "#             )\n",
    "            \n",
    "#             # Add variance explained to axis labels\n",
    "#             fig.update_xaxes(\n",
    "#                 title_text=f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)',\n",
    "#                 row=2, col=3\n",
    "#             )\n",
    "#             fig.update_yaxes(\n",
    "#                 title_text=f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)',\n",
    "#                 row=2, col=3\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             print(f\"  Could not create clustering plot: {e}\")\n",
    "#             fig.add_annotation(\n",
    "#                 text=\"Clustering not available\",\n",
    "#                 xref=\"x domain\", yref=\"y domain\",\n",
    "#                 x=0.5, y=0.5,\n",
    "#                 showarrow=False,\n",
    "#                 font=dict(size=14),\n",
    "#                 row=2, col=3\n",
    "#             )\n",
    "#     else:\n",
    "#         fig.add_annotation(\n",
    "#             text=\"Need at least 2 datasets for clustering\",\n",
    "#             xref=\"x domain\", yref=\"y domain\",\n",
    "#             x=0.5, y=0.5,\n",
    "#             showarrow=False,\n",
    "#             font=dict(size=14),\n",
    "#             row=2, col=3\n",
    "#         )\n",
    "    \n",
    "#     # Update layout\n",
    "#     fig.update_layout(\n",
    "#         title='Interactive Dataset Comparison Dashboard',\n",
    "#         title_font_size=20,\n",
    "#         showlegend=False,\n",
    "#         height=1000,\n",
    "#         width=1600,\n",
    "#         template='plotly_white'\n",
    "#     )\n",
    "    \n",
    "#     # Update axes labels\n",
    "#     fig.update_xaxes(title_text=\"Dataset\", row=1, col=1)\n",
    "#     fig.update_yaxes(title_text=\"Average Difficulty\", row=1, col=1)\n",
    "#     fig.update_xaxes(title_text=\"Number of Topics\", row=1, col=2)\n",
    "#     fig.update_yaxes(title_text=\"Documents (thousands)\", row=1, col=2)\n",
    "#     fig.update_xaxes(title_text=\"Network Density\", row=1, col=3)\n",
    "#     fig.update_yaxes(title_text=\"Average Difficulty\", row=1, col=3)\n",
    "#     fig.update_xaxes(title_text=\"Dataset\", row=2, col=1)\n",
    "#     fig.update_yaxes(title_text=\"Perplexity\", row=2, col=1)\n",
    "#     fig.update_xaxes(title_text=\"Dataset\", row=2, col=2)\n",
    "#     fig.update_yaxes(title_text=\"Difficulty Range\", row=2, col=2)\n",
    "    \n",
    "#     # Save comparison dashboard\n",
    "#     comparison_path = \"results/tmcl/topic_models/interactive_dataset_comparison.html\"\n",
    "#     os.makedirs(os.path.dirname(comparison_path), exist_ok=True)\n",
    "#     fig.write_html(comparison_path)\n",
    "#     print(f\"  Interactive comparison dashboard saved to: {comparison_path}\")\n",
    "    \n",
    "#     return fig\n",
    "\n",
    "# def create_all_interactive_visualizations():\n",
    "#     \"\"\"\n",
    "#     Create all interactive visualizations for the analyzed datasets\n",
    "#     \"\"\"\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"CREATING INTERACTIVE TOPIC VISUALIZATIONS\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     # Try to import plotly for image export\n",
    "#     try:\n",
    "#         import plotly.io as pio\n",
    "#         pio.kaleido.scope.default_format = \"png\"\n",
    "#     except:\n",
    "#         pass\n",
    "    \n",
    "#     # Load all results\n",
    "#     datasets = ['ag_news', 'imdb', 'cifar10', 'cifar100', 'mnist', 'fashion_mnist']\n",
    "#     all_analysis_results = {}\n",
    "    \n",
    "#     # Create visualizations for each dataset\n",
    "#     for dataset_name in datasets:\n",
    "#         print(f\"\\nProcessing {dataset_name}...\")\n",
    "        \n",
    "#         # Load results\n",
    "#         results_path = f\"results/tmcl/topic_models/{dataset_name}_tmcl_results.pkl\"\n",
    "#         if os.path.exists(results_path):\n",
    "#             try:\n",
    "#                 with open(results_path, 'rb') as f:\n",
    "#                     results = pickle.load(f)\n",
    "                \n",
    "#                 print(f\"  Successfully loaded results from {results_path}\")\n",
    "                \n",
    "#                 # Get feature names for text datasets\n",
    "#                 feature_names = None\n",
    "#                 if dataset_name in ['ag_news', 'imdb']:\n",
    "#                     if 'vocabulary' in results:\n",
    "#                         feature_names = results['vocabulary']\n",
    "#                         print(f\"  Using saved vocabulary of size {len(feature_names)}\")\n",
    "#                     elif 'vectorizer' in results:\n",
    "#                         # Try to get vocabulary from vectorizer\n",
    "#                         vectorizer = results['vectorizer']\n",
    "#                         if hasattr(vectorizer, 'get_feature_names_out'):\n",
    "#                             feature_names = vectorizer.get_feature_names_out()\n",
    "#                             print(f\"  Using vectorizer vocabulary of size {len(feature_names)}\")\n",
    "#                         elif hasattr(vectorizer, 'get_feature_names'):\n",
    "#                             feature_names = vectorizer.get_feature_names()\n",
    "#                             print(f\"  Using vectorizer vocabulary of size {len(feature_names)}\")\n",
    "#                     else:\n",
    "#                         # Try to load from a vocabulary file\n",
    "#                         vocab_path = f\"results/tmcl/topic_models/{dataset_name}_vocabulary.pkl\"\n",
    "#                         if os.path.exists(vocab_path):\n",
    "#                             with open(vocab_path, 'rb') as f:\n",
    "#                                 feature_names = pickle.load(f)\n",
    "#                             print(f\"  Loaded vocabulary from file of size {len(feature_names)}\")\n",
    "#                         else:\n",
    "#                             # Create placeholder vocabulary\n",
    "#                             if 'feature_matrix' in results:\n",
    "#                                 n_features = results['feature_matrix'].shape[1]\n",
    "#                             else:\n",
    "#                                 n_features = 1000\n",
    "#                             feature_names = [f'feature_{i}' for i in range(n_features)]\n",
    "#                             print(f\"  WARNING: Using placeholder feature names (n={n_features})\")\n",
    "                \n",
    "#                 # 1. Create interactive topic network\n",
    "#                 try:\n",
    "#                     network_fig, network_graph = create_interactive_topic_network(\n",
    "#                         dataset_name, results, feature_names\n",
    "#                     )\n",
    "#                     print(f\"  ✓ Created interactive topic network\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"  ✗ Error creating network: {e}\")\n",
    "                \n",
    "#                 # 2. Create interactive dashboard\n",
    "#                 try:\n",
    "#                     dashboard_fig = create_interactive_topic_dashboard(\n",
    "#                         dataset_name, results, feature_names\n",
    "#                     )\n",
    "#                     print(f\"  ✓ Created interactive dashboard\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"  ✗ Error creating dashboard: {e}\")\n",
    "                \n",
    "#                 # 3. Create topic evolution timeline\n",
    "#                 try:\n",
    "#                     timeline_fig = create_topic_evolution_timeline(\n",
    "#                         dataset_name, results, feature_names\n",
    "#                     )\n",
    "#                     print(f\"  ✓ Created topic evolution timeline\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"  ✗ Error creating timeline: {e}\")\n",
    "                \n",
    "#                 # Load enhanced analysis results for comparison\n",
    "#                 enhanced_path = f\"results/tmcl/topic_models/{dataset_name}_enhanced_analysis.pkl\"\n",
    "#                 if os.path.exists(enhanced_path):\n",
    "#                     try:\n",
    "#                         with open(enhanced_path, 'rb') as f:\n",
    "#                             all_analysis_results[dataset_name] = pickle.load(f)\n",
    "#                         print(f\"  ✓ Loaded enhanced analysis results\")\n",
    "#                     except:\n",
    "#                         print(f\"  ✗ Could not load enhanced analysis results\")\n",
    "#                 else:\n",
    "#                     print(f\"  Note: Enhanced analysis results not found at {enhanced_path}\")\n",
    "                    \n",
    "#             except Exception as e:\n",
    "#                 print(f\"  ✗ Error loading results for {dataset_name}: {e}\")\n",
    "#         else:\n",
    "#             print(f\"  ✗ Results file not found for {dataset_name}\")\n",
    "    \n",
    "#     # 4. Create interactive comparison across all datasets\n",
    "#     if all_analysis_results:\n",
    "#         try:\n",
    "#             comparison_fig = create_interactive_topic_comparison(all_analysis_results)\n",
    "#             if comparison_fig:\n",
    "#                 print(f\"  ✓ Created interactive comparison across datasets\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"  ✗ Error creating comparison: {e}\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"INTERACTIVE VISUALIZATIONS COMPLETE!\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     # Print summary of created files\n",
    "#     print(\"\\nCreated interactive visualizations:\")\n",
    "#     print(\"-\" * 40)\n",
    "#     for dataset_name in datasets:\n",
    "#         print(f\"\\n{dataset_name}:\")\n",
    "#         network_file = f\"results/tmcl/topic_models/{dataset_name}_interactive_network.html\"\n",
    "#         if os.path.exists(network_file):\n",
    "#             print(f\"  ✓ Network: {network_file}\")\n",
    "        \n",
    "#         dashboard_file = f\"results/tmcl/topic_models/{dataset_name}_interactive_dashboard.html\"\n",
    "#         if os.path.exists(dashboard_file):\n",
    "#             print(f\"  ✓ Dashboard: {dashboard_file}\")\n",
    "        \n",
    "#         timeline_file = f\"results/tmcl/topic_models/{dataset_name}_topic_timeline.html\"\n",
    "#         if os.path.exists(timeline_file):\n",
    "#             print(f\"  ✓ Timeline: {timeline_file}\")\n",
    "    \n",
    "#     comparison_file = \"results/tmcl/topic_models/interactive_dataset_comparison.html\"\n",
    "#     if os.path.exists(comparison_file):\n",
    "#         print(f\"\\nCross-dataset comparison: {comparison_file}\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"To view interactive visualizations:\")\n",
    "#     print(\"1. Open the HTML files in a web browser\")\n",
    "#     print(\"2. Hover over nodes/points to see details\")\n",
    "#     print(\"3. Use zoom and pan to explore the visualizations\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     # Additional instructions for text datasets\n",
    "#     print(\"\\nFor text datasets (ag_news, imdb):\")\n",
    "#     print(\"• Topic labels show T0, T1, etc. with top words\")\n",
    "#     print(\"• Hover over nodes to see all top words for each topic\")\n",
    "#     print(\"• Word indices are shown if actual words are not available\")\n",
    "#     print(\"=\"*80)\n",
    "\n",
    "# # Execute the interactive visualizations\n",
    "# if __name__ == \"__main__\":\n",
    "#     create_all_interactive_visualizations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synexian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
